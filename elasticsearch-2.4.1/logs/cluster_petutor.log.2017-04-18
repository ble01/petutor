[2017-04-18 09:21:19,549][DEBUG][action.search            ] [node_petutor_01] [documents][0], node[YOHEOP2WSoCuhgQoUiRgfQ], [P], v[2], s[STARTED], a[id=Wo--kqxpRYKI5tUHhv1sKQ]: Failed to execute [org.elasticsearch.action.search.SearchRequest@1c0c2874] lastShard [true]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.buildGroupedQueries(MultiMatchQuery.java:111)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:19,549][DEBUG][action.search            ] [node_petutor_01] [documents][0], node[YOHEOP2WSoCuhgQoUiRgfQ], [P], v[2], s[STARTED], a[id=Wo--kqxpRYKI5tUHhv1sKQ]: Failed to execute [org.elasticsearch.action.search.SearchRequest@245a155c] lastShard [true]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$CrossFieldsQueryBuilder.buildGroupedQueries(MultiMatchQuery.java:205)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:19,549][DEBUG][action.search            ] [node_petutor_01] [documents][4], node[YOHEOP2WSoCuhgQoUiRgfQ], [P], v[2], s[STARTED], a[id=efS9yKD9RW26Bd0XCv86Ww]: Failed to execute [org.elasticsearch.action.search.SearchRequest@1c0c2874] lastShard [true]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.buildGroupedQueries(MultiMatchQuery.java:111)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:19,549][DEBUG][action.search            ] [node_petutor_01] [documents][1], node[YOHEOP2WSoCuhgQoUiRgfQ], [P], v[2], s[STARTED], a[id=vyP9Ea-qQsGnlG4EhDsWQQ]: Failed to execute [org.elasticsearch.action.search.SearchRequest@245a155c] lastShard [true]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$CrossFieldsQueryBuilder.buildGroupedQueries(MultiMatchQuery.java:205)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:19,549][DEBUG][action.search            ] [node_petutor_01] [documents][1], node[YOHEOP2WSoCuhgQoUiRgfQ], [P], v[2], s[STARTED], a[id=vyP9Ea-qQsGnlG4EhDsWQQ]: Failed to execute [org.elasticsearch.action.search.SearchRequest@1c0c2874]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.buildGroupedQueries(MultiMatchQuery.java:111)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:19,549][DEBUG][action.search            ] [node_petutor_01] [documents][3], node[YOHEOP2WSoCuhgQoUiRgfQ], [P], v[2], s[STARTED], a[id=By9oDv23Q0iDkCr7JPjtug]: Failed to execute [org.elasticsearch.action.search.SearchRequest@1c0c2874] lastShard [true]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.buildGroupedQueries(MultiMatchQuery.java:111)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:19,549][DEBUG][action.search            ] [node_petutor_01] [documents][2], node[YOHEOP2WSoCuhgQoUiRgfQ], [P], v[2], s[STARTED], a[id=U5eQrJs0Qpy_Z-HKE9pe8A]: Failed to execute [org.elasticsearch.action.search.SearchRequest@1c0c2874] lastShard [true]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.buildGroupedQueries(MultiMatchQuery.java:111)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:24,759][DEBUG][action.search            ] [node_petutor_01] All shards failed for phase: [query]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.buildGroupedQueries(MultiMatchQuery.java:111)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:24,728][DEBUG][action.search            ] [node_petutor_01] [documents][1], node[YOHEOP2WSoCuhgQoUiRgfQ], [P], v[2], s[STARTED], a[id=vyP9Ea-qQsGnlG4EhDsWQQ]: Failed to execute [org.elasticsearch.action.search.SearchRequest@47f31be3] lastShard [true]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.buildGroupedQueries(MultiMatchQuery.java:111)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:24,695][DEBUG][action.search            ] [node_petutor_01] [documents][0], node[YOHEOP2WSoCuhgQoUiRgfQ], [P], v[2], s[STARTED], a[id=Wo--kqxpRYKI5tUHhv1sKQ]: Failed to execute [org.elasticsearch.action.search.SearchRequest@47f31be3] lastShard [true]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.buildGroupedQueries(MultiMatchQuery.java:111)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:24,376][DEBUG][action.search            ] [node_petutor_01] [documents][4], node[YOHEOP2WSoCuhgQoUiRgfQ], [P], v[2], s[STARTED], a[id=efS9yKD9RW26Bd0XCv86Ww]: Failed to execute [org.elasticsearch.action.search.SearchRequest@245a155c]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$CrossFieldsQueryBuilder.buildGroupedQueries(MultiMatchQuery.java:205)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:24,354][DEBUG][action.search            ] [node_petutor_01] [documents][3], node[YOHEOP2WSoCuhgQoUiRgfQ], [P], v[2], s[STARTED], a[id=By9oDv23Q0iDkCr7JPjtug]: Failed to execute [org.elasticsearch.action.search.SearchRequest@245a155c] lastShard [true]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$CrossFieldsQueryBuilder.buildGroupedQueries(MultiMatchQuery.java:205)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:24,049][DEBUG][action.search            ] [node_petutor_01] [documents][2], node[YOHEOP2WSoCuhgQoUiRgfQ], [P], v[2], s[STARTED], a[id=U5eQrJs0Qpy_Z-HKE9pe8A]: Failed to execute [org.elasticsearch.action.search.SearchRequest@245a155c] lastShard [true]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$CrossFieldsQueryBuilder.buildGroupedQueries(MultiMatchQuery.java:205)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:25,304][DEBUG][action.search            ] [node_petutor_01] [documents][0], node[YOHEOP2WSoCuhgQoUiRgfQ], [P], v[2], s[STARTED], a[id=Wo--kqxpRYKI5tUHhv1sKQ]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3ce676e9] lastShard [true]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$CrossFieldsQueryBuilder.buildGroupedQueries(MultiMatchQuery.java:205)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:25,281][DEBUG][action.search            ] [node_petutor_01] All shards failed for phase: [query]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$CrossFieldsQueryBuilder.buildGroupedQueries(MultiMatchQuery.java:205)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:25,250][DEBUG][action.search            ] [node_petutor_01] [documents][4], node[YOHEOP2WSoCuhgQoUiRgfQ], [P], v[2], s[STARTED], a[id=efS9yKD9RW26Bd0XCv86Ww]: Failed to execute [org.elasticsearch.action.search.SearchRequest@47f31be3]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.buildGroupedQueries(MultiMatchQuery.java:111)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:24,959][INFO ][monitor.jvm              ] [node_petutor_01] [gc][young][305329][723] duration [773ms], collections [1]/[1s], total [773ms]/[1m], memory [166.7mb]->[102.2mb]/[990.7mb], all_pools {[young] [67.2mb]->[1.7mb]/[266.2mb]}{[survivor] [427kb]->[1.4mb]/[33.2mb]}{[old] [99mb]->[99mb]/[691.2mb]}
[2017-04-18 09:21:24,875][DEBUG][action.search            ] [node_petutor_01] [documents][3], node[YOHEOP2WSoCuhgQoUiRgfQ], [P], v[2], s[STARTED], a[id=By9oDv23Q0iDkCr7JPjtug]: Failed to execute [org.elasticsearch.action.search.SearchRequest@47f31be3] lastShard [true]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.buildGroupedQueries(MultiMatchQuery.java:111)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:24,844][DEBUG][action.search            ] [node_petutor_01] [documents][2], node[YOHEOP2WSoCuhgQoUiRgfQ], [P], v[2], s[STARTED], a[id=U5eQrJs0Qpy_Z-HKE9pe8A]: Failed to execute [org.elasticsearch.action.search.SearchRequest@47f31be3] lastShard [true]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.buildGroupedQueries(MultiMatchQuery.java:111)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:25,717][DEBUG][action.search            ] [node_petutor_01] [documents][3], node[YOHEOP2WSoCuhgQoUiRgfQ], [P], v[2], s[STARTED], a[id=By9oDv23Q0iDkCr7JPjtug]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3ce676e9] lastShard [true]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$CrossFieldsQueryBuilder.buildGroupedQueries(MultiMatchQuery.java:205)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:25,328][DEBUG][action.search            ] [node_petutor_01] All shards failed for phase: [query]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.buildGroupedQueries(MultiMatchQuery.java:111)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:25,328][DEBUG][action.search            ] [node_petutor_01] [documents][2], node[YOHEOP2WSoCuhgQoUiRgfQ], [P], v[2], s[STARTED], a[id=U5eQrJs0Qpy_Z-HKE9pe8A]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3ce676e9] lastShard [true]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$CrossFieldsQueryBuilder.buildGroupedQueries(MultiMatchQuery.java:205)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:25,318][DEBUG][action.search            ] [node_petutor_01] [documents][1], node[YOHEOP2WSoCuhgQoUiRgfQ], [P], v[2], s[STARTED], a[id=vyP9Ea-qQsGnlG4EhDsWQQ]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3ce676e9] lastShard [true]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$CrossFieldsQueryBuilder.buildGroupedQueries(MultiMatchQuery.java:205)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:25,731][DEBUG][action.search            ] [node_petutor_01] [documents][4], node[YOHEOP2WSoCuhgQoUiRgfQ], [P], v[2], s[STARTED], a[id=efS9yKD9RW26Bd0XCv86Ww]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3ce676e9]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$CrossFieldsQueryBuilder.buildGroupedQueries(MultiMatchQuery.java:205)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:25,763][DEBUG][action.search            ] [node_petutor_01] All shards failed for phase: [query]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$CrossFieldsQueryBuilder.buildGroupedQueries(MultiMatchQuery.java:205)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:25,844][WARN ][rest.suppressed          ] path: /documents/concept/_search, params: {index=documents, type=concept}
Failed to execute phase [query], all shards failed; shardFailures {[YOHEOP2WSoCuhgQoUiRgfQ][documents][0]: RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024]; }{[YOHEOP2WSoCuhgQoUiRgfQ][documents][1]: RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024]; }{[YOHEOP2WSoCuhgQoUiRgfQ][documents][2]: RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024]; }{[YOHEOP2WSoCuhgQoUiRgfQ][documents][3]: RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024]; }{[YOHEOP2WSoCuhgQoUiRgfQ][documents][4]: RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024]; }
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onFirstPhaseResult(AbstractSearchAsyncAction.java:206)
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:152)
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:46)
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:872)
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:850)
	at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:387)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:39)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [maxClauseCount is set to 1024]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.ElasticsearchException.guessRootCauses(ElasticsearchException.java:386)
	at org.elasticsearch.action.search.SearchPhaseExecutionException.guessRootCauses(SearchPhaseExecutionException.java:152)
	at org.elasticsearch.action.search.SearchPhaseExecutionException.getCause(SearchPhaseExecutionException.java:99)
	at java.lang.Throwable.printStackTrace(Throwable.java:665)
	at java.lang.Throwable.printStackTrace(Throwable.java:721)
	at org.apache.log4j.DefaultThrowableRenderer.render(DefaultThrowableRenderer.java:60)
	at org.apache.log4j.spi.ThrowableInformation.getThrowableStrRep(ThrowableInformation.java:87)
	at org.apache.log4j.spi.LoggingEvent.getThrowableStrRep(LoggingEvent.java:413)
	at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:313)
	at org.apache.log4j.WriterAppender.append(WriterAppender.java:162)
	at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
	at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
	at org.apache.log4j.Category.callAppenders(Category.java:206)
	at org.apache.log4j.Category.forcedLog(Category.java:391)
	at org.apache.log4j.Category.log(Category.java:856)
	at org.elasticsearch.common.logging.log4j.Log4jESLogger.internalWarn(Log4jESLogger.java:135)
	at org.elasticsearch.common.logging.support.AbstractESLogger.warn(AbstractESLogger.java:109)
	at org.elasticsearch.rest.BytesRestResponse.convert(BytesRestResponse.java:134)
	at org.elasticsearch.rest.BytesRestResponse.<init>(BytesRestResponse.java:96)
	at org.elasticsearch.rest.BytesRestResponse.<init>(BytesRestResponse.java:87)
	at org.elasticsearch.rest.action.support.RestActionListener.onFailure(RestActionListener.java:60)
	at org.elasticsearch.action.support.TransportAction$1.onFailure(TransportAction.java:95)
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.raiseEarlyFailure(AbstractSearchAsyncAction.java:294)
	... 10 more
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.buildGroupedQueries(MultiMatchQuery.java:111)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	... 3 more
[2017-04-18 09:21:25,844][WARN ][rest.suppressed          ] path: /documents/chapter/_search, params: {index=documents, type=chapter}
Failed to execute phase [query], all shards failed; shardFailures {[YOHEOP2WSoCuhgQoUiRgfQ][documents][0]: RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024]; }{[YOHEOP2WSoCuhgQoUiRgfQ][documents][1]: RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024]; }{[YOHEOP2WSoCuhgQoUiRgfQ][documents][2]: RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024]; }{[YOHEOP2WSoCuhgQoUiRgfQ][documents][3]: RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024]; }{[YOHEOP2WSoCuhgQoUiRgfQ][documents][4]: RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network\",\"conceptDescription\":\"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024]; }
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onFirstPhaseResult(AbstractSearchAsyncAction.java:206)
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:152)
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:46)
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:872)
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:850)
	at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:387)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:39)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [maxClauseCount is set to 1024]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.ElasticsearchException.guessRootCauses(ElasticsearchException.java:386)
	at org.elasticsearch.action.search.SearchPhaseExecutionException.guessRootCauses(SearchPhaseExecutionException.java:152)
	at org.elasticsearch.action.search.SearchPhaseExecutionException.getCause(SearchPhaseExecutionException.java:99)
	at java.lang.Throwable.printStackTrace(Throwable.java:665)
	at java.lang.Throwable.printStackTrace(Throwable.java:721)
	at org.apache.log4j.DefaultThrowableRenderer.render(DefaultThrowableRenderer.java:60)
	at org.apache.log4j.spi.ThrowableInformation.getThrowableStrRep(ThrowableInformation.java:87)
	at org.apache.log4j.spi.LoggingEvent.getThrowableStrRep(LoggingEvent.java:413)
	at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:313)
	at org.apache.log4j.WriterAppender.append(WriterAppender.java:162)
	at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
	at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
	at org.apache.log4j.Category.callAppenders(Category.java:206)
	at org.apache.log4j.Category.forcedLog(Category.java:391)
	at org.apache.log4j.Category.log(Category.java:856)
	at org.elasticsearch.common.logging.log4j.Log4jESLogger.internalWarn(Log4jESLogger.java:135)
	at org.elasticsearch.common.logging.support.AbstractESLogger.warn(AbstractESLogger.java:109)
	at org.elasticsearch.rest.BytesRestResponse.convert(BytesRestResponse.java:134)
	at org.elasticsearch.rest.BytesRestResponse.<init>(BytesRestResponse.java:96)
	at org.elasticsearch.rest.BytesRestResponse.<init>(BytesRestResponse.java:87)
	at org.elasticsearch.rest.action.support.RestActionListener.onFailure(RestActionListener.java:60)
	at org.elasticsearch.action.support.TransportAction$1.onFailure(TransportAction.java:95)
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.raiseEarlyFailure(AbstractSearchAsyncAction.java:294)
	... 10 more
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$CrossFieldsQueryBuilder.buildGroupedQueries(MultiMatchQuery.java:205)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	... 3 more
[2017-04-18 09:21:25,844][WARN ][rest.suppressed          ] path: /documents/concept/_search, params: {index=documents, type=concept}
Failed to execute phase [query], all shards failed; shardFailures {[YOHEOP2WSoCuhgQoUiRgfQ][documents][0]: RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024]; }{[YOHEOP2WSoCuhgQoUiRgfQ][documents][1]: RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024]; }{[YOHEOP2WSoCuhgQoUiRgfQ][documents][2]: RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024]; }{[YOHEOP2WSoCuhgQoUiRgfQ][documents][3]: RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024]; }{[YOHEOP2WSoCuhgQoUiRgfQ][documents][4]: RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024]; }
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onFirstPhaseResult(AbstractSearchAsyncAction.java:206)
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:152)
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:46)
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:872)
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:850)
	at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:387)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:39)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [maxClauseCount is set to 1024]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.ElasticsearchException.guessRootCauses(ElasticsearchException.java:386)
	at org.elasticsearch.action.search.SearchPhaseExecutionException.guessRootCauses(SearchPhaseExecutionException.java:152)
	at org.elasticsearch.action.search.SearchPhaseExecutionException.getCause(SearchPhaseExecutionException.java:99)
	at java.lang.Throwable.printStackTrace(Throwable.java:665)
	at java.lang.Throwable.printStackTrace(Throwable.java:721)
	at org.apache.log4j.DefaultThrowableRenderer.render(DefaultThrowableRenderer.java:60)
	at org.apache.log4j.spi.ThrowableInformation.getThrowableStrRep(ThrowableInformation.java:87)
	at org.apache.log4j.spi.LoggingEvent.getThrowableStrRep(LoggingEvent.java:413)
	at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:313)
	at org.apache.log4j.WriterAppender.append(WriterAppender.java:162)
	at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
	at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
	at org.apache.log4j.Category.callAppenders(Category.java:206)
	at org.apache.log4j.Category.forcedLog(Category.java:391)
	at org.apache.log4j.Category.log(Category.java:856)
	at org.elasticsearch.common.logging.log4j.Log4jESLogger.internalWarn(Log4jESLogger.java:135)
	at org.elasticsearch.common.logging.support.AbstractESLogger.warn(AbstractESLogger.java:109)
	at org.elasticsearch.rest.BytesRestResponse.convert(BytesRestResponse.java:134)
	at org.elasticsearch.rest.BytesRestResponse.<init>(BytesRestResponse.java:96)
	at org.elasticsearch.rest.BytesRestResponse.<init>(BytesRestResponse.java:87)
	at org.elasticsearch.rest.action.support.RestActionListener.onFailure(RestActionListener.java:60)
	at org.elasticsearch.action.support.TransportAction$1.onFailure(TransportAction.java:95)
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.raiseEarlyFailure(AbstractSearchAsyncAction.java:294)
	... 10 more
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.buildGroupedQueries(MultiMatchQuery.java:111)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	... 3 more
[2017-04-18 09:21:25,844][WARN ][rest.suppressed          ] path: /documents/chapter/_search, params: {index=documents, type=chapter}
Failed to execute phase [query], all shards failed; shardFailures {[YOHEOP2WSoCuhgQoUiRgfQ][documents][0]: RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024]; }{[YOHEOP2WSoCuhgQoUiRgfQ][documents][1]: RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024]; }{[YOHEOP2WSoCuhgQoUiRgfQ][documents][2]: RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024]; }{[YOHEOP2WSoCuhgQoUiRgfQ][documents][3]: RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024]; }{[YOHEOP2WSoCuhgQoUiRgfQ][documents][4]: RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":10,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","type":"cross_fields","fields":["title","description"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024]; }
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onFirstPhaseResult(AbstractSearchAsyncAction.java:206)
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:152)
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:46)
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:872)
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:850)
	at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:387)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:39)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [maxClauseCount is set to 1024]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.ElasticsearchException.guessRootCauses(ElasticsearchException.java:386)
	at org.elasticsearch.action.search.SearchPhaseExecutionException.guessRootCauses(SearchPhaseExecutionException.java:152)
	at org.elasticsearch.action.search.SearchPhaseExecutionException.getCause(SearchPhaseExecutionException.java:99)
	at java.lang.Throwable.printStackTrace(Throwable.java:665)
	at java.lang.Throwable.printStackTrace(Throwable.java:721)
	at org.apache.log4j.DefaultThrowableRenderer.render(DefaultThrowableRenderer.java:60)
	at org.apache.log4j.spi.ThrowableInformation.getThrowableStrRep(ThrowableInformation.java:87)
	at org.apache.log4j.spi.LoggingEvent.getThrowableStrRep(LoggingEvent.java:413)
	at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:313)
	at org.apache.log4j.WriterAppender.append(WriterAppender.java:162)
	at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
	at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
	at org.apache.log4j.Category.callAppenders(Category.java:206)
	at org.apache.log4j.Category.forcedLog(Category.java:391)
	at org.apache.log4j.Category.log(Category.java:856)
	at org.elasticsearch.common.logging.log4j.Log4jESLogger.internalWarn(Log4jESLogger.java:135)
	at org.elasticsearch.common.logging.support.AbstractESLogger.warn(AbstractESLogger.java:109)
	at org.elasticsearch.rest.BytesRestResponse.convert(BytesRestResponse.java:134)
	at org.elasticsearch.rest.BytesRestResponse.<init>(BytesRestResponse.java:96)
	at org.elasticsearch.rest.BytesRestResponse.<init>(BytesRestResponse.java:87)
	at org.elasticsearch.rest.action.support.RestActionListener.onFailure(RestActionListener.java:60)
	at org.elasticsearch.action.support.TransportAction$1.onFailure(TransportAction.java:95)
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.raiseEarlyFailure(AbstractSearchAsyncAction.java:294)
	... 10 more
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$CrossFieldsQueryBuilder.buildGroupedQueries(MultiMatchQuery.java:205)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	... 3 more
[2017-04-18 09:21:39,169][DEBUG][action.search            ] [node_petutor_01] [data][1], node[YOHEOP2WSoCuhgQoUiRgfQ], [P], v[2], s[STARTED], a[id=Pee4ixBgQaWrhzRHp2yPng]: Failed to execute [org.elasticsearch.action.search.SearchRequest@486a8c3b] lastShard [true]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.buildGroupedQueries(MultiMatchQuery.java:111)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:39,278][DEBUG][action.search            ] [node_petutor_01] [data][4], node[YOHEOP2WSoCuhgQoUiRgfQ], [P], v[2], s[STARTED], a[id=K0123O5UT3-xSFV8evZdag]: Failed to execute [org.elasticsearch.action.search.SearchRequest@486a8c3b] lastShard [true]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.buildGroupedQueries(MultiMatchQuery.java:111)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:39,176][DEBUG][action.search            ] [node_petutor_01] [data][2], node[YOHEOP2WSoCuhgQoUiRgfQ], [P], v[2], s[STARTED], a[id=HGvtNVl8SoatFH8XaJ8g9w]: Failed to execute [org.elasticsearch.action.search.SearchRequest@486a8c3b] lastShard [true]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.buildGroupedQueries(MultiMatchQuery.java:111)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:39,169][DEBUG][action.search            ] [node_petutor_01] [data][0], node[YOHEOP2WSoCuhgQoUiRgfQ], [P], v[2], s[STARTED], a[id=scDHloMxTxuBu-nXkV_9qA]: Failed to execute [org.elasticsearch.action.search.SearchRequest@486a8c3b] lastShard [true]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.buildGroupedQueries(MultiMatchQuery.java:111)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:39,278][DEBUG][action.search            ] [node_petutor_01] [data][3], node[YOHEOP2WSoCuhgQoUiRgfQ], [P], v[2], s[STARTED], a[id=LCoJjixIS0aW6478ABWKDA]: Failed to execute [org.elasticsearch.action.search.SearchRequest@486a8c3b]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.buildGroupedQueries(MultiMatchQuery.java:111)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:39,395][DEBUG][action.search            ] [node_petutor_01] All shards failed for phase: [query]
RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
Caused by: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:873)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.buildGroupedQueries(MultiMatchQuery.java:111)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	... 12 more
[2017-04-18 09:21:39,410][WARN ][rest.suppressed          ] path: /data/concept/_search, params: {index=data, type=concept}
Failed to execute phase [query], all shards failed; shardFailures {[YOHEOP2WSoCuhgQoUiRgfQ][data][0]: RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024]; }{[YOHEOP2WSoCuhgQoUiRgfQ][data][1]: RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024]; }{[YOHEOP2WSoCuhgQoUiRgfQ][data][2]: RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024]; }{[YOHEOP2WSoCuhgQoUiRgfQ][data][3]: RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024]; }{[YOHEOP2WSoCuhgQoUiRgfQ][data][4]: RemoteTransportException[[node_petutor_01][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":3,"from":0,"query":{"multi_match":{"query":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind","fields":["conceptLabel","conceptDescription"]}}}]]; nested: TooManyClauses[maxClauseCount is set to 1024]; }
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onFirstPhaseResult(AbstractSearchAsyncAction.java:206)
	at org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:152)
	at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:46)
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:872)
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:850)
	at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:387)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:39)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [maxClauseCount is set to 1024]; nested: TooManyClauses[maxClauseCount is set to 1024];
	at org.elasticsearch.ElasticsearchException.guessRootCauses(ElasticsearchException.java:386)
	at org.elasticsearch.action.search.SearchPhaseExecutionException.guessRootCauses(SearchPhaseExecutionException.java:152)
	at org.elasticsearch.action.search.SearchPhaseExecutionException.getCause(SearchPhaseExecutionException.java:99)
	at java.lang.Throwable.printStackTrace(Throwable.java:665)
	at java.lang.Throwable.printStackTrace(Throwable.java:721)
	at org.apache.log4j.DefaultThrowableRenderer.render(DefaultThrowableRenderer.java:60)
	at org.apache.log4j.spi.ThrowableInformation.getThrowableStrRep(ThrowableInformation.java:87)
	at org.apache.log4j.spi.LoggingEvent.getThrowableStrRep(LoggingEvent.java:413)
	at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:313)
	at org.apache.log4j.WriterAppender.append(WriterAppender.java:162)
	at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
	at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
	at org.apache.log4j.Category.callAppenders(Category.java:206)
	at org.apache.log4j.Category.forcedLog(Category.java:391)
	at org.apache.log4j.Category.log(Category.java:856)
	at org.elasticsearch.common.logging.log4j.Log4jESLogger.internalWarn(Log4jESLogger.java:135)
	at org.elasticsearch.common.logging.support.AbstractESLogger.warn(AbstractESLogger.java:109)
	at org.elasticsearch.rest.BytesRestResponse.convert(BytesRestResponse.java:134)
	at org.elasticsearch.rest.BytesRestResponse.<init>(BytesRestResponse.java:96)
	at org.elasticsearch.rest.BytesRestResponse.<init>(BytesRestResponse.java:87)
	at org.elasticsearch.rest.action.support.RestActionListener.onFailure(RestActionListener.java:60)
	at org.elasticsearch.action.support.TransportAction$1.onFailure(TransportAction.java:95)
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.raiseEarlyFailure(AbstractSearchAsyncAction.java:294)
	... 10 more
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
	at org.apache.lucene.search.BooleanQuery$Builder.add(BooleanQuery.java:137)
	at org.apache.lucene.util.QueryBuilder.add(QueryBuilder.java:302)
	at org.apache.lucene.util.QueryBuilder.analyzeMultiBoolean(QueryBuilder.java:322)
	at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:257)
	at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
	at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:176)
	at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:54)
	at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:41)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:120)
	at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.buildGroupedQueries(MultiMatchQuery.java:111)
	at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:88)
	at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:168)
	at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:250)
	at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:320)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
	at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:218)
	at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:856)
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:667)
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:633)
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:377)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
	at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	... 3 more
[2017-04-18 10:05:19,170][INFO ][node                     ] [node_petutor_01] stopping ...
[2017-04-18 10:05:33,713][DEBUG][action.admin.cluster.node.stats] [node_petutor_01] failed to execute on node [YOHEOP2WSoCuhgQoUiRgfQ]
SendRequestTransportException[[node_petutor_01][127.0.0.1:9300][cluster:monitor/nodes/stats[n]]]; nested: TransportException[TransportService is closed stopped can't send request];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:340)
	at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction.start(TransportNodesAction.java:165)
	at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction.access$100(TransportNodesAction.java:107)
	at org.elasticsearch.action.support.nodes.TransportNodesAction.doExecute(TransportNodesAction.java:77)
	at org.elasticsearch.action.support.nodes.TransportNodesAction.doExecute(TransportNodesAction.java:46)
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:137)
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:85)
	at org.elasticsearch.cluster.InternalClusterInfoService.updateNodeStats(InternalClusterInfoService.java:289)
	at org.elasticsearch.cluster.InternalClusterInfoService.refresh(InternalClusterInfoService.java:325)
	at org.elasticsearch.cluster.InternalClusterInfoService.maybeRefresh(InternalClusterInfoService.java:310)
	at org.elasticsearch.cluster.InternalClusterInfoService.access$600(InternalClusterInfoService.java:69)
	at org.elasticsearch.cluster.InternalClusterInfoService$SubmitReschedulingClusterInfoUpdatedJob$1.run(InternalClusterInfoService.java:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: TransportException[TransportService is closed stopped can't send request]
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:320)
	... 14 more
[2017-04-18 10:05:33,900][DEBUG][action.admin.indices.stats] [node_petutor_01] failed to execute [indices:monitor/stats] on node [YOHEOP2WSoCuhgQoUiRgfQ]
SendRequestTransportException[[node_petutor_01][127.0.0.1:9300][indices:monitor/stats[n]]]; nested: TransportException[TransportService is closed stopped can't send request];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:340)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:299)
	at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$AsyncAction.sendNodeRequest(TransportBroadcastByNodeAction.java:327)
	at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$AsyncAction.start(TransportBroadcastByNodeAction.java:315)
	at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction.doExecute(TransportBroadcastByNodeAction.java:238)
	at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction.doExecute(TransportBroadcastByNodeAction.java:79)
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:137)
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:85)
	at org.elasticsearch.cluster.InternalClusterInfoService.updateIndicesStats(InternalClusterInfoService.java:303)
	at org.elasticsearch.cluster.InternalClusterInfoService.refresh(InternalClusterInfoService.java:354)
	at org.elasticsearch.cluster.InternalClusterInfoService.maybeRefresh(InternalClusterInfoService.java:310)
	at org.elasticsearch.cluster.InternalClusterInfoService.access$600(InternalClusterInfoService.java:69)
	at org.elasticsearch.cluster.InternalClusterInfoService$SubmitReschedulingClusterInfoUpdatedJob$1.run(InternalClusterInfoService.java:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: TransportException[TransportService is closed stopped can't send request]
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:320)
	... 15 more
[2017-04-18 10:05:35,452][INFO ][node                     ] [node_petutor_01] stopped
[2017-04-18 10:05:35,455][INFO ][node                     ] [node_petutor_01] closing ...
[2017-04-18 10:05:36,152][INFO ][node                     ] [node_petutor_01] closed
[2017-04-18 10:06:33,577][INFO ][node                     ] [node_petutor_01] version[2.4.1], pid[14172], build[c67dc32/2016-09-27T18:57:55Z]
[2017-04-18 10:06:33,577][INFO ][node                     ] [node_petutor_01] initializing ...
[2017-04-18 10:06:36,185][INFO ][plugins                  ] [node_petutor_01] modules [reindex, lang-expression, lang-groovy], plugins [head], sites [head]
[2017-04-18 10:06:36,365][INFO ][env                      ] [node_petutor_01] using [1] data paths, mounts [[OS (C:)]], net usable_space [643.8gb], net total_space [921gb], spins? [unknown], types [NTFS]
[2017-04-18 10:06:36,366][INFO ][env                      ] [node_petutor_01] heap size [990.7mb], compressed ordinary object pointers [true]
[2017-04-18 10:06:43,179][INFO ][script                   ] [node_petutor_01] compiling script file [C:\ElasticSearch\elasticsearch-2.4.1\config\scripts\query_concepts_template.mustache]
[2017-04-18 10:06:45,005][INFO ][node                     ] [node_petutor_01] initialized
[2017-04-18 10:06:45,005][INFO ][node                     ] [node_petutor_01] starting ...
[2017-04-18 10:06:46,329][INFO ][transport                ] [node_petutor_01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}, {[::1]:9300}
[2017-04-18 10:06:46,345][INFO ][discovery                ] [node_petutor_01] cluster_petutor/QaotNFvCSqWf1NoOj9Mmww
[2017-04-18 10:06:50,498][INFO ][cluster.service          ] [node_petutor_01] new_master {node_petutor_01}{QaotNFvCSqWf1NoOj9Mmww}{127.0.0.1}{127.0.0.1:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2017-04-18 10:06:51,272][INFO ][http                     ] [node_petutor_01] publish_address {127.0.0.1:9201}, bound_addresses {127.0.0.1:9201}, {[::1]:9201}
[2017-04-18 10:06:51,272][INFO ][node                     ] [node_petutor_01] started
[2017-04-18 10:06:51,821][INFO ][gateway                  ] [node_petutor_01] recovered [9] indices into cluster_state
[2017-04-18 10:07:02,435][INFO ][cluster.routing.allocation] [node_petutor_01] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[.kibana][0]] ...]).
[2017-04-18 11:08:25,393][INFO ][node                     ] [node_petutor_01] stopping ...
[2017-04-18 11:08:25,703][INFO ][node                     ] [node_petutor_01] stopped
[2017-04-18 11:08:25,704][INFO ][node                     ] [node_petutor_01] closing ...
[2017-04-18 11:08:25,710][INFO ][node                     ] [node_petutor_01] closed
[2017-04-18 11:08:37,762][INFO ][node                     ] [node_petutor_01] version[2.4.1], pid[27204], build[c67dc32/2016-09-27T18:57:55Z]
[2017-04-18 11:08:37,763][INFO ][node                     ] [node_petutor_01] initializing ...
[2017-04-18 11:08:38,455][INFO ][plugins                  ] [node_petutor_01] modules [reindex, lang-expression, lang-groovy], plugins [head], sites [head]
[2017-04-18 11:08:38,484][INFO ][env                      ] [node_petutor_01] using [1] data paths, mounts [[OS (C:)]], net usable_space [643.8gb], net total_space [921gb], spins? [unknown], types [NTFS]
[2017-04-18 11:08:38,485][INFO ][env                      ] [node_petutor_01] heap size [990.7mb], compressed ordinary object pointers [true]
[2017-04-18 11:08:40,861][INFO ][script                   ] [node_petutor_01] compiling script file [C:\ElasticSearch\elasticsearch-2.4.1\config\scripts\query_concepts_template.mustache]
[2017-04-18 11:08:41,233][INFO ][node                     ] [node_petutor_01] initialized
[2017-04-18 11:08:41,233][INFO ][node                     ] [node_petutor_01] starting ...
[2017-04-18 11:08:41,532][INFO ][transport                ] [node_petutor_01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}, {[::1]:9300}
[2017-04-18 11:08:41,537][INFO ][discovery                ] [node_petutor_01] cluster_petutor/M2bJsYM8T9GSxw9cLl_BjQ
[2017-04-18 11:08:45,631][INFO ][cluster.service          ] [node_petutor_01] new_master {node_petutor_01}{M2bJsYM8T9GSxw9cLl_BjQ}{127.0.0.1}{127.0.0.1:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2017-04-18 11:08:46,070][INFO ][http                     ] [node_petutor_01] publish_address {127.0.0.1:9201}, bound_addresses {127.0.0.1:9201}, {[::1]:9201}
[2017-04-18 11:08:46,070][INFO ][node                     ] [node_petutor_01] started
[2017-04-18 11:08:46,186][INFO ][gateway                  ] [node_petutor_01] recovered [9] indices into cluster_state
[2017-04-18 11:08:51,812][INFO ][cluster.routing.allocation] [node_petutor_01] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[.kibana][0]] ...]).
