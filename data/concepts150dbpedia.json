{"index":{"_id":1}}
{"conceptLabelTag":"cca","conceptLabel":"cca","conceptDescription":"canonical correlation  statistics canonicalcorrelation analysis cca   way  making sense  crosscovariance matrices    two vectors x x x  y y y  random variables    correlations among  variables  canonicalcorrelation analysis will find linear combinations   x  y   maximum correlation    t r knapp notes virtually    commonly encountered parametric tests  significance can  treated  special cases  canonicalcorrelation analysis    general procedure  investigating  relationships  two sets  variables  method  first introduced  harold hotelling  definition given two column vectors formula  formula  random variables  second moments one may define  crosscovariance formula    formula matrix whose formula entry   covariance formula  practice   estimate  covariance matrix based  sampled data  formula  formula ie   pair  data matrices canonicalcorrelation analysis seeks vectors formula  formula    random variables formula  formula maximize  correlation formula  random variables formula  formula   first pair  canonical variables  one seeks vectors maximizing   correlation subject   constraint      uncorrelated   first pair  canonical variables  gives  second pair  canonical variables  procedure may  continued   formula times computation derivation let formula  formula  parameter  maximize   first step   define  change  basis  define  thus     cauchyschwarz inequality     equality   vectors formula  formula  collinear  addition  maximum  correlation  attained  formula   eigenvector   maximum eigenvalue   matrix formula see rayleigh quotient  subsequent pairs  found  using eigenvalues  decreasing magnitudes orthogonality  guaranteed   symmetry   correlation matrices solution  solution  therefore reciprocally   also reversing  change  coordinates     canonical variables  defined  implementation cca can  computed using singular value decomposition   correlation matrix   available   function  hypothesis testing  row can  tested  significance   following method since  correlations  sorted saying  row formula  zero implies   correlations  also zero    formula independent observations   sample  formula   estimated correlation  formula   formulath row  test statistic    asymptotically distributed   chisquared  formula degrees  dom  large formula since   correlations  formula  formula  logically zero  estimated  way also  product   terms   point  irrelevant practical uses  typical use  canonical correlation   experimental context   take two sets  variables  see   common amongst  two sets  example  psychological testing   take two well established multidimensional personality tests    minnesota multiphasic personality inventory mmpi   neo  seeing   mmpi factors relate   neo factors   gain insight   dimensions  common   tests   much variance  shared  example  might find   extraversion  neuroticism dimension accounted   substantial amount  shared variance   two tests one can also use canonicalcorrelation analysis  produce  model equation  relates two sets  variables  example  set  performance measures   set  explanatory variables   set  outputs  set  inputs constraint restrictions can  imposed    model  ensure  reflects theoretical requirements  intuitively obvious conditions  type  model  known   maximum correlation model visualization   results  canonical correlation  usually  bar plots   coefficients   two sets  variables   pairs  canonical variates showing significant correlation  authors suggest    best visualized  plotting   heliographs  circular format  ray like bars   half representing  two sets  variables examples let formula  zero expected value ie formula  formula ie formula  formula  perfectly correlated  eg formula  formula    first     example pair  canonical variables  formula  formula  formula ie formula  formula  perfectly anticorrelated  eg formula  formula    first     example pair  canonical variables  formula  formula  notice    cases formula  illustrates   canonicalcorrelation analysis treats correlated  anticorrelated variables similarly connection  principal angles assuming  formula  formula  zero expected values ie formula  covariance matrices formula  formula can  viewed  gram matrices   inner product   entries  formula  formula correspondingly   interpretation  random variables entries formula  formula  formula  formula  treated  elements   vector space   inner product given   covariance formula see covariancerelationship  inner products  definition   canonical variables formula  formula   equivalent   definition  principal vectors   pair  subspaces spanned   entries  formula  formula  respect   inner product  canonical correlations formula  equal   cosine  principal angles\r\n"}
{"index":{"_id":2}}
{"conceptLabelTag":"artificial neural network","conceptLabel":"artificial neural network","conceptDescription":"artificial neural network neural networks  connectionist systems   computational approach used  computer science   research disciplines   based   large collection  neural units artificial neurons loosely mimicking  way  biological brain solves problems  large clusters  biological neurons connected  axons  neural unit  connected  many others  links can  enforcing  inhibitory   effect   activation state  connected neural units  individual neural unit may   summation function  combines  values    inputs together  may   threshold function  limiting function   connection    unit     signal must surpass  limit  propagating   neurons  systems  selflearning  trained rather  explicitly programmed  excel  areas   solution  feature detection  difficult  express   traditional computer program neural networks typically consist  multiple layers   cube design   signal path traverses  front  back back propagation   use  forward stimulation  reset weights   front neural units    sometimes done  combination  training   correct result  known  modern networks   bit   flowing  terms  stimulation  inhibition  connections interacting   much  chaotic  complex fashion dynamic neural networks    advanced    dynamically can based  rules form new connections  even new neural units  disabling others  goal   neural network   solve problems    way   human brain  although several neural networks   abstract modern neural network projects typically work    thousand    million neural units  millions  connections   still several orders  magnitude less complex   human brain  closer   computing power   worm new brain research often stimulates new patterns  neural networks one new approach  using connections  span much   link processing layers rather  always  localized  adjacent neurons  research  explored   different types  signal  time  axons propagate   deep learning interpolates greater complexity   set  boolean variables  simply    neural networks  based  real numbers   value   core    axon typically   representation    interesting facet   systems     unpredictable   success  selflearning  training  become great problem solvers  others dont perform  well  order  train  several thousand cycles  interaction typically occur like  machine learning methods systems  learn  data neural networks   used  solve  wide variety  tasks like computer vision  speech recognition   hard  solve using ordinary rulebased programming historically  use  neural network models marked  directional shift   late eighties  highlevel symbolic artificial intelligence characterized  expert systems  knowledge embodied  ifthen rules  lowlevel subsymbolic machine learning characterized  knowledge embodied   parameters   cognitive model   dynamical system history warren mcculloch  walter pitts created  computational model  neural networks based  mathematics  algorithms called threshold logic  model paved  way  neural network research  split  two distinct approaches one approach focused  biological processes   brain    focused   application  neural networks  artificial intelligence hebbian learning   late s psychologist donald hebb created  hypothesis  learning based   mechanism  neural plasticity   now known  hebbian learning hebbian learning  considered    typical unsupervised learning rule   later variants  early models  long term potentiation researchers started applying  ideas  computational models   turings btype machines farley  wesley  clark first used computational machines  called calculators  simulate  hebbian network  mit  neural network computational machines  created  rochester holland habit  duda frank rosenblatt created  perceptron  algorithm  pattern recognition based   twolayer computer learning network using simple addition  subtraction  mathematical notation rosenblatt also described circuitry    basic perceptron    exclusiveor circuit  circuit     processed  neural networks    backpropagation algorithm  created  paul werbos neural network research stagnated   publication  machine learning research  marvin minsky  seymour papert  discovered two key issues   computational machines  processed neural networks  first   basic perceptrons  incapable  processing  exclusiveor circuit  second significant issue   computers didnt  enough processing power  effectively handle  long run time required  large neural networks neural network research slowed  computers achieved greater processing power backpropagation  resurgence  key advance  came later   backpropagation algorithm  effectively solved  exclusiveor problem   generally  problem  quickly training multilayer neural networks werbos   mids parallel distributed processing became popular   name connectionism  textbook  david e rumelhart  james mcclelland provided  full exposition   use  connectionism  computers  simulate neural processes neural networks  used  artificial intelligence  traditionally  viewed  simplified models  neural processing   brain even though  relation   model   biological architecture   brain  debated   clear   degree artificial neural networks mirror brain function support vector machines   much simpler methods   linear classifiers gradually overtook neural networks  machine learning popularity  earlier challenges  training deep neural networks  successfully addressed  methods   unsupervised pretraining  computing power increased   use  gpus  distributed computing neural networks   deployed   large scale particularly  image  visual recognition problems  became known  deep learning although deep learning   strictly synonymous  deep neural networks improvements since computational devices   created  cmos   biophysical simulation  neuromorphic computing  recent efforts show promise  creating nanodevices   large scale principal components analyses  convolution  successful   create  new class  neural computing   depends  learning rather  programming     fundamentally analog rather  digital even though  first instantiations may  fact   cmos digital devices    recurrent neural networks  deep feedforward neural networks developed   research group  j rgen schmidhuber   swiss ai lab idsia  won eight international competitions  pattern recognition  machine learning  example  bidirectional  multidimensional long shortterm memory lstm  alex graves et al won three competitions  connected handwriting recognition   international conference  document analysis  recognition icdar without  prior knowledge   three different languages   learned fast gpubased implementations   approach  dan ciresan  colleagues  idsia  won several pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  electron microscopy stacks challenge  others  neural networks also   first artificial pattern recognizers  achieve humancompetitive  even superhuman performance  important benchmarks   traffic sign recognition ijcnn   mnist handwritten digits problem  yann lecun  nyu deep highly nonlinear neural architectures similar   neocognitron  kunihiko fukushima   standard architecture  vision inspired   simple  complex cells identified  david h hubel  torsten wiesel   primary visual cortex can also  pretrained  unsupervised methods  geoff hintons lab  university  toronto  team   lab won  contest sponsored  merck  design software  help find molecules  might lead  new drugs models neural network models  artificial intelligence  usually referred   artificial neural networks anns   essentially simple mathematical models defining  function formula   distribution  formula   formula  formula  sometimes models  also intimately associated   particular learning algorithm  learning rule  common use   phrase ann model  really  definition   class   functions  members   class  obtained  varying parameters connection weights  specifics   architecture    number  neurons   connectivity network function  word network   term artificial neural network refers   interconnections   neurons   different layers   system  example system  three layers  first layer  input neurons  send data via synapses   second layer  neurons   via  synapses   third layer  output neurons  complex systems will   layers  neurons   increased layers  input neurons  output neurons  synapses store parameters called weights  manipulate  data   calculations  ann  typically defined  three types  parameters mathematically  neurons network function formula  defined   composition   functions formula  can   defined   composition   functions  can  conveniently represented   network structure  arrows depicting  dependencies  variables  widely used type  composition   nonlinear weighted sum  formula  formula commonly referred    activation function   predefined function    hyperbolic tangent  sigmoid function  important characteristic   activation function    provides  smooth transition  input values change ie  small change  input produces  small change  output  will  convenient   following  refer   collection  functions formula  simply  vector formula  figure depicts   decomposition  formula  dependencies  variables indicated  arrows  can  interpreted  two ways  first view   functional view  input formula  transformed   dimensional vector formula    transformed   dimensional vector formula   finally transformed  formula  view   commonly encountered   context  optimization  second view   probabilistic view  random variable formula depends upon  random variable formula  depends upon formula  depends upon  random variable formula  view   commonly encountered   context  graphical models  two views  largely equivalent  either case   particular network architecture  components  individual layers  independent    eg  components  formula  independent    given  input formula  naturally enables  degree  parallelism   implementation networks    previous one  commonly called feedforward   graph   directed acyclic graph networks  cycles  commonly called recurrent  networks  commonly depicted   manner shown   top   figure  formula  shown   dependent upon  however  implied temporal dependence   shown learning   attracted   interest  neural networks   possibility  learning given  specific task  solve   class  functions formula learning means using  set  observations  find formula  solves  task   optimal sense  entails defining  cost function formula     optimal solution formula formula formula ie  solution   cost less   cost   optimal solution see mathematical optimization  cost function formula   important concept  learning     measure   far away  particular solution    optimal solution   problem   solved learning algorithms search   solution space  find  function    smallest possible cost  applications   solution  dependent   data  cost must necessarily   function   observations otherwise     modelling anything related   data   frequently defined   statistic    approximations can  made   simple example consider  problem  finding  model formula  minimizes formula  data pairs formula drawn   distribution formula  practical situations     formula samples  formula  thus    example    minimize formula thus  cost  minimized   sample   data rather   entire distribution generating  data  formula  form  online machine learning must  used   cost  partially minimized   new example  seen  online machine learning  often used  formula  fixed    useful   case   distribution changes slowly  time  neural network methods  form  online machine learning  frequently used  finite datasets choosing  cost function    possible  define  arbitrary ad hoc cost function frequently  particular cost will  used either    desirable properties   convexity    arises naturally   particular formulation   problem eg   probabilistic formulation  posterior probability   model can  used   inverse cost ultimately  cost function will depend   desired task  overview   three main categories  learning tasks  provided  learning paradigms   three major learning paradigms  corresponding   particular abstract learning task   supervised learning unsupervised learning  reinforcement learning supervised learning  supervised learning   given  set  example pairs formula   aim   find  function formula   allowed class  functions  matches  examples   words  wish  infer  mapping implied   data  cost function  related   mismatch   mapping   data   implicitly contains prior knowledge   problem domain  commonly used cost   meansquared error  tries  minimize  average squared error   networks output formula   target value formula    example pairs  one tries  minimize  cost using gradient descent   class  neural networks called multilayer perceptrons mlp one obtains  common  wellknown backpropagation algorithm  training neural networks tasks  fall within  paradigm  supervised learning  pattern recognition also known  classification  regression also known  function approximation  supervised learning paradigm  also applicable  sequential data eg  speech  gesture recognition  can  thought   learning   teacher   form   function  provides continuous feedback   quality  solutions obtained thus far unsupervised learning  unsupervised learning  data formula  given   cost function   minimized  can   function   data formula   networks output formula  cost function  dependent   task    trying  model    priori assumptions  implicit properties   model  parameters   observed variables   trivial example consider  model formula  formula   constant   cost formula minimizing  cost will give us  value  formula   equal   mean   data  cost function can  much  complicated  form depends   application  example  compression    related   mutual information  formula  formula whereas  statistical modeling    related   posterior probability   model given  data note      examples  quantities   maximized rather  minimized tasks  fall within  paradigm  unsupervised learning   general estimation problems  applications include clustering  estimation  statistical distributions compression  filtering reinforcement learning  reinforcement learning data formula  usually  given  generated   agents interactions   environment   point  time formula  agent performs  action formula   environment generates  observation formula   instantaneous cost formula according   usually unknown dynamics  aim   discover  policy  selecting actions  minimizes  measure   longterm cost eg  expected cumulative cost  environments dynamics   longterm cost   policy  usually unknown  can  estimated  formally  environment  modeled   markov decision process mdp  states formula  actions formula   following probability distributions  instantaneous cost distribution formula  observation distribution formula   transition formula   policy  defined   conditional distribution  actions given  observations taken together  two  define  markov chain mc  aim   discover  policy ie  mc  minimizes  cost anns  frequently used  reinforcement learning  part   overall algorithm dynamic programming   coupled  anns giving neurodynamic programming  bertsekas  tsitsiklis  applied  multidimensional nonlinear problems    involved  vehicle routing natural resources management  medicine    ability  anns  mitigate losses  accuracy even  reducing  discretization grid density  numerically approximating  solution   original control problems tasks  fall within  paradigm  reinforcement learning  control problems games   sequential decision making tasks learning algorithms training  neural network model essentially means selecting one model   set  allowed models    bayesian framework determining  distribution   set  allowed models  minimizes  cost criterion   numerous algorithms available  training neural network models    can  viewed   straightforward application  optimization theory  statistical estimation    algorithms used  training artificial neural networks employ  form  gradient descent using backpropagation  compute  actual gradients   done  simply taking  derivative   cost function  respect   network parameters   changing  parameters   gradientrelated direction  backpropagation training algorithms  usually classified  three categories evolutionary methods gene expression programming simulated annealing expectationmaximization nonparametric methods  particle swarm optimization    methods  training neural networks employing artificial neural networks perhaps  greatest advantage  anns   ability   used   arbitrary function approximation mechanism  learns  observed data however using     straightforward   relatively good understanding   underlying theory  essential   correct implementation anns can  used naturally  online learning  large data set applications  simple implementation   existence  mostly local dependencies exhibited   structure allows  fast parallel implementations  hardware applications  utility  artificial neural network models lies   fact   can  used  infer  function  observations   particularly useful  applications   complexity   data  task makes  design    function  hand impracticable reallife applications  tasks artificial neural networks  applied  tend  fall within  following broad categories application areas include  system identification  control vehicle control trajectory prediction process control natural resources management quantum chemistry gameplaying  decision making backgammon chess poker pattern recognition radar systems face identification object recognition   sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications eg automated trading systems data mining  knowledge discovery  databases kdd visualization  email spam filtering artificial neural networks  also  used  diagnose several cancers  ann based hybrid lung cancer detection system named hlnd improves  accuracy  diagnosis   speed  lung cancer radiology  networks  also  used  diagnose prostate cancer  diagnoses can  used  make specific models taken   large group  patients compared  information  one given patient  models   depend  assumptions  correlations  different variables colorectal cancer  also  predicted using  neural networks neural networks  predict  outcome   patient  colorectal cancer   accuracy   current clinical methods  training  networks  predict multiple patient outcomes  unrelated institutions neural networks  neuroscience theoretical  computational neuroscience   field concerned   theoretical analysis   computational modeling  biological neural systems since neural systems  intimately related  cognitive processes  behavior  field  closely related  cognitive  behavioral modeling  aim   field   create models  biological neural systems  order  understand  biological systems work  gain  understanding neuroscientists strive  make  link  observed biological processes data biologically plausible mechanisms  neural processing  learning biological neural network models  theory statistical learning theory  information theory types  models many models  used   field defined  different levels  abstraction  modeling different aspects  neural systems  range  models   shortterm behavior  individual neurons eg models    dynamics  neural circuitry arise  interactions  individual neurons  finally  models   behavior can arise  abstract neural modules  represent complete subsystems  include models   longterm  shortterm plasticity  neural systems   relations  learning  memory   individual neuron   system level networks  memory integrating external memory components  artificial neural networks   long history dating back  early research  distributed representations  selforganizing maps eg  sparse distributed memory  patterns encoded  neural networks  used  memory addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders  recently deep learning  shown   useful  semantic hashing   deep graphical model   wordcount vectors  obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document memory networks  another extension  neural networks incorporating longterm memory   developed  facebook research  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response neural turing machines developed  google deepmind extend  capabilities  deep neural networks  coupling   external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples differentiable neural computers dnc   extension  neural turing machines also  deepmind   outperformed neural turing machines long shortterm memory systems  memory networks  sequenceprocessing tasks neural network software neural network software  used  simulate research develop  apply artificial neural networks biological neural networks    cases  wider array  adaptive systems types artificial neural network types vary     one  two layers  single direction logic  complicated multiinput many directional feedback loops  layers   whole  systems use algorithms   programming  determine control  organization   functions  systems use weights  change  parameters   throughput   varying connections   neurons artificial neural networks can  autonomous  learn  input  outside teachers  even selfteaching  writtenin rules neural cube style neural networks first pioneered  gianna giavelli provide  dynamic space   networks dynamically recombine information  links across billions  self adapting nodes utilizing neural darwinism  technique developed  gerald edelman  allows   biologically modeled systems theoretical properties computational power  multilayer perceptron   universal function approximator  proven   universal approximation theorem however  proof   constructive regarding  number  neurons required  network topology  settings   weights   learning parameters work  hava siegelmann  eduardo d sontag  provided  proof   specific recurrent architecture  rational valued weights  opposed  full precision real numbervalued weights   full power   universal turing machine using  finite number  neurons  standard linear connections     shown   use  irrational values  weights results   machine  superturing power capacity artificial neural network models   property called capacity  roughly corresponds   ability  model  given function   related   amount  information  can  stored   network    notion  complexity convergence nothing can  said  general  convergence since  depends   number  factors firstly  may exist many local minima  depends   cost function   model secondly  optimization method used might   guaranteed  converge  far away   local minimum thirdly    large amount  data  parameters  methods become impractical  general    found  theoretical guarantees regarding convergence   unreliable guide  practical application generalization  statistics  applications   goal   create  system  generalizes well  unseen examples  problem  overtraining  emerged  arises  convoluted  overspecified systems   capacity   network significantly exceeds  needed  parameters   two schools  thought  avoiding  problem  first   use crossvalidation  similar techniques  check   presence  overtraining  optimally select hyperparameters    minimize  generalization error  second   use  form  regularization    concept  emerges naturally   probabilistic bayesian framework   regularization can  performed  selecting  larger prior probability  simpler models  also  statistical learning theory   goal   minimize  two quantities  empirical risk   structural risk  roughly corresponds   error   training set   predicted error  unseen data due  overfitting supervised neural networks  use  mean squared error mse cost function can use formal statistical methods  determine  confidence   trained model  mse   validation set can  used   estimate  variance  value can   used  calculate  confidence interval   output   network assuming  normal distribution  confidence analysis made  way  statistically valid  long   output probability distribution stays     network   modified  assigning  softmax activation function  generalization   logistic function   output layer   neural network   softmax component   componentbased neural network  categorical target variables  outputs can  interpreted  posterior probabilities    useful  classification   gives  certainty measure  classifications  softmax activation function  criticism training issues  common criticism  neural networks particularly  robotics    require  large diversity  training  realworld operation    surprising since  learning machine needs sufficient representative examples  order  capture  underlying structure  allows   generalize  new cases dean  pomerleau   research presented   paper knowledgebased training  artificial neural networks  autonomous robot driving uses  neural network  train  robotic vehicle  drive  multiple types  roads single lane multilane dirt etc  large amount   research  devoted  extrapolating multiple training scenarios   single training experience  preserving past training diversity    system   become overtrained   example   presented   series  right turns    learn  always turn right  issues  common  neural networks  must decide  amongst  wide variety  responses  can  dealt   several ways  example  randomly shuffling  training examples  using  numerical optimization algorithm    take  large steps  changing  network connections following  example   grouping examples  socalled minibatches theoretical issues  k dewdney  mathematician  computer scientist  university  western ontario  former scientific american columnist wrote  although neural nets  solve   toy problems  powers  computation   limited    surprised anyone takes  seriously   general problemsolving tool  neural network  ever  shown  solves computationally difficult problems    nqueens problem  travelling salesman problem   problem  factoring large integers aside   utility  fundamental objection  artificial neural networks    fail  reflect  real neurons function back propagation    heart   artificial neural networks       evidence    mechanism  natural neural networks  seems  contradict  fundamental principle  real neurons  information can  flow forward along  axon  information  coded  real neurons   yet known   known   sensor neurons fire action potentials  frequently  sensor activation  muscle cells pull  strongly   associated motor neurons receive action potentials  frequently    simplest case  just relaying information   sensor neuron   motor neuron almost nothing   underlying general principles   information  handled  real neural networks  known  motivation behind artificial neural networks   necessarily  replicate real neural function   use natural neural networks   inspiration   approach  computing   inherently parallel   provides solutions  problems     now  considered intractable  central claim  artificial neural networks  therefore   embodies  new  powerful general principle  processing information unfortunately  general principles  illdefined    often claimed    emergent   neural network   allows simple statistical association  basic function  artificial neural networks   described  learning  recognition   result artificial neural networks   somethingfornothing quality one  imparts  peculiar aura  laziness   distinct lack  curiosity  just  good  computing systems   human hand  mind intervenes solutions  found    magic   one  seems  learned anything hardware issues  implement large  effective software neural networks considerable processing  storage resources need   committed   brain  hardware tailored   task  processing signals   graph  neurons simulating even   simplified form  von neumann architecture may compel  neural network designer  fill many millions  database rows   connections  can consume vast amounts  computer memory  hard disk space furthermore  designer  neural network systems will often need  simulate  transmission  signals  many   connections   associated neurons  must often  matched  incredible amounts  cpu processing power  time j rgen schmidhuber notes   resurgence  neural networks   twentyfirst century   renewed success  image recognition tasks  largely attributable  advances  hardware   computing power especially  delivered  gpgpus  gpus  increased around  millionfold making  standard backpropagation algorithm feasible  training networks   several layers deeper    adds   doesnt overcome algorithmic problems   vanishing gradients   fundamental way  use  gpus instead  ordinary cpus can bring training times   networks   months  mere days computing power continues  grow roughly according  moores law  may provide sufficient resources  accomplish new tasks neuromorphic engineering addresses  hardware difficulty directly  constructing nonvonneumann chips  circuits designed  implement neural nets   ground  google  also designed  chip optimized  neural network processing called  tensor processing unit  tpu practical counterexamples  criticisms arguments  dewdneys position   neural networks   successfully used  solve many complex  diverse tasks ranging  autonomously flying aircraft  detecting credit card fraud technology writer roger bridgman commented  dewdneys statements  neural nets neural networks  instance    dock       hyped  high heaven  hasnt  also    create  successful net without understanding   worked  bunch  numbers  captures  behaviour    probability   opaque unreadable tablevalueless   scientific resource  spite   emphatic declaration  science   technology dewdney seems   pillory neural nets  bad science     devising   just trying   good engineers  unreadable table   useful machine  read  still  well worth  although   true  analyzing    learned   artificial neural network  difficult   much easier      analyze    learned   biological neural network furthermore researchers involved  exploring learning algorithms  neural networks  gradually uncovering generic principles  allow  learning machine   successful  example bengio  lecun wrote  article regarding local vs nonlocal learning  well  shallow vs deep architecture hybrid approaches   criticisms come  advocates  hybrid models combining neural networks  symbolic approaches  believe   intermix   two approaches can better capture  mechanisms   human mind\r\n"}
{"index":{"_id":3}}
{"conceptLabelTag":"collaborative filtering","conceptLabel":"collaborative filtering","conceptDescription":"collaborative filtering collaborative filtering cf   technique used  recommender systems collaborative filtering  two senses  narrow one    general one   newer narrower sense collaborative filtering   method  making automatic predictions filtering   interests   user  collecting preferences  taste information  many users collaborating  underlying assumption   collaborative filtering approach     person     opinion   person b   issue    likely   bs opinion   different issue     randomly chosen person  example  collaborative filtering recommendation system  television tastes  make predictions   television show  user  like given  partial list   users tastes likes  dislikes note   predictions  specific   user  use information gleaned  many users  differs   simpler approach  giving  average nonspecific score   item  interest  example based   number  votes    general sense collaborative filtering   process  filtering  information  patterns using techniques involving collaboration among multiple agents viewpoints data sources etc applications  collaborative filtering typically involve  large data sets collaborative filtering methods   applied  many different kinds  data including sensing  monitoring data    mineral exploration environmental sensing  large areas  multiple sensors financial data   financial service institutions  integrate many financial sources   electronic commerce  web applications   focus   user data etc  remainder   discussion focuses  collaborative filtering  user data although    methods  approaches may apply    major applications  well introduction  growth   internet  made  much  difficult  effectively extract useful information    available online information  overwhelming amount  data necessitates mechanisms  efficient information filtering collaborative filtering  one   techniques used  dealing   problem  motivation  collaborative filtering comes   idea  people often get  best recommendations  someone  tastes similar   collaborative filtering encompasses techniques  matching people  similar interests  making recommendations   basis collaborative filtering algorithms often require users active participation  easy way  represent users interests  algorithms   able  match people  similar interests typically  workflow   collaborative filtering system   key problem  collaborative filtering    combine  weight  preferences  user neighbors sometimes users can immediately rate  recommended items   result  system gains  increasingly accurate representation  user preferences  time methodology collaborative filtering systems  many forms  many common systems can  reduced  two steps  falls   category  userbased collaborative filtering  specific application     userbased nearest neighbor algorithm alternatively itembased collaborative filtering users  bought x also bought y proceeds   itemcentric manner see  example  slope one itembased collaborative filtering family another form  collaborative filtering can  based  implicit observations  normal user behavior  opposed   artificial behavior imposed   rating task  systems observe   user  done together    users  done  music   listened   items   bought  use  data  predict  users behavior   future   predict   user might like  behave given  chance  predictions     filtered  business logic  determine   might affect  actions   business system  example    useful  offer  sell somebody  particular album  music   already  demonstrated     music relying   scoring  rating system   averaged across  users ignores specific demands   user   particularly poor  tasks    large variation  interest    recommendation  music however    methods  combat information explosion   web search  data clustering types memorybased  approach uses user rating data  compute  similarity  users  items   used  making recommendations    early approach used  many commercial systems  effective  easy  implement typical examples   approach  neighbourhoodbased cf  itembaseduserbased topn recommendations  example  user based approaches  value  ratings user u gives  item   calculated   aggregation   similar users rating   item  u denotes  set  top n users    similar  user u  rated item   examples   aggregation function includes  k   normalizing factor defined  formula  formula   average rating  user u    items rated  u  neighborhoodbased algorithm calculates  similarity  two users  items produces  prediction   user  taking  weighted average    ratings similarity computation  items  users   important part   approach multiple measures   pearson correlation  vector cosine based similarity  used    pearson correlation similarity  two users x y  defined      set  items rated   user x  user y  cosinebased approach defines  cosinesimilarity  two users x  y   user based topn recommendation algorithm uses  similaritybased vector model  identify  k  similar users   active user   k  similar users  found  corresponding useritem matrices  aggregated  identify  set  items   recommended  popular method  find  similar users   localitysensitive hashing  implements  nearest neighbor mechanism  linear time  advantages   approach include  explainability   results    important aspect  recommendation systems easy creation  use easy facilitation  new data contentindependence   items  recommended good scaling  corated items   also several disadvantages   approach  performance decreases  data gets sparse  occurs frequently  webrelated items  hinders  scalability   approach  creates problems  large datasets although  can efficiently handle new users   relies   data structure adding new items becomes  complicated since  representation usually relies   specific vector space adding new items requires inclusion   new item   reinsertion    elements   structure modelbased models  developed using data mining machine learning algorithms  find patterns based  training data   used  make predictions  real data   many modelbased cf algorithms  include bayesian networks clustering models latent semantic models   singular value decomposition probabilistic latent semantic analysis multiple multiplicative factor latent dirichlet allocation  markov decision process based models  approach    holistic goal  uncover latent factors  explain observed ratings    models  based  creating  classification  clustering technique  identify  user based   training set  number   parameters can  reduced based  types  principal component analysis   several advantages   paradigm  handles  sparsity better  memory based ones  helps  scalability  large data sets  improves  prediction performance  gives  intuitive rationale   recommendations  disadvantages   approach    expensive model building one needs    tradeoff  prediction performance  scalability one can lose useful information due  reduction models  number  models  difficulty explaining  predictions hybrid  number  applications combine  memorybased   modelbased cf algorithms  overcome  limitations  native cf approaches  improve prediction performance importantly  overcome  cf problems   sparsity  loss  information however   increased complexity   expensive  implement usually  commercial recommender systems  hybrid  example  google news recommender system application  social web unlike  traditional model  mainstream media      editors  set guidelines collaboratively filtered social media can    large number  editors  content improves   number  participants increases services like reddit youtube  lastfm  typical example  collaborative filtering based media one scenario  collaborative filtering application   recommend interesting  popular information  judged   community   typical example stories appear   front page  reddit    voted  rated positively   community   community becomes larger   diverse  promoted stories can better reflect  average interest   community members another aspect  collaborative filtering systems   ability  generate  personalized recommendations  analyzing information   past activity   specific user   history   users deemed    similar taste   given user  resources  used  user profiling  helps  site recommend content   userbyuser basis    given user makes use   system  better  recommendations become   system gains data  improve  model   user problems  collaborative filtering system   necessarily succeed  automatically matching content  ones preferences unless  platform achieves unusually good diversity  independence  opinions one point  view will always dominate another   particular community    personalized recommendation scenario  introduction  new users  new items can cause  cold start problem   will  insufficient data   new entries   collaborative filtering  work accurately  order  make appropriate recommendations   new user  system must first learn  users preferences  analysing past voting  rating activities  collaborative filtering system requires  substantial number  users  rate  new item   item can  recommended challenges data sparsity  practice many commercial recommender systems  based  large datasets   result  useritem matrix used  collaborative filtering   extremely large  sparse  brings   challenges   performances   recommendation one typical problem caused   data sparsity   cold start problem  collaborative filtering methods recommend items based  users past preferences new users will need  rate sufficient number  items  enable  system  capture  preferences accurately  thus provides reliable recommendations similarly new items also    problem  new items  added  system  need   rated  substantial number  users     recommended  users   similar tastes   ones rated   new item problem   limit  contentbased recommendation   recommendation   item  based   discrete set  descriptive qualities rather   ratings scalability   numbers  users  items grow traditional cf algorithms will suffer serious scalability problems  example  tens  millions  customers formula  millions  items formula  cf algorithm   complexity  formula  already  large  well many systems need  react immediately  online requirements  make recommendations   users regardless   purchases  ratings history  demands  higher scalability   cf system large web companies   twitter use clusters  machines  scale recommendations   millions  users   computations happening   large memory machines synonyms synonyms refers   tendency   number      similar items   different names  entries  recommender systems  unable  discover  latent association  thus treat  products differently  example  seemingly different items children movie  children film  actually referring    item indeed  degree  variability  descriptive term usage  greater  commonly suspected  prevalence  synonyms decreases  recommendation performance  cf systems topic modeling like  latent dirichlet allocation technique  solve   grouping different words belonging    topic gray sheep gray sheep refers   users whose opinions   consistently agree  disagree   group  people  thus   benefit  collaborative filtering black sheep   opposite group whose idiosyncratic tastes make recommendations nearly impossible although    failure   recommender system nonelectronic recommenders also  great problems   cases  black sheep   acceptable failure shilling attacks   recommendation system  everyone can give  ratings people may give lots  positive ratings    items  negative ratings   competitors   often necessary   collaborative filtering systems  introduce precautions  discourage  kind  manipulations diversity   long tail collaborative filters  expected  increase diversity   help us discover new products  algorithms however may unintentionally   opposite  collaborative filters recommend products based  past sales  ratings   usually recommend products  limited historical data  can create  richgetricher effect  popular products akin  positive feedback  bias toward popularity can prevent   otherwise better consumerproduct matches  wharton study details  phenomenon along  several ideas  may promote diversity   long tail several collaborative filtering algorithms   developed  promote diversity   long tail  recommending novel unexpected  serendipitous items\r\n"}
{"index":{"_id":4}}
{"conceptLabelTag":"isomap","conceptLabel":"isomap","conceptDescription":"isomap isomap   nonlinear dimensionality reduction method   one  several widely used lowdimensional embedding methods isomap  used  computing  quasiisometric lowdimensional embedding   set  highdimensional data points  algorithm provides  simple method  estimating  intrinsic geometry   data manifold based   rough estimate   data points neighbors   manifold isomap  highly efficient  generally applicable   broad range  data sources  dimensionalities introduction isomap  one representative  isometric mapping methods  extends metric multidimensional scaling mds  incorporating  geodesic distances imposed   weighted graph   specific  classical scaling  metric mds performs lowdimensional embedding based   pairwise distance  data points   generally measured using straightline euclidean distance isomap  distinguished   use   geodesic distance induced   neighborhood graph embedded   classical scaling   done  incorporate manifold structure   resulting embedding isomap defines  geodesic distance    sum  edge weights along  shortest path  two nodes computed using dijkstras algorithm  example  top n eigenvectors   geodesic distance matrix represent  coordinates   new ndimensional euclidean space algorithm   highlevel description  isomap algorithm  given  possible issues  connectivity   data point   neighborhood graph  defined   nearest k euclidean neighbors   highdimensional space  step  vulnerable  shortcircuit errors  k   large  respect   manifold structure   noise   data moves  points slightly   manifold even  single shortcircuit error can alter many entries   geodesic distance matrix   turn can lead   drastically different  incorrect lowdimensional embedding conversely  k   small  neighborhood graph may become  sparse  approximate geodesic paths accurately  improvements   made   algorithm  make  work better  sparse  noisy data sets relationship   methods following  connection   classical scaling  pca metric mds can  interpreted  kernel pca   similar manner  geodesic distance matrix  isomap can  viewed   kernel matrix  doubly centered geodesic distance matrix k  isomap    form  formula   elementwise square   geodesic distance matrix d d h   centering matrix given  however  kernel matrix k   always positive semidefinite  main idea  kernel isomap   make  k   mercer kernel matrix   positive semidefinite using  constantshifting method  order  relate   kernel pca    generalization property naturally emerges\r\n"}
{"index":{"_id":5}}
{"conceptLabelTag":"regression analysis","conceptLabel":"regression analysis","conceptDescription":"regression analysis  statistical modeling regression analysis   statistical process  estimating  relationships among variables  includes many techniques  modeling  analyzing several variables   focus    relationship   dependent variable  one   independent variables  predictors  specifically regression analysis helps one understand   typical value   dependent variable  criterion variable changes   one   independent variables  varied    independent variables  held fixed  commonly regression analysis estimates  conditional expectation   dependent variable given  independent variables    average value   dependent variable   independent variables  fixed less commonly  focus    quantile   location parameter   conditional distribution   dependent variable given  independent variables   cases  estimation target   function   independent variables called  regression function  regression analysis   also  interest  characterize  variation   dependent variable around  regression function  can  described   probability distribution  related  distinct approach  necessary condition analysis nca  estimates  maximum rather  average value   dependent variable   given value   independent variable ceiling line rather  central line  order  identify  value   independent variable  necessary   sufficient   given value   dependent variable regression analysis  widely used  prediction  forecasting   use  substantial overlap   field  machine learning regression analysis  also used  understand  among  independent variables  related   dependent variable   explore  forms   relationships  restricted circumstances regression analysis can  used  infer causal relationships   independent  dependent variables however  can lead  illusions  false relationships  caution  advisable  example correlation   imply causation many techniques  carrying  regression analysis   developed familiar methods   linear regression  ordinary least squares regression  parametric    regression function  defined  terms   finite number  unknown parameters   estimated   data nonparametric regression refers  techniques  allow  regression function  lie   specified set  functions  may  infinitedimensional  performance  regression analysis methods  practice depends   form   data generating process    relates   regression approach  used since  true form   datagenerating process  generally  known regression analysis often depends   extent  making assumptions   process  assumptions  sometimes testable   sufficient quantity  data  available regression models  prediction  often useful even   assumptions  moderately violated although  may  perform optimally however  many applications especially  small effects  questions  causality based  observational data regression methods can give misleading results   narrower sense regression may refer specifically   estimation  continuous response variables  opposed   discrete response variables used  classification  case   continuous output variable may   specifically referred   metric regression  distinguish   related problems history  earliest form  regression   method  least squares   published  legendre    gauss  legendre  gauss  applied  method   problem  determining  astronomical observations  orbits  bodies   sun mostly comets  also later   newly discovered minor planets gauss published   development   theory  least squares  including  version   gaussmarkov theorem  term regression  coined  francis galton   nineteenth century  describe  biological phenomenon  phenomenon    heights  descendants  tall ancestors tend  regress  towards  normal average  phenomenon also known  regression toward  mean  galton regression    biological meaning   work  later extended  udny yule  karl pearson    general statistical context   work  yule  pearson  joint distribution   response  explanatory variables  assumed   gaussian  assumption  weakened  ra fisher   works   fisher assumed   conditional distribution   response variable  gaussian   joint distribution need     respect fishers assumption  closer  gausss formulation    s  s economists used electromechanical desk calculators  calculate regressions   sometimes took   hours  receive  result  one regression regression methods continue    area  active research  recent decades new methods   developed  robust regression regression involving correlated responses   time series  growth curves regression    predictor independent variable  response variables  curves images graphs   complex data objects regression methods accommodating various types  missing data nonparametric regression bayesian methods  regression regression    predictor variables  measured  error regression   predictor variables  observations  causal inference  regression regression models regression models involve  following variables  various fields  application different terminologies  used  place  dependent  independent variables  regression model relates y   function  x   approximation  usually formalized  ey x fx  carry  regression analysis  form   function f must  specified sometimes  form   function  based  knowledge   relationship  y  x    rely   data    knowledge  available  flexible  convenient form  f  chosen assume now   vector  unknown parameters   length k  order  perform  regression analysis  user must provide information   dependent variable y   last case  regression analysis provides  tools  necessary number  independent measurements consider  regression model   three unknown parameters  suppose  experimenter performs measurements   exactly   value  independent variable vector x  contains  independent variables x x  x   case regression analysis fails  give  unique set  estimated values   three unknown parameters  experimenter   provide enough information  best one can    estimate  average value   standard deviation   dependent variable y similarly measuring  two different values  x  give enough data   regression  two unknowns    three   unknowns   experimenter  performed measurements  three different values   independent variable vector x  regression analysis  provide  unique set  estimates   three unknown parameters    case  general linear regression   statement  equivalent   requirement   matrix xx  invertible statistical assumptions   number  measurements n  larger   number  unknown parameters k   measurement errors  normally distributed   excess  information contained  n k measurements  used  make statistical predictions   unknown parameters  excess  information  referred    degrees  dom   regression underlying assumptions classical assumptions  regression analysis include   sufficient conditions   leastsquares estimator  possess desirable properties  particular  assumptions imply   parameter estimates will  unbiased consistent  efficient   class  linear unbiased estimators   important  note  actual data rarely satisfies  assumptions    method  used even though  assumptions   true variation   assumptions can sometimes  used   measure   far  model    useful many   assumptions may  relaxed   advanced treatments reports  statistical analyses usually include analyses  tests   sample data  methodology   fit  usefulness   model assumptions include  geometrical support   variables independent  dependent variables often refer  values measured  point locations  may  spatial trends  spatial autocorrelation   variables  violate statistical assumptions  regression geographic weighted regression  one technique  deal   data also variables may include values aggregated  areas  aggregated data  modifiable areal unit problem can cause extreme variation  regression parameters  analyzing data aggregated  political boundaries postal codes  census areas results may   distinct   different choice  units linear regression  linear regression  model specification    dependent variable formula   linear combination   parameters  need   linear   independent variables  example  simple linear regression  modeling formula data points   one independent variable formula  two parameters formula  formula  multiple linear regression   several independent variables  functions  independent variables adding  term  x   preceding regression gives   still linear regression although  expression   right hand side  quadratic   independent variable formula   linear   parameters formula formula  formula   cases formula   error term   subscript formula indexes  particular observation returning  attention   straight line case given  random sample   population  estimate  population parameters  obtain  sample linear regression model  residual formula   difference   value   dependent variable predicted   model formula   true value   dependent variable formula one method  estimation  ordinary least squares  method obtains parameter estimates  minimize  sum  squared residuals sse also sometimes denoted rss minimization   function results   set  normal equations  set  simultaneous linear equations   parameters   solved  yield  parameter estimators formula   case  simple regression  formulas   least squares estimates   formula   mean average   formula values  formula   mean   formula values   assumption   population error term   constant variance  estimate   variance  given    called  mean square error mse   regression  denominator   sample size reduced   number  model parameters estimated    data np  p regressors  np   intercept  used   case p   denominator  n  standard errors   parameter estimates  given     assumption   population error term  normally distributed  researcher can use  estimated standard errors  create confidence intervals  conduct hypothesis tests   population parameters general linear model    general multiple regression model   p independent variables  x    observation   j independent variable   first independent variable takes  value    x  formula  called  regression intercept  least squares parameter estimates  obtained  p normal equations  residual can  written   normal equations   matrix notation  normal equations  written    ij element  x  x   element   column vector y  y   j element  formula  formula thus x  np y  n  formula  p  solution  diagnostics   regression model   constructed  may  important  confirm  goodness  fit   model   statistical significance   estimated parameters commonly used checks  goodness  fit include  rsquared analyses   pattern  residuals  hypothesis testing statistical significance can  checked   ftest   overall fit followed  ttests  individual parameters interpretations   diagnostic tests rest heavily   model assumptions although examination   residuals can  used  invalidate  model  results   ttest  ftest  sometimes  difficult  interpret   models assumptions  violated  example   error term     normal distribution  small samples  estimated parameters will  follow normal distributions  complicate inference  relatively large samples however  central limit theorem can  invoked   hypothesis testing may proceed using asymptotic approximations limited dependent variables  phrase limited dependent  used  econometric statistics  categorical  constrained variables  response variable may  noncontinuous limited  lie   subset   real line  binary zero  one variables  analysis proceeds  leastsquares linear regression  model  called  linear probability model nonlinear models  binary dependent variables include  probit  logit model  multivariate probit model   standard method  estimating  joint relationship  several binary dependent variables   independent variables  categorical variables    two values    multinomial logit  ordinal variables    two values    ordered logit  ordered probit models censored regression models may  used   dependent variable   sometimes observed  heckman correction type models may  used   sample   randomly selected   population  interest  alternative   procedures  linear regression based  polychoric correlation  polyserial correlations   categorical variables  procedures differ   assumptions made   distribution   variables   population   variable  positive  low values  represents  repetition   occurrence   event  count models like  poisson regression   negative binomial model may  used instead interpolation  extrapolation regression models predict  value   y variable given known values   x variables prediction within  range  values   dataset used  modelfitting  known informally  interpolation prediction outside  range   data  known  extrapolation performing extrapolation relies strongly   regression assumptions    extrapolation goes outside  data   room     model  fail due  differences   assumptions   sample data   true values   generally advised   performing extrapolation one  accompany  estimated value   dependent variable   prediction interval  represents  uncertainty  intervals tend  expand rapidly   values   independent variables moved outside  range covered   observed data   reasons  others  tend  say   might  unwise  undertake extrapolation however    cover  full set  modelling errors  may   made  particular  assumption   particular form   relation  y  x  properly conducted regression analysis will include  assessment   well  assumed form  matched   observed data   can    within  range  values   independent variables actually available  means   extrapolation  particularly reliant   assumptions  made   structural form   regression relationship bestpractice advice     linearinvariables  linearinparameters relationship    chosen simply  computational convenience    available knowledge   deployed  constructing  regression model   knowledge includes  fact   dependent variable  go outside  certain range  values  can  made use   selecting  model even   observed dataset   values particularly near  bounds  implications   step  choosing  appropriate functional form   regression can  great  extrapolation  considered   minimum  can ensure   extrapolation arising   fitted model  realistic   accord    known nonlinear regression   model function   linear   parameters  sum  squares must  minimized   iterative procedure  introduces many complications   summarized  differences  linear  nonlinear least squares power  sample size calculations    generally agreed methods  relating  number  observations versus  number  independent variables   model one rule  thumb suggested  good  hardin  formula  formula   sample size formula   number  independent variables  formula   number  observations needed  reach  desired precision   model   one independent variable  example  researcher  building  linear regression model using  dataset  contains patients formula   researcher decides  five observations  needed  precisely define  straight line formula   maximum number  independent variables  model can support   formula  methods although  parameters   regression model  usually estimated using  method  least squares  methods    used include software  major statistical software packages perform least squares regression analysis  inference simple linear regression  multiple regression using least squares can  done   spreadsheet applications    calculators  many statistical software packages can perform various types  nonparametric  robust regression  methods  less standardized different software packages implement different methods   method   given name may  implemented differently  different packages specialized regression software   developed  use  fields   survey analysis  neuroimaging\r\n"}
{"index":{"_id":6}}
{"conceptLabelTag":"affinity analysis","conceptLabel":"affinity analysis","conceptDescription":"affinity analysis affinity analysis   data analysis  data mining technique  discovers cooccurrence relationships among activities performed   recorded  specific individuals  groups  general  can  applied   process  agents can  uniquely identified  information   activities can  recorded  retail affinity analysis  used  perform market basket analysis   retailers seek  understand  purchase behavior  customers  information can   used  purposes  crossselling  upselling  addition  influencing sales promotions loyalty programs store design  discount plans examples market basket analysis might tell  retailer  customers often purchase shampoo  conditioner together  putting  items  promotion    time   create  significant increase  revenue   promotion involving just one   items  likely drive sales    market basket analysis may provide  retailer  information  understand  purchase behavior   buyer  information will enable  retailer  understand  buyers needs  rewrite  stores layout accordingly develop crosspromotional programs  even capture new buyers much like  crossselling concept  apocryphal early illustrative example     one super market chain discovered   analysis  male customers  bought diapers often bought beer  well  put  diapers close  beer coolers   sales increased dramatically although  urban legend    example  professors use  illustrate  concept  students  explanation   imaginary phenomenon might   fathers   sent   buy diapers often buy  beer  well   reward  kind  analysis  supposedly  example   use  data mining  widely used example  cross selling   web  market basket analysis  amazoncoms use  customers  bought book  also bought book b eg people  read history  portugal  also interested  naval history market basket analysis can  used  divide customers  groups  company  look    items people purchase along  eggs  classify   baking  cake    buying eggs along  flour  sugar  making omelets    buying eggs along  bacon  cheese  identification    used  drive  programs similarly  can  used  divide products  natural groups  company  look   products   frequently sold together  align  category management around  cliques business use business use  market basket analysis  significantly increased since  introduction  electronic point  sale amazon uses affinity analysis  crossselling   recommends products  people based   purchase history   purchase history   people  bought   item family dollar plans  use market basket analysis  help maintain sales growth  moving towards stocking  lowmargin consumable goods  common urban legend highlighting  unexpected insights  can  found involves  chain often incorrectly given  walmart discovering  beer  diapers  often purchased together  responding    moving  beer closer   diapers  drive sales however   relationship seems    noted   unclear whether  action  taken  promote selling  together\r\n"}
{"index":{"_id":7}}
{"conceptLabelTag":"biclustering","conceptLabel":"biclustering","conceptDescription":"biclustering biclustering block clustering coclustering  twomode clustering  term  first introduced  mirkin although  technique  originally introduced much earlier ie  ja hartigan given  set  formula rows  formula columns ie  formula matrix  biclustering algorithm generates biclusters  subset  rows  exhibit similar behavior across  subset  columns  vice versa development biclustering  originally introduced  j  hartigan   term biclustering  later used  mirkin  algorithm   generalized   y cheng  g m church proposed  biclustering algorithm based  variance  applied   biological gene expression data  paper  still   important literature   gene expression biclustering field   isdhillon put forward two algorithms applying biclustering  files  words one version  based  bipartite spectral graph partitioning    based  information theory dhillon assumed  loss  mutual information  biclustering  equal   klkullbackleiblerdistance  p  q p means  distribution  files  feature words  biclustering q   distribution  biclustering kldistance   measuring  difference  two random distributions kl   two distributions     kl increases   difference increases thus  aim   algorithm   find  minimum kldistance  p  q  abanerjee used  weightedbregman distance instead  kldistance  design  biclustering algorithm   suitable   kind  matrix unlike  kldistance algorithm  cluster   two types  objects  bekkerman expanded  mutual information  dhillons theorem   single pair  multiple pairs complexity  complexity   biclustering problem depends   exact problem formulation  particularly   merit function used  evaluate  quality   given bicluster however  interesting variants   problem  npcomplete npcomplete  two conditions   simple case     element  either    binary matrix   bicluster  equal   biclique   corresponding bipartite graph  maximum size bicluster  equivalent  maximum edge biclique  bipartite graph   complex case  element  matrix   used  compute  quality   given bicluster  solve   restricted version   problem  requires either large computational effort   use  lossy heuristics  shortcircuit  calculation type  bicluster different biclustering algorithms  different definitions  bicluster   bicluster  constant values   biclustering algorithm tries  find  constant bicluster  normal way     reorder  rows  columns   matrix   can group together similar rowscolumns  find biclusters  similar values  method  ok   data  tidy    data can  noisy    times   cant satisfy us  sophisticated methods   used  perfect constant bicluster   matrixij   values aij  equal   real data aij can  seen  nij  nij   noise according  hartigans algorithm  splitting  original data matrix   set  biclusters variance  used  compute constant biclusters   perfect bicluster   matrix  variance zero also  order  prevent  partitioning   data matrix  biclusters   one row  one column hartigan assumes    k biclusters within  data matrix   data matrix  partitioned  k biclusters  algorithm ends biclusters  constant values  rows  columns  kind  biclusters cant  evaluated just  variance   values  finish  identification  columns   rows   normalized  first    algorithms without normalization step can find biclusters  rows  columns  different approaches biclusters  coherent values  biclusters  coherent values  rows  columns  overall improvement   algorithms  biclusters  constant values  rows   columns   considered  means  sophisticated algorithm  needed  algorithm may contain analysis  variance  groups using covariance   rows  columns  cheng  churchs theorem  bicluster  defined   subset  rows  columns  almost   score  similarity score  used  measure  coherence  rows  columns  relationship   cluster models   types  clustering   correlation clustering  discussed  algorithms   many biclustering algorithms developed  bioinformatics including block clustering ctwc coupled twoway clustering itwc interrelated twoway clustering bicluster pcluster pattern floc opc plaid model opsms orderpreserving submatrixes gibbs samba statisticalalgorithmic method  bicluster analysis robust biclustering algorithm roba crossing minimization cmonkey prms dcc leb localize  extract biclusters qubic qualitative biclustering bcca bicorrelation clustering algorithm bimax isa  fabia factor analysis  bicluster acquisition biclustering algorithms  also  proposed  used   application fields   names coclustering bidimensional clustering  subspace clustering given  known importance  discovering local patterns  timeseries data recent proposals  addressed  biclustering problem   specific case  time series gene expression data   case  interesting biclusters can  restricted    contiguous columns  restriction leads   tractable problem  enables  development  efficient exhaustive enumeration algorithms   cccbiclustering  ecccbiclustering  approximate patterns  cccbiclustering algorithms allow  given number  errors per gene relatively   expression profile representing  expression pattern   bicluster  ecccbiclustering algorithm uses approximate expressions  find  report  maximal cccbiclusters   discretized matrix   efficient string processing techniques  algorithms nd  report  maximal biclusters  coherent  contiguous columns  perfectapproximate expression patterns  time linearpolynomial   obtained  manipulating  discretized version  original expression matrix   size   time series gene expression matrix using ecient string processing techniques based  suffix trees  algorithms  also applied  solve problems  sketch  analysis  computational complexity  recent algorithms  attempted  include additional support  biclustering rectangular matrices   form   datatypes including cmonkey    ongoing debate    judge  results   methods  biclustering allows overlap  clusters   algorithms allow  exclusion  hardtoreconcile columnsconditions     available algorithms  deterministic   analyst must pay attention   degree   results represent stable minima     unsupervised classification problem  lack   gold standard makes  difficult  spot errors   results one approach   utilize multiple biclustering algorithms  majority  supermajority voting amongst  deciding  best result another way   analyse  quality  shifting  scaling patterns  biclusters biclustering   used   domain  text mining  classification    popularly known  coclustering text corpora  represented   vectorial form   matrix d whose rows denote  documents  whose columns denote  words   dictionary matrix elements d denote occurrence  word j  document  coclustering algorithms   applied  discover blocks  d  correspond   group  documents rows characterized   group  wordscolumns test clustering can solve  highdimensional sparse problem  means clustering text  words    time  clustering text  need  think     words information  also  information  words clusters   composed  words  according  similarity  feature words   text will eventually cluster  feature words   called coclustering   two advantages  coclustering one  clustering  test based  words clusters can extremely decrease  dimension  clustering  can also appropriate  measure  distance   tests second  mining  useful information  can get  corresponding information  test clusters  words clusters  corresponding information can  used  describe  type  texts  words    time  result  words clustering can  also used  text mining  information retrieval several approaches   proposed based   information contents   resulting blocks matrixbased approaches   svd  bvd  graphbased approaches informationtheoretic algorithms iteratively assign  row   cluster  documents   column   cluster  words    mutual information  maximized matrixbased methods focus   decomposition  matrices  blocks    error   original matrix   regenerated matrices   decomposition  minimized graphbased methods tend  minimize  cuts   clusters given two groups  documents d  d  number  cuts can  measured   number  words  occur  documents  groups d  d  recently bisson  hussain  proposed  new approach  using  similarity  words   similarity  documents  cocluster  matrix  method known  sim  cross similarity  based  finding documentdocument similarity  wordword similarity   using classical clustering methods   hierarchical clustering instead  explicitly clustering rows  columns alternately  consider higherorder occurrences  words inherently taking  account  documents    occur thus  similarity  two words  calculated based   documents    occur  also  documents   similar words occur  idea    two documents    topic   necessarily use   set  words  describe    subset   words   similar words   characteristic   topic  approach  taking higherorder similarities takes  latent semantic structure   whole corpus  consideration   result  generating  better clustering   documents  words  text databases   document collection defined   document  term d matrix  size m  n m number  documents n number  terms  covercoefficient based clustering methodology yields   number  clusters   documents  terms words using  doublestage probability experiment according   cover coefficient concept number  clusters can also  roughly estimated   following formula formula  t   number  nonzero entries  d note   d  row   column must contain  least one nonzero element  contrast   approaches fabia   multiplicative model  assumes realistic nongaussian signal distributions  heavy tails fabia utilizes well understood model selection techniques like variational approaches  applies  bayesian framework  generative framework allows fabia  determine  information content   bicluster  separate spurious biclusters  true biclusters\r\n"}
{"index":{"_id":8}}
{"conceptLabelTag":"rule induction","conceptLabel":"rule induction","conceptDescription":"rule induction rule induction   area  machine learning   formal rules  extracted   set  observations  rules extracted may represent  full scientific model   data  merely represent local patterns   data paradigms  major rule induction paradigms  algorithms  rule induction algorithms \r\n"}
{"index":{"_id":9}}
{"conceptLabelTag":"speech recognition","conceptLabel":"speech recognition","conceptDescription":"speech recognition speech recognition sr   interdisciplinary subfield  computational linguistics  develops methodologies  technologies  enables  recognition  translation  spoken language  text  computers   also known  automatic speech recognition asr computer speech recognition  just speech  text stt  incorporates knowledge  research   linguistics computer science  electrical engineering fields  sr systems use training also called enrollment   individual speaker reads text  isolated vocabulary   system  system analyzes  persons specific voice  uses   finetune  recognition   persons speech resulting  increased accuracy systems    use training  called speaker independent systems systems  use training  called speaker dependent speech recognition applications include voice user interfaces   voice dialing eg call home call routing eg   like  make  collect call domotic appliance control search eg find  podcast  particular words  spoken simple data entry eg entering  credit card number preparation  structured documents eg  radiology report speechtotext processing eg word processors  emails  aircraft usually termed direct voice input  term voice recognition  speaker identification refers  identifying  speaker rather     saying recognizing  speaker can simplify  task  translating speech  systems    trained   specific persons voice   can  used  authenticate  verify  identity   speaker  part   security process   technology perspective speech recognition   long history  several waves  major innovations  recently  field  benefited  advances  deep learning  big data  advances  evidenced     surge  academic papers published   field   importantly   worldwide industry adoption   variety  deep learning methods  designing  deploying speech recognition systems  speech industry players include google microsoft ibm baidu apple amazon nuance soundhound iflytek cdac many    publicized  core technology   speech recognition systems   based  deep learning history  early  bell labs researchers like harvey fletcher  investigating  science  speech perception  three bell labs researchers built  system  singlespeaker digit recognition  system worked  locating  formants   power spectrum   utterance  s era technology  limited  singlespeaker systems  vocabularies  around ten words gunnar fant developed  sourcefilter model  speech production  published    proved    useful model  speech production unfortunately funding  bell labs dried   several years    influential john pierce wrote  open letter   critical  speech recognition research pierces letter stated    strong reasons  believing  spoken english   general  recognizable phoneme  phoneme  word  word pierce defunded speech recognition research  bell labs   research  speech recognition  done  pierce retired  james l flanagan took  raj reddy   first person  take  continuous speech recognition   graduate student  stanford university   late s previous systems required  users  make  pause   word reddys system  designed  issue spoken commands   game  chess also around  time soviet researchers invented  dynamic time warping dtw algorithm  used   create  recognizer capable  operating   word vocabulary  dtw algorithm processed  speech signal  dividing   short frames eg ms segments  processing  frame   single unit although dtw   superseded  later algorithms  technique  dividing  signal  frames  carry  achieving speaker independence   major unsolved goal  researchers   time period  darpa funded five years  speech recognition research   speech understanding research program  ambitious end goals including  minimum vocabulary size  words bbn ibm carnegie mellon  stanford research institute  participated   program  government funding revived speech recognition research    largely abandoned   united states  john pierces letter despite  fact  cmus harpy system met  goals established   outset   program many   predictions turned    nothing   hype disappointing darpa administrators  disappointment led  darpa  continuing  funding several innovations happened   time    invention  beam search  use  cmus harpy system  field also benefited   discovery  several algorithms   fields   linear predictive coding  cepstral analysis   late s leonard baum developed  mathematics  markov chains   institute  defense analysis  cmu raj reddys students james baker  janet baker began using  hidden markov model hmm  speech recognition james baker  learned  hmms   summer job   institute  defense analysis   undergraduate education  use  hmms allowed researchers  combine different sources  knowledge   acoustics language  syntax   unified probabilistic model  fred jelineks lead ibm created  voice activated typewriter called tangora   handle  word vocabulary   mid s jelineks statistical approach put less emphasis  emulating  way  human brain processes  understands speech  favor  using statistical modeling techniques like hmms jelineks group independently discovered  application  hmms  speech   controversial  linguists since hmms   simplistic  account  many common features  human languages however  hmm proved    highly useful way  modeling speech  replaced dynamic time warping  become  dominant speech recognition algorithm   s ibm    competitors including dragon systems founded  james  janet baker   s also saw  introduction   ngram language model katz introduced  backoff model   allowed language models  use multiple length ngrams    time also cselt  using hmm  diphonies  studied since  recognize language like italian    time cselt led  series  european projects esprit  ii  summarized  stateoftheart   book later reprinted much   progress   field  owed   rapidly increasing capabilities  computers   end   darpa program   best computer available  researchers   pdp  mb ram using  computers   take   minutes  decode just seconds  speech   decades later researchers  access  tens  thousands  times  much computing power   technology advanced  computers got faster researchers began tackling harder problems   larger vocabularies speaker independence noisy environments  conversational speech  particular  shifting   difficult tasks  characterized darpa funding  speech recognition since  s  example progress  made  speaker independence first  training   larger variety  speakers   later   explicit speaker adaptation  decoding  reductions  word error rate came  researchers shifted acoustic models   discriminative instead  using maximum likelihood models another one  raj reddys former students xuedong huang developed  sphinxii system  cmu  sphinxii system   first   speakerindependent large vocabulary continuous speech recognition     best performance  darpas evaluation huang went   found  speech recognition group  microsoft   s saw  first introduction  commercially successful speech recognition technologies   point  vocabulary   typical commercial speech recognition system  larger   average human vocabulary  lernout hauspie acquired dragon systems    industry leader   accounting scandal brought  end   company   lh speech technology  bought  scansoft  became nuance  apple originally licensed software  nuance  provide speech recognition capability   digital assistant siri st century   s darpa sponsored two speech recognition programs effective affordable reusable speechtotext ears   global autonomous language exploitation gale four teams participated   ears program ibm  team led  bbn  limsi  univ  pittsburgh cambridge university   team composed  isci sri  university  washington  gale program focused  arabic  mandarin broadcast news speech googles first effort  speech recognition came   hiring  researchers  nuance  first product  goog  telephone based directory service  recordings  goog produced valuable data  helped google improve  recognition systems google voice search  now supported   languages   united states  national security agency  made use   type  speech recognition  keyword spotting since  least  technology allows analysts  search  large volumes  recorded conversations  isolate mentions  keywords recordings can  indexed  analysts can run queries   database  find conversations  interest  government research programs focused  intelligence applications  speech recognition eg darpas earss program  iarpas babel program   early s speech recognition  still dominated  traditional approaches   hidden markov models combined  feedforward artificial neural networks today however many aspects  speech recognition   taken    deep learning method called long shortterm memory lstm  recurrent neural network published  sepp hochreiter j rgen schmidhuber  lstm rnns avoid  vanishing gradient problem  can learn  deep learning tasks  require memories  events  happened thousands  discrete time steps ago   important  speech around lstm trained  connectionist temporal classification ctc started  outperform traditional speech recognition  certain applications  googles speech recognition reportedly experienced  dramatic performance    ctctrained lstm   now available  google voice   smartphone users  use  deep feedforward nonrecurrent networks  acoustic modeling  introduced  later part   geoffrey hinton   students  university  toronto   li deng  colleagues  microsoft research initially   collaborative work  microsoft  university  toronto   subsequently expanded  include ibm  google hence  shared views  four research groups subtitle   review paper  microsoft research executive called  innovation   dramatic change  accuracy since  contrast   steady incremental improvements   past  decades  application  deep learning decreased word error rate   innovation  quickly adopted across  field researchers  begun  use deep learning techniques  language modeling  well   long history  speech recognition  shallow form  deep form eg recurrent nets  artificial neural networks   explored  many years  s s    years   s   methods never won   nonuniform internalhandcrafting gaussian mixture modelhidden markov model gmmhmm technology based  generative models  speech trained discriminatively  number  key difficulties   methodologically analyzed   s including gradient diminishing  weak temporal correlation structure   neural predictive models   difficulties   addition   lack  big training data  big computing power   early days  speech recognition researchers  understood  barriers hence subsequently moved away  neural nets  pursue generative modeling approaches   recent resurgence  deep learning starting around   overcome   difficulties hinton et al  deng et al reviewed part   recent history    collaboration       colleagues across four groups university  toronto microsoft google  ibm ignited  renaissance  applications  deep feedforward neural networks  speech recognition models methods  algorithms  acoustic modeling  language modeling  important parts  modern statisticallybased speech recognition algorithms hidden markov models hmms  widely used  many systems language modeling  also used  many  natural language processing applications   document classification  statistical machine translation hidden markov models modern generalpurpose speech recognition systems  based  hidden markov models   statistical models  output  sequence  symbols  quantities hmms  used  speech recognition   speech signal can  viewed   piecewise stationary signal   shorttime stationary signal   short timescale eg milliseconds speech can  approximated   stationary process speech can  thought    markov model  many stochastic purposes another reason  hmms  popular    can  trained automatically   simple  computationally feasible  use  speech recognition  hidden markov model  output  sequence  ndimensional realvalued vectors  n   small integer   outputting one   every milliseconds  vectors  consist  cepstral coefficients   obtained  taking  fourier transform   short time window  speech  decorrelating  spectrum using  cosine transform  taking  first  significant coefficients  hidden markov model will tend     state  statistical distribution    mixture  diagonal covariance gaussians  will give  likelihood   observed vector  word    general speech recognition systems  phoneme will   different output distribution  hidden markov model   sequence  words  phonemes  made  concatenating  individual trained hidden markov models   separate words  phonemes described    core elements    common hmmbased approach  speech recognition modern speech recognition systems use various combinations   number  standard techniques  order  improve results   basic approach described   typical largevocabulary system  need context dependency   phonemes  phonemes  different left  right context  different realizations  hmm states   use cepstral normalization  normalize  different speaker  recording conditions   speaker normalization  might use vocal tract length normalization vtln  malefemale normalization  maximum likelihood linear regression mllr   general speaker adaptation  features   socalled delta  deltadelta coefficients  capture speech dynamics   addition might use heteroscedastic linear discriminant analysis hlda  might skip  delta  deltadelta coefficients  use splicing   ldabased projection followed perhaps  heteroscedastic linear discriminant analysis   global semitied co variance transform also known  maximum likelihood linear transform  mllt many systems use socalled discriminative training techniques  dispense   purely statistical approach  hmm parameter estimation  instead optimize  classificationrelated measure   training data examples  maximum mutual information mmi minimum classification error mce  minimum phone error mpe decoding   speech  term   happens   system  presented   new utterance  must compute   likely source sentence  probably use  viterbi algorithm  find  best path      choice  dynamically creating  combination hidden markov model  includes   acoustic  language model information  combining  statically beforehand  finite state transducer  fst approach  possible improvement  decoding   keep  set  good candidates instead  just keeping  best candidate   use  better scoring function re scoring  rate  good candidates    may pick  best one according   refined score  set  candidates can  kept either   list  nbest list approach    subset   models  lattice re scoring  usually done  trying  minimize  bayes risk   approximation thereof instead  taking  source sentence  maximal probability  try  take  sentence  minimizes  expectancy   given loss function  regards   possible transcriptions ie  take  sentence  minimizes  average distance   possible sentences weighted   estimated probability  loss function  usually  levenshtein distance though  can  different distances  specific tasks  set  possible transcriptions   course pruned  maintain tractability efficient algorithms   devised  re score lattices represented  weighted finite state transducers  edit distances represented    finite state transducer verifying certain assumptions dynamic time warping dtwbased speech recognition dynamic time warping   approach   historically used  speech recognition   now largely  displaced    successful hmmbased approach dynamic time warping   algorithm  measuring similarity  two sequences  may vary  time  speed  instance similarities  walking patterns   detected even   one video  person  walking slowly    another     walking  quickly  even    accelerations  deceleration   course  one observation dtw   applied  video audio  graphics indeed  data  can  turned   linear representation can  analyzed  dtw  wellknown application   automatic speech recognition  cope  different speaking speeds  general    method  allows  computer  find  optimal match  two given sequences eg time series  certain restrictions    sequences  warped nonlinearly  match    sequence alignment method  often used   context  hidden markov models neural networks neural networks emerged   attractive acoustic modeling approach  asr   late s since  neural networks   used  many aspects  speech recognition   phoneme classification isolated word recognition  speaker adaptation  contrast  hmms neural networks make  assumptions  feature statistical properties   several qualities making  attractive recognition models  speech recognition  used  estimate  probabilities   speech feature segment neural networks allow discriminative training   natural  efficient manner  assumptions   statistics  input features  made  neural networks however  spite   effectiveness  classifying shorttime units   individual phones  isolated words neural networks  rarely successful  continuous recognition tasks largely    lack  ability  model temporal dependencies however recently lstm recurrent neural networks rnns  time delay neural networkstdnns   used    shown   able  identify latent temporal dependencies  use  information  perform  task  speech recognition deep neural networks  denoising autoencoders  also  experimented   tackle  problem   effective manner due   inability  feedforward neural networks  model temporal dependencies  alternative approach   use neural networks   preprocessing eg feature transformation dimensionality reduction   hmm based recognition deep feedforward  recurrent neural networks  deep feedforward neural network dnn   artificial neural network  multiple hidden layers  units   input  output layers similar  shallow neural networks dnns can model complex nonlinear relationships dnn architectures generate compositional models  extra layers enable composition  features  lower layers giving  huge learning capacity  thus  potential  modeling complex patterns  speech data  success  dnns  large vocabulary speech recognition occurred   industrial researchers  collaboration  academic researchers  large output layers   dnn based  context dependent hmm states constructed  decision trees  adopted recent overview articles one fundamental principle  deep learning    away  handcrafted feature engineering   use raw features  principle  first explored successfully   architecture  deep autoencoder   raw spectrogram  linear filterbank features showing  superiority   melcepstral features  contain   stages  fixed transformation  spectrograms  true raw features  speech waveforms   recently  shown  produce excellent largerscale speech recognition results endtoend automatic speech recognition since    much research interest  endtoend asr traditional phoneticbased ie  hmmbased model approaches required separate components  training   pronunciation acoustic  language model endtoend models jointly learn   components   speech recognizer   valuable since  simplifies  training process  deployment process  example  ngram language model  required   hmmbased systems   typical ngram language model often takes several gigabytes  memory making  impractical  deploy  mobile devices consequently modern commercial asr systems  google  apple    deployed   cloud  require  network connection  opposed   device locally  first attempt  endtoend asr   connectionist temporal classification ctc based systems introduced  alex graves  google deepmind  navdeep jaitly   university  toronto   model consisted  recurrent neural networks   ctc layer jointly  rnnctc model learns  pronunciation  acoustic model together however   incapable  learning  language due  conditional independence assumptions similar   hmm consequently ctc models can directly learn  map speech acoustics  english characters   models make many common spelling mistakes  must rely   separate language model  clean   transcripts later baidu expanded   work  extremely large datasets  demonstrated  commercial success  chinese mandarin  english  alternative approach  ctcbased models  attentionbased models attentionbased asr models  introduced simultaneously  chan et al  carnegie mellon university  google brain  bahdanaua et al   university  montreal   model named listen attend  spell las literally listens   acoustic signal pays attention  different parts   signal  spells   transcript one character   time unlike ctcbased models attentionbased models    conditionalindependence assumptions  can learn   components   speech recognizer including  pronunciation acoustic  language model directly  means  deployment    need  carry around  language model making   practical  deployment onto applications  limited memory   end   attentionbased models  seen considerable success including outperforming  ctc models   without  external language model various extensions   proposed since  original las model latent sequence decompositions lsd  proposed  carnegie mellon university mit  google brain  directly emit subword units    natural  english characters university  oxford  google deepmind extended las  watch listen attend  spell wlas  handle lip reading  surpassing humanlevel performance   first time applications incar systems typically  manual control input  example  means   finger control   steeringwheel enables  speech recognition system    signalled   driver   audio prompt following  audio prompt  system   listening window    may accept  speech input  recognition simple voice commands may  used  initiate phone calls select radio stations  play music   compatible smartphone mp player  musicloaded flash drive voice recognition capabilities vary  car make  model     recent car models offer naturallanguage speech recognition  place   fixed set  commands allowing  driver  use full sentences  common phrases   systems   therefore  need   user  memorize  set  fixed command words health care medical documentation   health care sector speech recognition can  implemented  frontend  backend   medical documentation process frontend speech recognition    provider dictates   speechrecognition engine  recognized words  displayed    spoken   dictator  responsible  editing  signing    document backend  deferred speech recognition    provider dictates   digital dictation system  voice  routed   speechrecognition machine   recognized draft document  routed along   original voice file   editor   draft  edited  report finalized deferred speech recognition  widely used   industry currently one   major issues relating   use  speech recognition  healthcare    american recovery  reinvestment act  arra provides  substantial financial benefits  physicians  utilize  emr according  meaningful use standards  standards require   substantial amount  data  maintained   emr now  commonly referred    electronic health record  ehr  use  speech recognition   naturally suited   generation  narrative text  part   radiologypathology interpretation progress note  discharge summary  ergonomic gains  using speech recognition  enter structured discrete data eg numeric values  codes   list   controlled vocabulary  relatively minimal  people   sighted   can operate  keyboard  mouse   significant issue    ehrs    expressly tailored  take advantage  voicerecognition capabilities  large part   clinicians interaction   ehr involves    user interface using menus  tabbutton clicks   heavily dependent  keyboard  mouse voicebased  provides  modest ergonomic benefits  contrast many highly customized systems  radiology  pathology dictation implement voice macros   use  certain phrases eg normal report will automatically fill   large number  default values andor generate boilerplate  will vary   type   exam eg  chest xray vs  gastrointestinal contrast series   radiology system   alternative     hand cascaded use  speech recognition  information extraction   studied   way  fill   handover form  clinical proofing  signoff  results  encouraging   paper also opens data together   related performance benchmarks   processing software   research  development community  studying clinical documentation  languageprocessing therapeutic use prolonged use  speech recognition software  conjunction  word processors  shown benefits  shorttermmemory restrengthening  brain avm patients    treated  resection  research needs   conducted  determine cognitive benefits  individuals whose avms   treated using radiologic techniques military highperformance fighter aircraft substantial efforts   devoted   last decade   test  evaluation  speech recognition  fighter aircraft  particular note    us program  speech recognition   advanced fighter technology integration aftif aircraft f vista  program  france  mirage aircraft   programs   uk dealing   variety  aircraft platforms   programs speech recognizers   operated successfully  fighter aircraft  applications including setting radio frequencies commanding  autopilot system setting steerpoint coordinates  weapons release parameters  controlling flight display working  swedish pilots flying   jas gripen cockpit englund found recognition deteriorated  increasing gloads  report also concluded  adaptation greatly improved  results   cases    introduction  models  breathing  shown  improve recognition scores significantly contrary   might   expected  effects   broken english   speakers  found   evident  spontaneous speech caused problems   recognizer  might   expected  restricted vocabulary     proper syntax  thus  expected  improve recognition accuracy substantially  eurofighter typhoon currently  service   uk raf employs  speakerdependent system requiring  pilot  create  template  system   used   safetycritical  weaponcritical tasks   weapon release  lowering   undercarriage   used   wide range   cockpit functions voice commands  confirmed  visual andor aural feedback  system  seen   major design feature   reduction  pilot workload  even allows  pilot  assign targets   aircraft  two simple voice commands      wingmen   five commands speakerindependent systems  also  developed    test   f lightning ii jsf   alenia aermacchi m master leadin fighter trainer  systems  produced word accuracy scores  excess  helicopters  problems  achieving high recognition accuracy  stress  noise pertain strongly   helicopter environment  well    jet fighter environment  acoustic noise problem  actually  severe   helicopter environment      high noise levels  also   helicopter pilot  general   wear  facemask   reduce acoustic noise   microphone substantial test  evaluation programs   carried    past decade  speech recognition systems applications  helicopters notably   us army avionics research  development activity avrada    royal aerospace establishment rae   uk work  france  included speech recognition   puma helicopter   also  much useful work  canada results   encouraging  voice applications  included control  communication radios setting   systems  control   automated target handover system   fighter applications  overriding issue  voice  helicopters   impact  pilot effectiveness encouraging results  reported   avrada tests although  represent   feasibility demonstration   test environment much remains   done   speech recognition   overall speech technology  order  consistently achieve performance improvements  operational settings training air traffic controllers training  air traffic controllers atc represents  excellent application  speech recognition systems many atc training systems currently require  person  act   pseudopilot engaging   voice dialog   trainee controller  simulates  dialog   controller    conduct  pilots   real atc situation speech recognition  synthesis techniques offer  potential  eliminate  need   person  act  pseudopilot thus reducing training  support personnel  theory air controller tasks  also characterized  highly structured speech   primary output   controller hence reducing  difficulty   speech recognition task   possible  practice   rarely  case  faa document details  phrases    used  air traffic controllers   document gives less  examples   phrases  number  phrases supported  one   simulation vendors speech recognition systems   excess   usaf usmc us army us navy  faa  well   number  international atc training organizations    royal australian air force  civil aviation authorities  italy brazil  canada  currently using atc simulators  speech recognition   number  different vendors telephony   domain asr   field  telephony  now commonplace    field  computer gaming  simulation  becoming  widespread despite  high level  integration  word processing  general personal computing however asr   field  document production   seen  expected increases  use  improvement  mobile processor speeds made feasible  speechenabled symbian  windows mobile smartphones speech  used mostly   part   user interface  creating predefined  custom speech commands leading software vendors   field  google microsoft corporation microsoft voice command digital syphon sonic extractor lumenvox nuance communications nuance voice control voicebox technology speech technology center vito technologies vito voicego speereo software speereo voice translator verbyx vrx  svox usage  education  daily life  language learning speech recognition can  useful  learning  second language  can teach proper pronunciation  addition  helping  person develop fluency   speaking skills students   blind see blindness  education    low vision can benefit  using  technology  convey words   hear  computer recite   well  use  computer  commanding   voice instead    look   screen  keyboard students   physically disabled  suffer  repetitive strain injuryother injuries   upper extremities can  relieved    worry  handwriting typing  working  scribe  school assignments  using speechtotext programs  can also utilize speech recognition technology  ly enjoy searching  internet  using  computer  home without   physically operate  mouse  keyboard speech recognition can allow students  learning disabilities  become better writers  saying  words aloud  can increase  fluidity   writing   alleviated  concerns regarding spelling punctuation   mechanics  writing also see learning disability use  voice recognition software  conjunction   digital audio recorder   personal computer running wordprocessing software  proven   positive  restoring damaged shorttermmemory capacity  stroke  craniotomy individuals people  disabilities people  disabilities can benefit  speech recognition programs  individuals   deaf  hard  hearing speech recognition software  used  automatically generate  closedcaptioning  conversations   discussions  conference rooms classroom lectures andor religious services speech recognition  also  useful  people   difficulty using  hands ranging  mild repetitive stress injuries  involve disabilities  preclude using conventional computer input devices  fact people  used  keyboard  lot  developed rsi became  urgent early market  speech recognition speech recognition  used  deaf telephony   voicemail  text relay services  captioned telephone individuals  learning disabilities   problems  thoughttopaper communication essentially  think   idea    processed incorrectly causing   end  differently  paper can possibly benefit   software   technology   bug proof also  whole idea  speak  text can  hard  intellectually disabled persons due   fact    rare  anyone tries  learn  technology  teach  person   disability  type  technology can help   dyslexia   disabilities  still  question  effectiveness   product   problem   hindering   effective although  kid may  able  say  word depending   clear  say   technology may think   saying another word  input  wrong one giving   work  fix causing     take  time  fixing  wrong word performance  performance  speech recognition systems  usually evaluated  terms  accuracy  speed accuracy  usually rated  word error rate wer whereas speed  measured   real time factor  measures  accuracy include single word error rate swer  command success rate csr however speech recognition   machine    complex problem vocalizations vary  terms  accent pronunciation articulation roughness nasality pitch volume  speed speech  distorted   background noise  echoes electrical characteristics accuracy  speech recognition vary   following accuracy  mentioned earlier   article accuracy  speech recognition varies   following eg  digits zero  nine can  recognized essentially perfectly  vocabulary sizes   may  error rates   respectively eg  letters   english alphabet  difficult  discriminate    confusable words  notoriously  eset b c d e g p t v z  error rate  considered good   vocabulary  speakerdependent system  intended  use   single speaker  speakerindependent system  intended  use   speaker  difficult  isolated speech single words  used therefore  becomes easier  recognize  speech  discontinuous speech full sentences separated  silence  used therefore  becomes easier  recognize  speech  well   isolated speech  continuous speech naturally spoken sentences  used therefore  becomes harder  recognize  speech different   isolated  discontinuous speech eg querying application may dismiss  hypothesis  apple  red eg constraints may  semantic rejecting  apple  angry eg syntactic rejecting red  apple  constraints  often represented   grammar   person reads  usually   context    previously prepared    person uses spontaneous speech   difficult  recognize  speech    disfluencies like uh  um false starts incomplete sentences stuttering coughing  laughter  limited vocabulary environmental noise eg noise   car   factory acoustical distortions eg echoes room acoustics speech recognition   multilevelled pattern recognition task eg phonemes words phrases  sentences eg known word pronunciations  legal word sequences  can compensate  errors  uncertainties  lower level  combining decisions probabilistically   lower levels  making  deterministic decisions    highest level speech recognition   machine   process broken  several phases computationally    problem    sound pattern    recognized  classified   category  represents  meaning   human every acoustic signal can  broken  smaller  basic subsignals    complex sound signal  broken   smaller subsounds different levels  created    top level   complex sounds   made  simpler sounds  lower level  going  lower levels even   create  basic  shorter  simpler sounds  lowest level   sounds    fundamental  machine  check  simple   probabilistic rules   sound  represent   sounds  put together   complex sound  upper level  new set   deterministic rules  predict  new complex sound  represent   upper level   deterministic rule  figure   meaning  complex expressions  order  expand  knowledge  speech recognition  need  take   consideration neural networks   four steps  neural network approaches  telephone speech  sampling rate  samples per second computed every ms  one ms section called  frame analysis  fourstep neural network approaches can  explained   information sound  produced  air    medium vibration   register  ears  machines  receivers basic sound creates  wave   descriptions amplitude  strong    frequency  often  vibrates per second  sound waves can  digitized sample  strength  short intervals like  picture   get bunch  numbers  approximate   time step  strength   wave collection   numbers represent analog wave  new wave  digital sound waves  complicated   superimpose one  top    like  waves   way  create oddlooking waves  example    two waves  interact     can add   creates new oddlooking wave given basic sound blocks   machine digitized one   bunch  numbers  describe  wave  waves describe words  frame   unit block  sound   broken  basic sound waves  represented  numbers   fourier transform can  statistically evaluated  set   class  sounds  belongs  nodes   figure   slide represent  feature   sound    feature   wave   first layer  nodes   second layer  nodes based  statistical analysis  analysis depends  programmers instructions   point  second layer  nodes represents higher level features   sound input    statistically evaluated  see  class  belong  last level  nodes   output nodes  tell us  high probability  original sound really   information conferences  journals popular speech recognition conferences held  year  two include speechtek  speechtek europe icassp interspeecheurospeech   ieee asru conferences   field  natural language processing   acl naacl emnlp  hlt  beginning  include papers  speech processing important journals include  ieee transactions  speech  audio processing later renamed ieee transactions  audio speech  language processing  since sept renamed ieeeacm transactions  audio speech  language processing  merging   acm publication computer speech  language  speech communication books books like fundamentals  speech recognition  lawrence rabiner can  useful  acquire basic knowledge  may   fully   date another good source can  statistical methods  speech recognition  frederick jelinek  spoken language processing  xuedong huang etc    date  computer speech  manfred r schroeder second edition published   speech processing  dynamic  optimizationoriented approach published   li deng  doug oshaughnessey  recently updated textbook  speech  language processing  jurafsky  martin presents  basics   state   art  asr speaker recognition also uses   features     frontend processing  classification techniques   done  speech recognition   recent comprehensive textbook fundamentals  speaker recognition  homayoon beigi    depth source    date details   theory  practice  good insight   techniques used   best modern systems can  gained  paying attention  government sponsored evaluations    organised  darpa  largest speech recognitionrelated project ongoing     gale project  involves  speech recognition  translation components  good  accessible introduction  speech recognition technology   history  provided   general audience book  voice   machine building computers  understand speech  roberto pieraccini   recent book  speech recognition  automatic speech recognition  deep learning approach publisher springer written  d yu  l deng published near  end   highly mathematicallyoriented technical detail   deep learning methods  derived  implemented  modern speech recognition systems based  dnns  related deep learning methods  related book published earlier  deep learning methods  applications  l deng  d yu provides  less technical   methodologyfocused overview  dnnbased speech recognition  placed within   general context  deep learning applications including   speech recognition  also image recognition natural language processing information retrieval multimodal processing  multitask learning software  terms  ly available resources carnegie mellon universitys sphinx toolkit  one place  start   learn  speech recognition   start experimenting another resource   copyrighted   htk book   accompanying htk toolkit   recent  stateoftheart techniques kaldi toolkit can  used  demo   online speech recognizer  available  cobalts webpage   software resources see list  speech recognition software\r\n"}
{"index":{"_id":10}}
{"conceptLabelTag":"winnow algorithm","conceptLabel":"winnow algorithm","conceptDescription":"winnow algorithm  winnow algorithm   technique  machine learning  learning  linear classifier  labeled examples    similar   perceptron algorithm however  perceptron algorithm uses  additive weightupdate scheme  winnow uses  multiplicative scheme  allows   perform much better  many dimensions  irrelevant hence  name    simple algorithm  scales well  highdimensional data  training winnow  shown  sequence  positive  negative examples    learns  decision hyperplane  can   used  label novel examples  positive  negative  algorithm can also  used   online learning setting   learning   classification phase   clearly separated algorithm  basic algorithm winnow   follows  instance space  formula    instance  described   set  booleanvalued features  algorithm maintains nonnegative weights formula  formula   initially set  one weight   feature   learner  given  example formula  applies  typical prediction rule  linear classifiers  formula   real number   called  threshold together   weights  threshold defines  dividing hyperplane   instance space good bounds  obtained  formula see    example     presented  learner applies  following update rule formula formula  typical value  formula    many variations   basic approach winnow  similar except    demotion step  weights  divided  formula instead   set  balanced winnow maintains two sets  weights  thus two hyperplanes  can   generalized  multilabel classification mistake bounds  certain circumstances  can  shown   number  mistakes winnow makes   learns   upper bound   independent   number  instances     presented   winnow algorithm uses formula  formula   target function    formulaliteral monotone disjunction given  formula    sequence  instances  total number  mistakes  bounded  formula\r\n"}
{"index":{"_id":11}}
{"conceptLabelTag":"multiclass classification","conceptLabel":"multiclass classification","conceptDescription":"multiclass classification  machine learning multiclass  multinomial classification   problem  classifying instances  one     two classes classifying instances  one   two classes  called binary classification   classification algorithms naturally permit  use    two classes others   nature binary algorithms  can however  turned  multinomial classifiers   variety  strategies multiclass classification    confused  multilabel classification  multiple labels    predicted   instance general strategies  existing multiclass classification techniques can  categorized   transformation  binary ii extension  binary  iii hierarchical classification transformation  binary  section discusses strategies  reducing  problem  multiclass classification  multiple binary classification problems  can  categorized  one vs rest  one vs one  techniques developed based  reducing  multiclass problem  multiple binary problems can also  called  problem transformation techniques onevsrest  onevsrest  onevsall ova  ovr oneagainstall oaa strategy involves training  single classifier per class   samples   class  positive samples    samples  negatives  strategy requires  base classifiers  produce  realvalued confidence score   decision rather  just  class label discrete class labels alone can lead  ambiguities  multiple classes  predicted   single sample  pseudocode  training algorithm   ova learner constructed   binary classification learner   follows making decisions means applying  classifiers   unseen sample  predicting  label    corresponding classifier reports  highest confidence score although  strategy  popular    heuristic  suffers  several problems firstly  scale   confidence values may differ   binary classifiers second even   class distribution  balanced   training set  binary classification learners see unbalanced distributions  typically  set  negatives  see  much larger   set  positives onevsone   onevsone ovo reduction one trains binary classifiers   way multiclass problem  receives  samples   pair  classes   original training set  must learn  distinguish  two classes  prediction time  voting scheme  applied  classifiers  applied   unseen sample   class  got  highest number  predictions gets predicted   combined classifier like ovr ovo suffers  ambiguities    regions   input space may receive   number  votes extension  binary  section discusses strategies  extending  existing binary classifiers  solve multiclass classification problems several algorithms   developed based  neural networks decision trees knearest neighbors naive bayes support vector machines  extreme learning machines  address multiclass classification problems  types  techniques can also  called  algorithm adaptation techniques neural networks multilayer perceptron provide  natural extension   multiclass problem instead  just  one neuron   output layer  binary output    n binary neurons leading  multiclass classification extreme learning machines extreme learning machines elm   special case  single hidden layer feed forward neural networks slfns    input weights   hidden node biases can  chosen  random many variants  developments  made   elm  multiclass classification knearest neighbours knearest neighbors knn  considered among  oldest nonparametric classification algorithms  classify  unknown example  distance   example  every  training example  measured  k smallest distances  identified    represented class   k classes  considered  output class label naive bayes naive bayes   successful classifier based upon  principle  maximum  posteriori map  approach  naturally extensible   case     two classes   shown  perform well  spite   underlying simplifying assumption  conditional independence decision trees decision trees   powerful classification technique  tree tries  infer  split   training data based   values   available features  produce  good generalization  algorithm can naturally handle binary  multiclass classification problems  leaf nodes can refer  either   k classes concerned support vector machines support vector machines  based upon  idea  maximizing  margin ie maximizing  minimum distance   separating hyperplane   nearest example  basic svm supports  binary classification  extensions   proposed  handle  multiclass classification case  well   extensions additional parameters  constraints  added   optimization problem  handle  separation   different classes hierarchical classification hierarchical classification tackles  multiclass classification problem  dividing  output space ie   tree  parent node  divided  multiple child nodes   process  continued   child node represents  one class several methods   proposed based  hierarchical classification learning paradigms based  learning paradigms  existing multiclass classification techniques can  classified  batch learning  online learning batch learning algorithms require   data samples   available beforehand  trains  model using  entire training data   predicts  test sample using  found relationship  online learning algorithms    hand incrementally build  models  sequential iterations  iteration t  online algorithm receives  sample x  predicts  label using  current model  algorithm  receives y  true label  x  updates  model based   samplelabel pair x y recently  new learning paradigm called progressive learning technique   developed  progressive learning technique  capable    learning  new samples  also capable  learning new classes  data  yet retain  knowledge learnt thus far\r\n"}
{"index":{"_id":12}}
{"conceptLabelTag":"fuzzy clustering","conceptLabel":"fuzzy clustering","conceptDescription":"fuzzy clustering fuzzy clustering also referred   soft clustering   form  clustering    data point can belong    one cluster clustering  cluster analysis involves assigning data points  clusters also called buckets bins  classes  homogeneous classes   items    class  cluster   similar  possible  items belonging  different classes   dissimilar  possible clusters  identified via similarity measures  similarity measures include distance connectivity  intensity different similarity measures may  chosen based   data   application comparison  hard clustering  nonfuzzy clustering also known  hard clustering data  divided  distinct clusters   data point can  belong  exactly one cluster  fuzzy clustering data points can potentially belong  multiple clusters membership membership grades  assigned     data points  membership grades indicate  degree   data points belong   cluster thus points   edge   cluster  lower membership grades may    cluster   lesser degree  points   center  cluster fuzzy cmeans clustering one    widely used fuzzy clustering algorithms   fuzzy cmeans clustering fcm algorithm history fuzzy cmeans fcm clustering  developed  jc dunn   improved  jc bezdek  general description  fuzzy cmeans algorithm   similar   kmeans algorithm centroid  point x   set  coefficients giving  degree     kth cluster wx  fuzzy cmeans  centroid   cluster   mean   points weighted   degree  belonging   cluster formula algorithm  fcm algorithm attempts  partition  finite collection  formula elements formula   collection  c fuzzy clusters  respect   given criterion given  finite set  data  algorithm returns  list  formula cluster centres formula   partition matrix formula   element formula tells  degree   element formula belongs  cluster formula  fcm aims  minimize  objective function  comparison  kmeans clustering kmeans clustering also attempts  minimize  objective function shown   method differs   kmeans objective function   addition   membership values formula   fuzzifier formula  formula  fuzzifier formula determines  level  cluster fuzziness  large formula results  smaller membership values formula  hence fuzzier clusters   limit formula  memberships formula converge    implies  crisp partitioning   absence  experimentation  domain knowledge formula  commonly set   algorithm minimizes intracluster variance  well     problems  kmeans  minimum   local minimum   results depend   initial choice  weights related algorithms using  mixture  gaussians along   expectationmaximization algorithm    statistically formalized method  includes    ideas partial membership  classes another algorithm closely related  fuzzy cmeans  soft kmeans applications clustering problems  applications  biology medicine psychology economics  many  disciplines bioinformatics   field  bioinformatics clustering  used   number  applications one use    pattern recognition technique  analyze gene expression data  microarrays   technology   case genes  similar expression patterns  grouped    cluster  different clusters display distinct wellseparated patterns  expression use  clustering can provide insight  gene function  regulation  fuzzy clustering allows genes  belong    one cluster  allows   identification  genes   conditionally coregulated  coexpressed  example one gene may  acted     one transcription factor  one gene may encode  protein     one function thus fuzzy clustering   appropriate  hard clustering image analysis fuzzy cmeans     important tool  image processing  clustering objects   image   s mathematicians introduced  spatial term   fcm algorithm  improve  accuracy  clustering  noise alternatively  fuzzy logic model can  described  fuzzy sets   defined  three components   hsl color space hsl  hsv  membership functions aim  describe colors follow  human intuition  color identification marketing  marketing customers can  grouped  fuzzy clusters based   needs brand choices psychographic profiles   marketing related partitions\r\n"}
{"index":{"_id":13}}
{"conceptLabelTag":"logistic regression","conceptLabel":"logistic regression","conceptDescription":"logistic regression  statistics logistic regression  logit regression  logit model   regression model   dependent variable dv  categorical  article covers  case   binary dependent variablethat    can take  two values   represent outcomes   passfail winlose alivedead  healthysick cases   dependent variable    two outcome categories may  analysed  multinomial logistic regression    multiple categories  ordered  ordinal logistic regression   terminology  economics logistic regression   example   qualitative responsediscrete choice model logistic regression  developed  statistician david cox   binary logistic model  used  estimate  probability   binary response based  one   predictor  independent variables features  allows one  say   presence   risk factor increases  probability   given outcome   specific percentage fields  example applications logistic regression  used  various fields including machine learning  medical fields  social sciences  example  trauma  injury severity score triss   widely used  predict mortality  injured patients  originally developed  boyd et al using logistic regression many  medical scales used  assess severity   patient   developed using logistic regression logistic regression may  used  predict whether  patient   given disease eg diabetes coronary heart disease based  observed characteristics   patient age sex body mass index results  various blood tests etc another example might   predict whether  american voter will vote democratic  republican based  age income sex race state  residence votes  previous elections etc  technique can also  used  engineering especially  predicting  probability  failure   given process system  product   also used  marketing applications   prediction   customers propensity  purchase  product  halt  subscription etc  economics  can  used  predict  likelihood   persons choosing     labor force   business application    predict  likelihood   homeowner defaulting   mortgage conditional random fields  extension  logistic regression  sequential data  used  natural language processing example probability  passing  exam versus hours  study  reason  using logistic regression   problem    dependent variable passfail represented     cardinal numbers   problem  changed   passfail  replaced   grade cardinal numbers  simple regression analysis   used  table shows  number  hours  student spent studying  whether  passed  failed  graph shows  probability  passing  exam versus  number  hours studying   logistic regression curve fitted   data  logistic regression analysis gives  following output  output indicates  hours studying  significantly associated   probability  passing  exam p wald test  output also provides  coefficients  intercept  hours  coefficients  entered   logistic regression equation  estimate  probability  passing  exam  example   student  studies hours entering  value hours   equation gives  estimated probability  passing  exam  p similarly   student  studies hours  estimated probability  passing  exam  p  table shows  probability  passing  exam  several values  hours studying  output   logistic regression analysis gives  pvalue  p   based   wald zscore rather   wald method  recommended method  calculate  pvalue  logistic regression   likelihood ratio test lrt    data gives p  discussion logistic regression can  binomial ordinal  multinomial binomial  binary logistic regression deals  situations    observed outcome   dependent variable can   two possible types   may represent  example dead vs alive  win vs loss multinomial logistic regression deals  situations   outcome can  three   possible types eg disease  vs disease b vs disease c    ordered ordinal logistic regression deals  dependent variables   ordered  binary logistic regression  outcome  usually coded     leads    straightforward interpretation   particular observed outcome   dependent variable   noteworthy possible outcome referred    success   case   usually coded    contrary outcome referred    failure   noncase  logistic regression  used  predict  odds    case based   values   independent variables predictors  odds  defined   probability   particular outcome   case divided   probability     noncase like  forms  regression analysis logistic regression makes use  one   predictor variables  may  either continuous  categorical unlike ordinary linear regression however logistic regression  used  predicting binary dependent variables treating  dependent variable   outcome   bernoulli trial rather   continuous outcome given  difference  assumptions  linear regression  violated  particular  residuals   normally distributed  addition linear regression may make nonsensical predictions   binary dependent variable   needed   way  convert  binary variable   continuous one  can take   real value negative  positive    logistic regression first takes  odds   event happening  different levels   independent variable  takes  ratio   odds   continuous    negative   takes  logarithm   ratio   referred   logit  logodds  create  continuous criterion   transformed version   dependent variable thus  logit transformation  referred    link function  logistic regressionalthough  dependent variable  logistic regression  binomial  logit   continuous criterion upon  linear regression  conducted  logit  success   fitted   predictors using linear regression analysis  predicted value   logit  converted back  predicted odds via  inverse   natural logarithm namely  exponential function thus although  observed dependent variable  logistic regression   zeroorone variable  logistic regression estimates  odds   continuous variable   dependent variable   success  case   applications  odds     needed  others  specific yesorno prediction  needed  whether  dependent variable      case  categorical prediction can  based   computed odds   success  predicted odds   chosen cutoff value  translated   prediction   success logistic regression vs  approaches logistic regression measures  relationship   categorical dependent variable  one   independent variables  estimating probabilities using  logistic function    cumulative logistic distribution thus  treats   set  problems  probit regression using similar techniques   latter using  cumulative normal distribution curve instead equivalently   latent variable interpretations   two methods  first assumes  standard logistic distribution  errors   second  standard normal distribution  errors logistic regression can  seen   special case   generalized linear model  thus analogous  linear regression  model  logistic regression however  based  quite different assumptions   relationship  dependent  independent variables    linear regression  particular  key differences   two models can  seen   following two features  logistic regression first  conditional distribution formula   bernoulli distribution rather   gaussian distribution   dependent variable  binary second  predicted values  probabilities   therefore restricted    logistic distribution function  logistic regression predicts  probability  particular outcomes logistic regression   alternative  fishers method linear discriminant analysis   assumptions  linear discriminant analysis hold  conditioning can  reversed  produce logistic regression  converse   true however  logistic regression   require  multivariate normal assumption  discriminant analysis latent variable interpretation  logistic regression can  understood simply  finding  formula parameters  best fit  formula   error distributed   standard logistic distribution   standard normal distribution  used instead    probit regression  associated latent variable  formula  error term formula   observed    formula  also  unobservable hence termed latent  observed data  values  formula  formula unlike ordinary regression however  formula parameters   expressed   direct formula   formula  formula values   observed data instead     found   iterative search process usually implemented   software program  finds  maximum   complicated likelihood expression    function     observed formula  formula values  estimation approach  explained  logistic function odds odds ratio  logit definition   logistic function  explanation  logistic regression can begin   explanation   standard logistic function  logistic function  useful   can take  real input formula formula whereas  output always takes values  zero  one  hence  interpretable   probability  logistic function formula  defined  follows  graph   logistic function   tinterval  shown  figure let us assume  formula   linear function   single explanatory variable formula  case  formula   linear combination  multiple explanatory variables  treated similarly  can  express formula  follows   logistic function can now  written  note  formula  interpreted   probability   dependent variable equaling  success  case rather   failure  noncase  clear   response variables formula   identically distributed formula differs  one data point formula  another though   independent given design matrix formula  shared  parameters formula definition   inverse   logistic function  can now define  inverse   logistic function formula  logit log odds  equivalently  exponentiating  sides interpretation   terms    equations  terms   follows definition   odds  odds   dependent variable equaling  case given  linear combination formula   predictors  equivalent   exponential function   linear regression expression  illustrates   logit serves   link function   probability   linear regression expression given   logit ranges  negative  positive infinity  provides  adequate criterion upon   conduct linear regression   logit  easily converted back   odds   define odds   dependent variable equaling  case given  linear combination formula   predictors  follows  odds ratio   continuous independent variable  odds ratio can  defined   exponential relationship provides  interpretation  formula  odds multiply  formula  every unit increase  x   binary independent variable  odds ratio  defined  formula   b c  d  cells   contingency table multiple explanatory variables    multiple explanatory variables   expression formula can  revised  formula     used   equation relating  logged odds   success   values   predictors  linear regression will   multiple regression  m explanators  parameters formula   j m   estimated model fitting estimation   model can  expressed   generalized linear model see   p ordinary least squares can suffice  rsquared   measure  goodness  fit   fitting space  p   complex methods  required rule  ten  widelyused rule  thumb states  logistic regression models give stable values   explanatory variables  based   minimum   events per explanatory variable epv  event denotes  cases belonging   less frequent category   dependent variable thus  study designed  use formula explanatory variables   event eg myocardial infarction expected  occur   proportion formula  participants   study will require  total  formula participants however   considerable debate   reliability   rule   based  simulation studies  lacks  secure theoretical underpinning according   authors  rule  overconservative  circumstances   authors stating   somewhat subjectively regard confidence interval coverage less  percent type  error greater  percent  relative bias greater  percent  problematic  results indicate  problems  fairly frequent  epv uncommon  epv  still observed  epv cox models appear   slightly  susceptible  logistic  worst instances   problem   severe  epv  usually comparable    epv maximum likelihood estimation  regression coefficients  usually estimated using maximum likelihood estimation unlike linear regression  normally distributed residuals    possible  find  closedform expression   coefficient values  maximize  likelihood function    iterative process must  used instead  example newtons method  process begins   tentative solution revises  slightly  see   can  improved  repeats  revision    improvement  made   point  process  said   converged   instances  model may  reach convergence nonconvergence   model indicates   coefficients   meaningful   iterative process  unable  find appropriate solutions  failure  converge may occur   number  reasons   large ratio  predictors  cases multicollinearity sparseness  complete separation iteratively reweighted least squares irls binary logistic regression formula  formula can  example  calculated using iteratively reweighted least squares irls   equivalent  minimizing  loglikelihood   bernoulli distributed process using newtons method   problem  written  vector matrix form  parameters formula explanatory variables formula  expected value   bernoulli distribution formula  parameters formula can  found using  following iterative algorithm  formula   diagonal weighting matrix formula  vector  expected values  regressor matrix  formula  vector  response variables  details can  found eg  evaluating goodness  fit discrimination  linear regression models  generally measured using r since    direct analog  logistic regression various methods including  following can  used instead deviance  likelihood ratio tests  linear regression analysis one  concerned  partitioning variance via  sum  squares calculations variance   criterion  essentially divided  variance accounted    predictors  residual variance  logistic regression analysis deviance  used  lieu  sum  squares calculations deviance  analogous   sum  squares calculations  linear regression    measure   lack  fit   data   logistic regression model   saturated model  available  model   theoretically perfect fit deviance  calculated  comparing  given model   saturated model  computation gives  likelihoodratio test    equation represents  deviance  ln represents  natural logarithm  log   likelihood ratio  ratio   fitted model   saturated model will produce  negative value hence  need   negative sign can  shown  follow  approximate chisquared distribution smaller values indicate better fit   fitted model deviates less   saturated model  assessed upon  chisquare distribution nonsignificant chisquare values indicate  little unexplained variance  thus good model fit conversely  significant chisquare value indicates   significant amount   variance  unexplained   saturated model   available  common case deviance  calculated simply  log likelihood   fitted model   reference   saturated models log likelihood can  removed    follows without harm two measures  deviance  particularly important  logistic regression  deviance  model deviance   deviance represents  difference   model    intercept  means  predictors   saturated model  model deviance represents  difference   model   least one predictor   saturated model   respect   model provides  baseline upon   compare predictor models given  deviance   measure   difference   given model   saturated model smaller values indicate better fit thus  assess  contribution   predictor  set  predictors one can subtract  model deviance    deviance  assess  difference   formula chisquare distribution  degrees  dom equal   difference   number  parameters estimated let   difference      model deviance  significantly smaller    deviance  one can conclude   predictor  set  predictors significantly improved model fit   analogous   test used  linear regression analysis  assess  significance  prediction pseudors  linear regression  squared multiple correlation r  used  assess goodness  fit   represents  proportion  variance   criterion   explained   predictors  logistic regression analysis    agreed upon analogous measure    several competing measures   limitations four    commonly used indices  one less commonly used one  examined   page r  given      analogous index   squared multiple correlation  linear regression  represents  proportional reduction   deviance wherein  deviance  treated   measure  variation analogous   identical   variance  linear regression analysis one limitation   likelihood ratio r      monotonically related   odds ratio meaning     necessarily increase   odds ratio increases    necessarily decrease   odds ratio decreases r   alternative index  goodness  fit related   r value  linear regression   given   l  l   likelihoods   model  fitted    model respectively  cox  snell index  problematic   maximum value  formula  highest  upper bound can     can easily   low    marginal proportion  cases  small r provides  correction   cox  snell r    maximum value  equal  nevertheless  cox  snell  likelihood ratio rs show greater agreement     either    nagelkerke r  course  might    case  values exceeding   cox  snell index  capped   value  likelihood ratio r  often preferred   alternatives     analogous  r  linear regression  independent   base rate  cox  snell  nagelkerke rs increase   proportion  cases increase    varies   r  defined    preferred  r  allison  two expressions r  r   related respectively  however allison now prefers r    relatively new measure developed  tjur  can  calculated  two steps  word  caution   order  interpreting pseudor statistics  reason  indices  fit  referred   pseudo r      represent  proportionate reduction  error   r  linear regression  linear regression assumes homoscedasticity   error variance      values   criterion logistic regression will always  heteroscedastic  error variances differ   value   predicted score   value   predicted score     different value   proportionate reduction  error therefore   inappropriate  think  r   proportionate reduction  error   universal sense  logistic regression hosmerlemeshow test  hosmerlemeshow test uses  test statistic  asymptotically follows  formula distribution  assess whether    observed event rates match expected event rates  subgroups   model population  test  considered   obsolete   statisticians    dependence  arbitrary binning  predicted probabilities  relative low power coefficients  fitting  model   likely  researchers will want  examine  contribution  individual predictors     will want  examine  regression coefficients  linear regression  regression coefficients represent  change   criterion   unit change   predictor  logistic regression however  regression coefficients represent  change   logit   unit change   predictor given   logit   intuitive researchers  likely  focus   predictors effect   exponential function   regression coefficient  odds ratio see definition  linear regression  significance   regression coefficient  assessed  computing  t test  logistic regression   several different tests designed  assess  significance   individual predictor  notably  likelihood ratio test   wald statistic likelihood ratio test  likelihoodratio test discussed   assess model fit  also  recommended procedure  assess  contribution  individual predictors   given model   case   single predictor model one simply compares  deviance   predictor model      model   chisquare distribution   single degree  dom   predictor model   significantly smaller deviance cf chisquare using  difference  degrees  dom   two models  one can conclude     significant association   predictor   outcome although  common statistical packages eg spss  provide likelihood ratio test statistics without  computationally intensive test     difficult  assess  contribution  individual predictors   multiple logistic regression case  assess  contribution  individual predictors one can enter  predictors hierarchically comparing  new model   previous  determine  contribution   predictor    debate among statisticians   appropriateness  socalled stepwise procedures  fear    may  preserve nominal statistical properties  may become misleading wald statistic alternatively  assessing  contribution  individual predictors   given model one may examine  significance   wald statistic  wald statistic analogous   ttest  linear regression  used  assess  significance  coefficients  wald statistic   ratio   square   regression coefficient   square   standard error   coefficient   asymptotically distributed   chisquare distribution although several statistical packages eg spss sas report  wald statistic  assess  contribution  individual predictors  wald statistic  limitations   regression coefficient  large  standard error   regression coefficient also tends   large increasing  probability  typeii error  wald statistic also tends   biased  data  sparse casecontrol sampling suppose cases  rare   might wish  sample   frequently   prevalence   population  example suppose    disease  affects person    collect  data  need    complete physical  may   expensive   thousands  physicals  healthy people  order  obtain data     diseased individuals thus  may evaluate  diseased individuals perhaps    rare outcomes   also retrospective sampling  equivalently   called unbalanced data   rule  thumb sampling controls   rate  five times  number  cases will produce sufficient control data logistic regression  unique    may  estimated  unbalanced data rather  randomly sampled data  still yield correct coefficient estimates   effects   independent variable   outcome    say   form  logistic model   data   model  correct   general population  formula parameters   correct except  formula  can correct formula   know  true prevalence  follows  formula   true prevalence  formula   prevalence   sample formal mathematical specification   various equivalent specifications  logistic regression  fit  different types   general models  different specifications allow  different sorts  useful generalizations setup  basic setup  logistic regression      standard linear regression   assumed     series  n observed data points  data point  consists   set  m explanatory variables x x also called independent variables predictor variables input variables features  attributes   associated binaryvalued outcome variable y also known   dependent variable response variable output variable outcome variable  class variable ie  can assume   two possible values often meaning   failure  often meaning yes  success  goal  logistic regression   explain  relationship   explanatory variables   outcome    outcome can  predicted   new set  explanatory variables  examples   linear regression  outcome variables y  assumed  depend   explanatory variables x x  shown     examples  explanatory variables may    type realvalued binary categorical etc  main distinction   continuous variables   income age  blood pressure  discrete variables   sex  race discrete variables referring    two possible choices  typically coded using dummy variables  indicator variables   separate explanatory variables taking  value   created   possible value   discrete variable   meaning variable    given value   meaning variable     value  example  fourway discrete variable  blood type   possible values  b ab o can  converted  four separate twoway dummy variables isa isb isab iso   one     value    rest   value  allows  separate regression coefficients   matched   possible value   discrete variable   case like   three   four dummy variables  independent      sense    values  three   variables  known  fourth  automatically determined thus   necessary  encode  three   four possibilities  dummy variables  also means    four possibilities  encoded  overall model   identifiable   absence  additional constraints    regularization constraint theoretically   cause problems   reality almost  logistic regression models  fitted  regularization constraints formally  outcomes y  described   bernoullidistributed data   outcome  determined   unobserved probability p   specific   outcome  hand  related   explanatory variables  can  expressed     following equivalent forms  meanings   four lines   basic idea  logistic regression   use  mechanism already developed  linear regression  modeling  probability p using  linear predictor function ie  linear combination   explanatory variables   set  regression coefficients   specific   model  hand      trials  linear predictor function formula   particular data point   written   formula  regression coefficients indicating  relative effect   particular explanatory variable   outcome  model  usually put    compact form  follows  makes  possible  write  linear predictor function  follows using  notation   dot product  two vectors   generalized linear model  particular model used  logistic regression  distinguishes   standard linear regression    types  regression analysis used  binaryvalued outcomes   way  probability   particular outcome  linked   linear predictor function written using   compact notation described     formulation expresses logistic regression   type  generalized linear model  predicts variables  various types  probability distributions  fitting  linear predictor function    form   sort  arbitrary transformation   expected value   variable  intuition  transforming using  logit function  natural log   odds  explained   also   practical effect  converting  probability   bounded       variable  ranges  formula thereby matching  potential range   linear prediction function   right side   equation note    probabilities p   regression coefficients  unobserved   means  determining    part   model    typically determined   sort  optimization procedure eg maximum likelihood estimation  finds values  best fit  observed data ie  give   accurate predictions   data already observed usually subject  regularization conditions  seek  exclude unlikely values eg extremely large values     regression coefficients  use   regularization condition  equivalent   maximum  posteriori map estimation  extension  maximum likelihood regularization   commonly done using  squared regularizing function   equivalent  placing  zeromean gaussian prior distribution   coefficients   regularizers  also possible whether   regularization  used   usually  possible  find  closedform solution instead  iterative numerical method must  used   iteratively reweighted least squares irls   commonly  days  quasinewton method    lbfgs method  interpretation   parameter estimates    additive effect   log   odds   unit change   jth explanatory variable   case   dichotomous explanatory variable  instance gender formula   estimate   odds    outcome  say males compared  females  equivalent formula uses  inverse   logit function    logistic function ie  formula can also  written   probability distribution specifically using  probability mass function   latentvariable model   model   equivalent formulation   latentvariable model  formulation  common   theory  discrete choice models  makes  easier  extend  certain  complicated models  multiple correlated choices  well   compare logistic regression   closely related probit model imagine    trial     continuous latent variable y ie  unobserved random variable   distributed  follows  ie  latent variable can  written directly  terms   linear predictor function   additive random error variable   distributed according   standard logistic distribution  y can  viewed   indicator  whether  latent variable  positive  choice  modeling  error variable specifically   standard logistic distribution rather   general logistic distribution   location  scale set  arbitrary values seems restrictive   fact     must  kept  mind   can choose  regression coefficients    often can use   offset changes   parameters   error variables distribution  example  logistic errorvariable distribution   nonzero location parameter  sets  mean  equivalent   distribution   zero location parameter    added   intercept coefficient  situations produce   value  y regardless  settings  explanatory variables similarly  arbitrary scale parameter s  equivalent  setting  scale parameter    dividing  regression coefficients  s   latter case  resulting value  y will  smaller   factor  s    former case   sets  explanatory variables  critically  will always remain    side   hence lead    y choice  turns    formulation  exactly equivalent   preceding one phrased  terms   generalized linear model  without  latent variables  can  shown  follows using  fact   cumulative distribution function cdf   standard logistic distribution   logistic function    inverse   logit function ie   formulationwhich  standard  discrete choice modelsmakes clear  relationship  logistic regression  logit model   probit model  uses  error variable distributed according   standard normal distribution instead   standard logistic distribution   logistic  normal distributions  symmetric   basic unimodal bell curve shape   difference    logistic distribution  somewhat heavier tails  means    less sensitive  outlying data  hence somewhat  robust  model misspecifications  erroneous data twoway latentvariable model yet another formulation uses two separate latent variables   ev   standard type extreme value distribution ie   model   separate latent variable   separate set  regression coefficients   possible outcome   dependent variable  reason   separation    makes  easy  extend logistic regression  multioutcome categorical variables    multinomial logit model    model   natural  model  possible outcome using  different set  regression coefficients   also possible  motivate    separate latent variables   theoretical utility associated  making  associated choice  thus motivate logistic regression  terms  utility theory  terms  utility theory  rational actor always chooses  choice   greatest associated utility    approach taken  economists  formulating discrete choice models    provides  theoretically strong foundation  facilitates intuitions   model   turn makes  easy  consider various sorts  extensions see  example   choice   type extreme value distribution seems fairly arbitrary   makes  mathematics work    may  possible  justify  use  rational choice theory  turns    model  equivalent   previous model although  seems nonobvious since   now two sets  regression coefficients  error variables   error variables   different distribution  fact  model reduces directly   previous one   following substitutions  intuition   comes   fact  since  choose based   maximum  two values   difference matters   exact values   effectively removes one degree  dom another critical fact    difference  two type extremevaluedistributed variables   logistic distribution ie formula  can demonstrate  equivalent  follows example   example consider  provincelevel election   choice    rightofcenter party  leftofcenter party   secessionist party eg  parti qu b cois  wants quebec  secede  canada    use three latent variables one   choice   accordance  utility theory  can  interpret  latent variables  expressing  utility  results  making    choices  can also interpret  regression coefficients  indicating  strength   associated factor ie explanatory variable   contributing   utility   correctly  amount    unit change   explanatory variable changes  utility   given choice  voter might expect   rightofcenter party  lower taxes especially  rich people   give lowincome people  benefit ie  change  utility since  usually dont pay taxes  cause moderate benefit ie somewhat  money  moderate utility increase  middleincoming people   cause significant benefits  highincome people    hand  leftofcenter party might  expected  raise taxes  offset   increased welfare   assistance   lower  middle classes   cause significant positive benefit  lowincome people perhaps weak benefit  middleincome people  significant negative benefit  highincome people finally  secessionist party  take  direct actions   economy  simply secede  lowincome  middleincome voter might expect basically  clear utility gain  loss     highincome voter might expect negative utility since heshe  likely   companies  will   harder time  business    environment  probably lose money  intuitions can  expressed  follows  clearly shows    loglinear model yet another formulation combines  twoway latent variable formulation    original formulation higher  without latent variables    process provides  link  one   standard formulations   multinomial logit  instead  writing  logit   probabilities p   linear predictor  separate  linear predictor  two one     two outcomes note  two separate sets  regression coefficients   introduced just    twoway latent variable model   two equations appear  form  writes  logarithm   associated probability   linear predictor   extra term formula   end  term   turns  serves   normalizing factor ensuring   result   distribution  can  seen  exponentiating  sides   form   clear   purpose  z   ensure   resulting distribution  y   fact  probability distribution ie  sums   means  z  simply  sum   unnormalized probabilities   dividing  probability  z  probabilities become normalized     resulting equations   generally  shows clearly   generalize  formulation    two outcomes   multinomial logit note   general formulation  exactly  softmax function    order  prove    equivalent   previous model note    model  overspecified   formula  formula   independently specified rather formula  knowing one automatically determines     result  model  nonidentifiable   multiple combinations   will produce   probabilities   possible explanatory variables  fact  can  seen  adding  constant vector     will produce   probabilities   result  can simplify matters  restore identifiability  picking  arbitrary value  one   two vectors  choose  set formula     shows   formulation  indeed equivalent   previous formulation    twoway latent variable formulation  settings  formula will produce equivalent results note   treatments   multinomial logit model start  either  extending  loglinear formulation presented    twoway latent variable formulation presented  since  clearly show  way   model   extended  multiway outcomes  general  presentation  latent variables   common  econometrics  political science  discrete choice models  utility theory reign   loglinear formulation    common  computer science eg machine learning  natural language processing   singlelayer perceptron  model   equivalent formulation  functional form  commonly called  singlelayer perceptron  singlelayer artificial neural network  singlelayer neural network computes  continuous output instead   step function  derivative  p  respect  x x x  computed   general form  fx   analytic function  x   choice  singlelayer neural network  identical   logistic regression model  function   continuous derivative  allows    used  backpropagation  function  also preferred   derivative  easily calculated  terms  binomial data  closely related model assumes     associated    single bernoulli trial   n independent identically distributed trials   observation y   number  successes observed  sum   individual bernoullidistributed random variables  hence follows  binomial distribution  example   distribution   fraction  seeds p  germinate  n  planted  terms  expected values  model  expressed  follows    equivalently  model can  fit using   sorts  methods     basic model bayesian   bayesian statistics context prior distributions  normally placed   regression coefficients usually   form  gaussian distributions    conjugate prior   likelihood function  logistic regression back  bayesian inference  performed analytically  made  posterior distribution difficult  calculate except   low dimensions now though automatic software   openbugs jags  stan allow  posteriors   computed using simulation  lack  conjugacy    concern however   sample size   number  parameters  large full bayesian simulation can  slow  people often use approximate methods   variational bayes  expectation propagation extensions   large numbers  extensions software  statistical software can  binary logistic regression notably microsoft excels statistics extension package   include \r\n"}
{"index":{"_id":14}}
{"conceptLabelTag":"kernel principal component analysis","conceptLabel":"kernel principal component analysis","conceptDescription":"kernel principal component analysis   field  multivariate statistics kernel principal component analysis kernel pca   extension  principal component analysis pca using techniques  kernel methods using  kernel  originally linear operations  pca  performed   reproducing kernel hilbert space background linear pca recall  conventional pca operates  zerocentered data    operates  diagonalizing  covariance matrix   words  gives  eigendecomposition   covariance matrix  can  rewritten  introduction   kernel  pca  understand  utility  kernel pca particularly  clustering observe   n points   general  linearly separated  formula dimensions  can almost always  linearly separated  formula dimensions   given n points formula   map    ndimensional space    easy  construct  hyperplane  divides  points  arbitrary clusters  course  formula creates linearly independent vectors     covariance    perform eigendecomposition explicitly     linear pca instead  kernel pca  nontrivial arbitrary formula function  chosen   never calculated explicitly allowing  possibility  use veryhighdimensional formulas   never   actually evaluate  data   space since  generally try  avoid working   formulaspace   will call  feature space  can create  nbyn kernel  represents  inner product space see gramian matrix   otherwise intractable feature space  dual form  arises   creation   kernel allows us  mathematically formulate  version  pca    never actually solve  eigenvectors  eigenvalues   covariance matrix   formulaspace see kernel trick  nelements   column  k represent  dot product  one point   transformed data  respect    transformed points n points  wellknown kernels  shown   example     never working directly   feature space  kernelformulation  pca  restricted    computes   principal components    projections   data onto  components  evaluate  projection   point   feature space formula onto  kth principal component formula  superscript k means  component k  powers  k  note  formula denotes dot product   simply  elements   kernel formula  seems  thats left   calculate  normalize  formula  can  done  solving  eigenvector equation  n   number  data points   set  formula  formula   eigenvalues  eigenvectors  k   normalize  eigenvectors formulas  require  care must  taken regarding  fact  whether   formula  zeromean   original space    guaranteed   centered   feature space   never compute explicitly since centered data  required  perform  effective principal component analysis  centralize k  become formula  formula denotes  nbyn matrix    element takes value formula  use formula  perform  kernel pca algorithm described  one caveat  kernel pca   illustrated   linear pca  can use  eigenvalues  rank  eigenvectors based   much   variation   data  captured   principal component   useful  data dimensionality reduction    also  applied  kpca however  practice   cases   variations   data     typically caused   wrong choice  kernel scale large datasets  practice  large data set leads   large k  storing k may become  problem one way  deal     perform clustering   dataset  populate  kernel   means   clusters since even  method may yield  relatively large k   common  compute   top p eigenvalues  eigenvectors  k example consider three concentric clouds  points shown  wish  use kernel pca  identify  groups  color   points   part   algorithm     show   data groups together     transformation first consider  kernel applying   kernel pca yields  next image now consider  gaussian kernel    kernel   measure  closeness equal    points coincide  equal   infinity note  particular   first principal component  enough  distinguish  three different groups   impossible using  linear pca  linear pca operates    given   case twodimensional space    concentric point clouds   linearly separable applications kernel pca   demonstrated   useful  novelty detection  image denoising\r\n"}
{"index":{"_id":15}}
{"conceptLabelTag":"feature selection","conceptLabel":"feature selection","conceptDescription":"feature selection  machine learning  statistics feature selection also known  variable selection attribute selection  variable subset selection   process  selecting  subset  relevant features variables predictors  use  model construction feature selection techniques  used  four reasons  central premise  using  feature selection technique    data contains many features   either redundant  irrelevant  can thus  removed without incurring much loss  information redundant  irrelevant features  two distinct notions since one relevant feature may  redundant   presence  another relevant feature     strongly correlated feature selection techniques   distinguished  feature extraction feature extraction creates new features  functions   original features whereas feature selection returns  subset   features feature selection techniques  often used  domains    many features  comparatively  samples  data points archetypal cases   application  feature selection include  analysis  written texts  dna microarray data    many thousands  features    tens  hundreds  samples introduction  feature selection algorithm can  seen   combination   search technique  proposing new feature subsets along   evaluation measure  scores  different feature subsets  simplest algorithm   test  possible subset  features finding  one  minimizes  error rate    exhaustive search   space   computationally intractable     smallest  feature sets  choice  evaluation metric heavily influences  algorithm     evaluation metrics  distinguish   three main categories  feature selection algorithms wrappers filters  embedded methods  traditional statistics   popular form  feature selection  stepwise regression    wrapper technique    greedy algorithm  adds  best feature  deletes  worst feature   round  main control issue  deciding   stop  algorithm  machine learning   typically done  crossvalidation  statistics  criteria  optimized  leads   inherent problem  nesting  robust methods   explored   branch  bound  piecewise linear network subset selection subset selection evaluates  subset  features   group  suitability subset selection algorithms can  broken   wrappers filters  embedded wrappers use  search algorithm  search   space  possible features  evaluate  subset  running  model   subset wrappers can  computationally expensive    risk   fitting   model filters  similar  wrappers   search approach  instead  evaluating   model  simpler filter  evaluated embedded techniques  embedded   specific   model many popular search approaches use greedy hill climbing  iteratively evaluates  candidate subset  features  modifies  subset  evaluates   new subset   improvement   old evaluation   subsets requires  scoring metric  grades  subset  features exhaustive search  generally impractical    implementor  operator defined stopping point  subset  features   highest score discovered    point  selected   satisfactory feature subset  stopping criterion varies  algorithm possible criteria include  subset score exceeds  threshold  programs maximum allowed run time   surpassed etc alternative searchbased techniques  based  targeted projection pursuit  finds lowdimensional projections   data  score highly  features    largest projections   lowerdimensional space   selected search approaches include two popular filter metrics  classification problems  correlation  mutual information although neither  true metrics  distance measures   mathematical sense since  fail  obey  triangle inequality  thus   compute  actual distance   rather  regarded  scores  scores  computed   candidate feature  set  features   desired output category   however true metrics    simple function   mutual information see   available filter metrics include optimality criteria  choice  optimality criteria  difficult    multiple objectives   feature selection task many common ones incorporate  measure  accuracy penalised   number  features selected eg  bayesian information criterion  oldest  mallowss c statistic  akaike information criterion aic  add variables   tstatistic  bigger  formula  criteria  bayesian information criterion bic  uses formula minimum description length mdl  asymptotically uses formula bonferroni ric  use formula maximum dependency feature selection   variety  new criteria   motivated  false discovery rate fdr  use something close  formula structure learning filter feature selection   specific case    general paradigm called structure learning feature selection finds  relevant feature set   specific target variable whereas structure learning finds  relationships    variables usually  expressing  relationships   graph   common structure learning algorithms assume  data  generated   bayesian network    structure   directed graphical model  optimal solution   filter feature selection problem   markov blanket   target node    bayesian network    unique markov blanket   node minimumredundancymaximumrelevance mrmr feature selection peng et al proposed  feature selection method  can use either mutual information correlation  distancesimilarity scores  select features  aim   penalise  features relevancy   redundancy   presence    selected features  relevance   feature set   class  defined   average value   mutual information values   individual feature   class  follows  redundancy   features   set   average value   mutual information values   feature   feature feature selection embedded  learning algorithms  learning algorithms perform feature selection  part   overall operation  include\r\n"}
{"index":{"_id":16}}
{"conceptLabelTag":"data preprocessing","conceptLabel":"data preprocessing","conceptDescription":"data preprocessing data preprocessing   important step   data mining process  phrase garbage  garbage   particularly applicable  data mining  machine learning projects datagathering methods  often loosely controlled resulting  outofrange values eg income impossible data combinations eg sex male pregnant yes missing values etc analyzing data     carefully screened   problems can produce misleading results thus  representation  quality  data  first  foremost  running  analysis    much irrelevant  redundant information present  noisy  unreliable data  knowledge discovery   training phase   difficult data preparation  filtering steps can take considerable amount  processing time data preprocessing includes cleaning instance selection normalization transformation feature extraction  selection etc  product  data preprocessing   final training set kotsiantis et al present  wellknown algorithm   step  data preprocessing\r\n"}
{"index":{"_id":17}}
{"conceptLabelTag":"birch","conceptLabel":"birch","conceptDescription":"birch birch balanced iterative reducing  clustering using hierarchies   unsupervised data mining algorithm used  perform hierarchical clustering  particularly large datasets  advantage  birch   ability  incrementally  dynamically cluster incoming multidimensional metric data points   attempt  produce  best quality clustering   given set  resources memory  time constraints   cases birch  requires  single scan   database  inventors claim birch    first clustering algorithm proposed   database area  handle noise data points    part   underlying pattern effectively beating dbscan  two months  algorithm received  sigmod year test  time award  problem  previous methods previous clustering algorithms performed less effectively   large databases    adequately consider  case wherein  dataset   large  fit  main memory   result    lot  overhead maintaining high clustering quality  minimizing  cost  addition io inputoutput operations furthermore   birchs predecessors inspect  data points   currently existing clusters equally   clustering decision    perform heuristic weighting based   distance   data points advantages  birch   local    clustering decision  made without scanning  data points  currently existing clusters  exploits  observation  data space   usually uniformly occupied   every data point  equally important  makes full use  available memory  derive  finest possible subclusters  minimizing io costs   also  incremental method    require  whole data set  advance algorithm  birch algorithm takes  input  set  data points represented  realvalued vectors   desired number  clusters  operates  four phases  second    optional  first phase builds  cf tree    data points  heightbalanced tree data structure defined  follows   second step  algorithm scans   leaf entries   initial cf tree  rebuild  smaller cf tree  removing outliers  grouping crowded subclusters  larger ones  step  marked optional   original presentation  birch  step three  existing clustering algorithm  used  cluster  leaf entries   agglomerative hierarchical clustering algorithm  applied directly   subclusters represented   cf vectors  also provides  flexibility  allowing  user  specify either  desired number  clusters   desired diameter threshold  clusters   step  set  clusters  obtained  captures major distribution pattern   data however  might exist minor  localized inaccuracies  can  handled   optional step  step  centroids   clusters produced  step  used  seeds  redistribute  data points   closest seeds  obtain  new set  clusters step also provides us   option  discarding outliers    point    far   closest seed can  treated   outlier\r\n"}
{"index":{"_id":18}}
{"conceptLabelTag":"hopfield network","conceptLabel":"hopfield network","conceptDescription":"hopfield network  hopfield network   form  recurrent artificial neural network popularized  john hopfield   described earlier  little  hopfield nets serve  contentaddressable memory systems  binary threshold nodes   guaranteed  converge   local minimum  will sometimes converge   false pattern wrong local minimum rather   stored pattern expected local minimum hopfield networks also provide  model  understanding human memory structure  units  hopfield nets  binary threshold units ie  units  take  two different values   states   value  determined  whether    units input exceeds  threshold hopfield nets normally  units  take  values     convention will  used throughout  page however  literature might use units  take values   every pair  units   j   hopfield network   connection   described   connectivity weight formula   sense  hopfield network can  formally described   complete undirected graph formula  formula   set  mccullochpitts neurons  formula   function  links pairs  nodes   real value  connectivity weight  connections   hopfield net typically   following restrictions  constraint  weights  symmetric guarantees  energy function decreases monotonically  following  activation rules   network may exhibit  periodic  chaotic behaviour  nonsymmetric weights  used however hopfield found   chaotic behavior  confined  relatively small parts   phase space    impair  networks ability  act   contentaddressable associative memory system updating updating one unit node   graph simulating  artificial neuron   hopfield network  performed using  following rule formula  updates   hopfield network can  performed  two different ways neurons attract  repel    weight  two units   powerful impact upon  values   neurons consider  connection weight formula  two neurons   j  formula  updating rule implies  thus  values  neurons   j will converge   weight    positive similarly  will diverge   weight  negative energy hopfield nets   scalar value associated   state   network referred    energy e   network   value  called  energy   definition ensures   units  randomly chosen  update  energy e will either lower  value  stay   furthermore  repeated updating  network will eventually converge   state    local minimum   energy function   considered    lyapunov function thus   state   local minimum   energy function    stable state   network note   energy function belongs   general class  models  physics   name  ising models   turn   special case  markov networks since  associated probability measure  gibbs measure   markov property initialization  running initialization   hopfield networks  done  setting  values   units   desired start pattern repeated updates   performed   network converges   attractor pattern convergence  generally assured  hopfield proved   attractors   nonlinear dynamical system  stable  periodic  chaotic     systems therefore   context  hopfield networks  attractor pattern   final stable state  pattern   change  value within   updating training training  hopfield net involves lowering  energy  states   net  remember  allows  net  serve   content addressable memory system    say  network will converge   remembered state    given  part   state  net can  used  recover   distorted input   trained state    similar   input   called associative memory   recovers memories   basis  similarity  example   train  hopfield net  five units    state   energy minimum   give  network  state  will converge  thus  network  properly trained   energy  states   network  remember  local minima learning rules   various different learning rules  can  used  store information   memory   hopfield network   desirable   learning rule      following two properties  properties  desirable since  learning rule satisfying    biologically plausible  example since  human brain  always learning new concepts one can reason  human learning  incremental  learning system    incremental  generally  trained     huge batch  training data hebbian learning rule  hopfield networks  hebbian theory  introduced  donald hebb   order  explain associative learning   simultaneous activation  neuron cells leads  pronounced increases  synaptic strength   cells   often summarized  neurons  fire together wire together neurons  fire   sync fail  link  hebbian rule   local  incremental   hopfield networks   implemented   following manner  learning formula binary patterns formula  formula represents bit   pattern formula   bits corresponding  neurons   j  equal  pattern formula   product formula will  positive    turn   positive effect   weight formula   values    j will tend  become equal  opposite happens   bits corresponding  neurons   j  different  storkey learning rule  rule  introduced  amos storkey     local  incremental storkey also showed   hopfield network trained using  rule   greater capacity   corresponding network trained using  hebbian rule  weight matrix   attractor neural network  said  follow  storkey learning rule   obeys formula  formula   form  local field  neuron   learning rule  local since  synapses take  account  neurons   sides  rule makes use   information   patterns  weights   generalized hebbian rule due   effect   local field spurious patterns patterns   network uses  training called retrieval states become attractors   system repeated updates  eventually lead  convergence  one   retrieval states however sometimes  network will converge  spurious patterns different   training patterns  energy   spurious patterns  also  local minimum   stored pattern x  negation x  also  spurious pattern  spurious state can also   linear combination   odd number  retrieval states  example  using patterns formula one can get  following spurious state formula spurious patterns    even number  states  exist since  might sum   zero capacity  network capacity   hopfield network model  determined  neuron amounts  connections within  given network therefore  number  memories   able   stored  dependent  neurons  connections furthermore   shown   recall accuracy  vectors  nodes  approximately vectors can  recalled  storage  every nodes hertz et al therefore   evident  many mistakes will occur  one tries  store  large number  vectors   hopfield model   recall  right pattern   possible   intrusion  taken place since semantically related items tend  confuse  individual  recollection   wrong pattern occurs therefore  hopfield network model  shown  confuse one stored item    another upon retrieval perfect recalls  high capacity can  loaded   network  hebbian learning method human memory  hopfield model accounts  associative memory   incorporation  memory vectors memory vectors can  slightly used    spark  retrieval    similar vector   network however  will find   due   process intrusions can occur  associative memory   hopfield network   two types  operations autoassociation  heteroassociation  first    vector  associated     latter   two different vectors  associated  storage furthermore  types  operations  possible  store within  single memory matrix     given representation matrix   one      operations  rather  combination autoassociative  heteroassociative   two   important  note  hopfields network model utilizes   learning rule  hebbs learning rule  basically tried  show  learning occurs   result   strengthening   weights   activity  occurring rizzuto  kahana  able  show   neural network model can account  repetition  recall accuracy  incorporating  probabilisticlearning algorithm   retrieval process  learning occurs   result  weights   network remain fixed showing   model  able  switch   learning stage   recall stage  adding contextual drift   able  show  rapid forgetting  occurs   hopfield model   cuedrecall task  entire network contributes   change   activation   single node mccullough  pitts dynamical rule  describes  behavior  neurons     way  shows   activations  multiple neurons map onto  activation   new neurons firing rate    weights   neurons strengthen  synaptic connections   new activated neuron    activated  hopfield  use mcculloughpittss dynamical rule  order  show  retrieval  possible   hopfield network however   important  note  hopfield      repetitious fashion hopfield  use  nonlinear activation function instead  using  linear function   therefore create  hopfield dynamical rule    hopfield  able  show    nonlinear activation function  dynamical rule will always modify  values   state vector   direction  one   stored patterns\r\n"}
{"index":{"_id":19}}
{"conceptLabelTag":"linear classifier","conceptLabel":"linear classifier","conceptDescription":"linear classifier   field  machine learning  goal  statistical classification   use  objects characteristics  identify  class  group  belongs   linear classifier achieves   making  classification decision based   value   linear combination   characteristics  objects characteristics  also known  feature values   typically presented   machine   vector called  feature vector  classifiers work well  practical problems   document classification   generally  problems  many variables features reaching accuracy levels comparable  nonlinear classifiers  taking less time  train  use definition   input feature vector   classifier   real vector formula   output score   formula   real vector  weights  f   function  converts  dot product   two vectors   desired output   words formula   oneform  linear functional mapping formula onto r  weight vector formula  learned   set  labeled training samples often f   simple function  maps  values   certain threshold   first class    values   second class   complex f might give  probability   item belongs   certain class   twoclass classification problem one can visualize  operation   linear classifier  splitting  highdimensional input space   hyperplane  points  one side   hyperplane  classified  yes   others  classified    linear classifier  often used  situations   speed  classification   issue since   often  fastest classifier especially  formula  sparse also linear classifiers often work  well   number  dimensions  formula  large   document classification   element  formula  typically  number  occurrences   word   document see documentterm matrix   cases  classifier   wellregularized generative models vs discriminative models   two broad classes  methods  determining  parameters   linear classifier formula  can  generative  discriminative models methods   first class model conditional density functions formula examples   algorithms include  second set  methods includes discriminative models  attempt  maximize  quality   output   training set additional terms   training cost function can easily perform regularization   final model examples  discriminative training  linear classifiers include note despite  name lda   belong   class  discriminative models   taxonomy however  name makes sense   compare lda    main linear dimensionality reduction algorithm principal components analysis pca lda   supervised learning algorithm  utilizes  labels   data  pca   unsupervised learning algorithm  ignores  labels  summarize  name   historical artifact discriminative training often yields higher accuracy  modeling  conditional density functions however handling missing data  often easier  conditional density models    linear classifier algorithms listed  can  converted  nonlinear algorithms operating   different input space formula using  kernel trick discriminative training discriminative training  linear classifiers usually proceeds   supervised way  means   optimization algorithm   given  training set  desired outputs   loss function  measures  discrepancy   classifiers outputs   desired outputs thus  learning algorithm solves  optimization problem   form  popular loss functions include  hinge loss  linear svms   log loss  linear logistic regression   regularization function  convex      convex problem many algorithms exist  solving  problems popular ones  linear classification include stochastic gradient descent lbfgs coordinate descent  newton methods\r\n"}
{"index":{"_id":20}}
{"conceptLabelTag":"classification","conceptLabel":"classification","conceptDescription":"statistical classification  machine learning  statistics classification   problem  identifying     set  categories subpopulations  new observation belongs   basis   training set  data containing observations  instances whose category membership  known  example   assigning  given email  spam  nonspam classes  assigning  diagnosis   given patient  described  observed characteristics   patient gender blood pressure presence  absence  certain symptoms etc classification   example  pattern recognition   terminology  machine learning classification  considered  instance  supervised learning ie learning   training set  correctly identified observations  available  corresponding unsupervised procedure  known  clustering  involves grouping data  categories based   measure  inherent similarity  distance often  individual observations  analyzed   set  quantifiable properties known variously  explanatory variables  features  properties may variously  categorical eg  b ab  o  blood type ordinal eg large medium  small integervalued eg  number  occurrences   particular word   email  realvalued eg  measurement  blood pressure  classifiers work  comparing observations  previous observations  means   similarity  distance function  algorithm  implements classification especially   concrete implementation  known   classifier  term classifier sometimes also refers   mathematical function implemented   classification algorithm  maps input data   category terminology across fields  quite varied  statistics  classification  often done  logistic regression   similar procedure  properties  observations  termed explanatory variables  independent variables regressors etc   categories   predicted  known  outcomes   considered   possible values   dependent variable  machine learning  observations  often known  instances  explanatory variables  termed features grouped   feature vector   possible categories   predicted  classes  fields may use different terminology eg  community ecology  term classification normally refers  cluster analysis ie  type  unsupervised learning rather   supervised learning described   article relation   problems classification  clustering  examples    general problem  pattern recognition    assignment   sort  output value   given input value  examples  regression  assigns  realvalued output   input sequence labeling  assigns  class   member   sequence  values  example part  speech tagging  assigns  part  speech   word   input sentence parsing  assigns  parse tree   input sentence describing  syntactic structure   sentence etc  common subclass  classification  probabilistic classification algorithms   nature use statistical inference  find  best class   given instance unlike  algorithms  simply output  best class probabilistic algorithms output  probability   instance   member     possible classes  best class  normally  selected   one   highest probability however   algorithm  numerous advantages  nonprobabilistic classifiers frequentist procedures early work  statistical classification  undertaken  fisher   context  twogroup problems leading  fishers linear discriminant function   rule  assigning  group   new observation  early work assumed  datavalues within    two groups   multivariate normal distribution  extension    context    twogroups  also  considered   restriction imposed   classification rule   linear later work   multivariate normal distribution allowed  classifier   nonlinear several classification rules can  derived based  slight different adjustments   mahalanobis distance   new observation  assigned   group whose centre   lowest adjusted distance   observation bayesian procedures unlike frequentist procedures bayesian classification procedures provide  natural way  taking  account  available information   relative sizes   subpopulations associated   different groups within  overall population bayesian procedures tend   computationally expensive    days  markov chain monte carlo computations  developed approximations  bayesian clustering rules  devised  bayesian procedures involve  calculation  group membership probabilities  can  viewed  providing   informative outcome   data analysis   simple attribution   single grouplabel   new observation binary  multiclass classification classification can  thought   two separate problems binary classification  multiclass classification  binary classification  better understood task  two classes  involved whereas multiclass classification involves assigning  object  one  several classes since many classification methods   developed specifically  binary classification multiclass classification often requires  combined use  multiple binary classifiers feature vectors  algorithms describe  individual instance whose category    predicted using  feature vector  individual measurable properties   instance  property  termed  feature also known  statistics   explanatory variable  independent variable although features may  may   statistically independent features may variously  binary eg male  female categorical eg  b ab  o  blood type ordinal eg large medium  small integervalued eg  number  occurrences   particular word   email  realvalued eg  measurement  blood pressure   instance   image  feature values might correspond   pixels   image   instance   piece  text  feature values might  occurrence frequencies  different words  algorithms work   terms  discrete data  require  realvalued  integervalued data  discretized  groups eg less     greater  linear classifiers  large number  algorithms  classification can  phrased  terms   linear function  assigns  score   possible category k  combining  feature vector   instance   vector  weights using  dot product  predicted category   one   highest score  type  score function  known   linear predictor function    following general form  x   feature vector  instance    vector  weights corresponding  category k  scorex k   score associated  assigning instance   category k  discrete choice theory  instances represent people  categories represent choices  score  considered  utility associated  person  choosing category k algorithms   basic setup  known  linear classifiers  distinguishes    procedure  determining training  optimal weightscoefficients   way   score  interpreted examples   algorithms  algorithms examples  classification algorithms include evaluation classifier performance depends greatly   characteristics   data   classified    single classifier  works best   given problems  phenomenon  may  explained   nolunch theorem various empirical tests   performed  compare classifier performance   find  characteristics  data  determine classifier performance determining  suitable classifier   given problem  however still   art   science  measures precision  recall  popular metrics used  evaluate  quality   classification system  recently receiver operating characteristic roc curves   used  evaluate  tradeoff  true  falsepositive rates  classification algorithms   performance metric  uncertainty coefficient   advantage  simple accuracy      affected   relative sizes   different classes   will  penalize  algorithm  simply rearranging  classes application domains classification  many applications       employed   data mining procedure   others  detailed statistical modeling  undertaken\r\n"}
{"index":{"_id":21}}
{"conceptLabelTag":"stochastic gradient descent","conceptLabel":"stochastic gradient descent","conceptDescription":"stochastic gradient descent stochastic gradient descent often shortened  sgd also known  incremental gradient descent   stochastic approximation   gradient descent optimization method  minimizing  objective function   written   sum  differentiable functions   words sgd tries  find minima  maxima  iteration background  statistical estimation  machine learning consider  problem  minimizing  objective function    form   sum   parameter formula  minimizes formula    estimated  summand function formula  typically associated   formulath observation   data set used  training  classical statistics summinimization problems arise  least squares   maximumlikelihood estimation  independent observations  general class  estimators  arise  minimizers  sums  called mestimators however  statistics    long recognized  requiring even local minimization   restrictive   problems  maximumlikelihood estimation therefore contemporary statistical theorists often consider stationary points   likelihood function  zeros   derivative  score function   estimating equations  summinimization problem also arises  empirical risk minimization   case formula   value   loss function  formulath example  formula   empirical risk  used  minimize   function  standard  batch gradient descent method  perform  following iterations  formula   step size sometimes called  learning rate  machine learning  many cases  summand functions   simple form  enables inexpensive evaluations   sumfunction   sum gradient  example  statistics oneparameter exponential families allow economical functionevaluations  gradientevaluations however   cases evaluating  sumgradient may require expensive evaluations   gradients   summand functions   training set  enormous   simple formulas exist evaluating  sums  gradients becomes  expensive  evaluating  gradient requires evaluating   summand functions gradients  economize   computational cost  every iteration stochastic gradient descent samples  subset  summand functions  every step    effective   case  largescale machine learning problems iterative method  stochastic  online gradient descent  true gradient  formula  approximated   gradient   single example   algorithm sweeps   training set  performs   update   training example several passes can  made   training set   algorithm converges    done  data can  shuffled   pass  prevent cycles typical implementations may use  adaptive learning rate    algorithm converges  pseudocode stochastic gradient descent can  presented  follows  compromise  computing  true gradient   gradient   single example   compute  gradient    one training example called  minibatch   step  can perform significantly better  true stochastic gradient descent   code can make use  vectorization libraries rather  computing  step separately  may also result  smoother convergence   gradient computed   step uses  training examples  convergence  stochastic gradient descent   analyzed using  theories  convex minimization   stochastic approximation briefly   learning rates formula decrease   appropriate rate  subject  relatively mild assumptions stochastic gradient descent converges almost surely   global minimum   objective function  convex  pseudoconvex  otherwise converges almost surely   local minimum    fact  consequence   robbinssiegmund theorem example lets suppose  want  fit  straight line formula   training set  twodimensional points formula using least squares  objective function   minimized   last line    pseudocode   specific problem will become applications stochastic gradient descent   popular algorithm  training  wide range  models  machine learning including linear support vector machines logistic regression see eg vowpal wabbit  graphical models  combined   backpropagation algorithm    de facto standard algorithm  training artificial neural networks  use   also reported   geophysics community specifically  applications  full waveform inversion fwi stochastic gradient descent competes   lbfgs algorithm   also widely used stochastic gradient descent   used since  least  training linear regression models originally   name adaline another popular stochastic gradient descent algorithm   least mean squares lms adaptive filter extensions  variants many improvements   basic stochastic gradient descent algorithm   proposed  used  particular  machine learning  need  set  learning rate step size   recognized  problematic setting  parameter  high can cause  algorithm  diverge setting   low makes  slow  converge  conceptually simple extension  stochastic gradient descent makes  learning rate  decreasing function   iteration number giving  learning rate schedule    first iterations cause large changes   parameters   later ones   finetuning  schedules   known since  work  macqueen  means clustering momentum  proposals include  momentum method  appeared  rumelhart hinton  williams seminal paper  backpropagation learning stochastic gradient descent  momentum remembers  update   iteration  determines  next update   convex combination   gradient   previous update    mathematically equivalent formulation  leads    parameter formula  minimizes formula    estimated  formula   step size sometimes called  learning rate  machine learning  name momentum stems   analogy  momentum  physics  weight vector thought    particle traveling  parameter space incurs acceleration   gradient   loss force unlike  classical stochastic gradient descent  tends  keep traveling    direction preventing oscillations momentum   used successfully  several decades averaging averaged stochastic gradient descent invented independently  ruppert  polyak   late s  ordinary stochastic gradient descent  records  average   parameter vector  time    update      ordinary stochastic gradient descent   algorithm also keeps track   optimization  done  averaged parameter vector takes  place  adagrad adagrad  adaptive gradient algorithm   modified stochastic gradient descent  perparameter learning rate first published  informally  increases  learning rate   sparse parameters  decreases  learning rate  less sparse ones  strategy often improves convergence performance  standard stochastic gradient descent  settings  data  sparse  sparse parameters   informative examples   applications include natural language processing  image recognition  still   base learning rate    multiplied   elements   vector    diagonal   outer product matrix  formula  gradient  iteration  diagonal  given   vector  updated  every iteration  formula   update  now  written  perparameter updates  gives rise   scaling factor   learning rate  applies   single parameter since  denominator   factor formula   norm  previous derivatives extreme parameter updates get dampened  parameters  get   small updates receive higher learning rates  designed  convex problems adagrad   successfully applied  nonconvex optimization rmsprop rmsprop  root mean square propagation  also  method    learning rate  adapted     parameters  idea   divide  learning rate   weight   running average   magnitudes  recent gradients   weight  first  running average  calculated  terms  means square  formula   forgetting factor   parameters  updated  rmsprop  shown excellent adaptation  learning rate  different applications rmsprop can  seen   generalization  rprop   capable  work  minibatches  well opposed   fullbatches adam adam  adaptive moment estimation   update  rmsprop optimizer   running average    gradients   magnitudes  used  three equations  define  optimizer   follows  formula  formula  two forgetting factors   algorithm respectively  gradients  magnitude  gradients ksgd kalmanbased stochastic gradient descent ksgd   online  offline algorithm  learning parameters  statistical problems  quasilikelihood models  include linear models nonlinear models generalized linear models  neural networks  squared error loss  special cases  online learning problems ksgd   special case   kalman filter  linear regression problems  special case   extended kalman filter  nonlinear regression problems  can  viewed   incremental gaussnewton method  benefits  ksgd  comparison   methods     sensitive   condition number   problem psigma  probability converging    rate depending  formula  formula   variance   residuals moreover  specific choices  formula ksgds objective function bias  iteration formula can  shown   formula  probability converging    rate depending  formula  formula   optimal parameter    robust choice  hyperparameters     stopping condition  drawbacks  ksgd    algorithm requires storing  dense covariance matrix  iterations  requires  matrixvector product   iteration  describe  algorithm suppose formula  formula  defined   example formula    formula  mean function ie  expected value  formula given formula  formula   variance function ie  variance  formula given formula   parameter update formula  covariance matrix update formula  given   following  formula  hyperparameters  formula update can result   covariance matrix becoming indefinite  can  avoided   cost   matrixmatrix multiplication formula can   positive definite symmetric matrix   typically taken    identity  noted  patel   problems besides linear regression restarts  required  ensure convergence   algorithm   theoretical  implementation details  given   closely related offline minibatch method  nonlinear regression analyzed  bertsekas  forgetting factor  used   covariance matrix update  prove convergence\r\n"}
{"index":{"_id":22}}
{"conceptLabelTag":"beam search","conceptLabel":"beam search","conceptDescription":"beam search  computer science beam search   heuristic search algorithm  explores  graph  expanding   promising node   limited set beam search   optimization  bestfirst search  reduces  memory requirements bestfirst search   graph search  orders  partial solutions states according   heuristic  attempts  predict  close  partial solution    complete solution goal state   beam search   predetermined number  best partial solutions  kept  candidates details beam search uses breadthfirst search  build  search tree   level   tree  generates  successors   states   current level sorting   increasing order  heuristic cost however   stores  predetermined number  best states   level called  beam width   states  expanded next  greater  beam width  fewer states  pruned   infinite beam width  states  pruned  beam search  identical  bestfirst search  beam width bounds  memory required  perform  search since  goal state  potentially  pruned beam search sacrifices completeness  guarantee   algorithm will terminate   solution  one exists beam search   optimal      guarantee   will find  best solution  returns  first solution found  beam width can either  fixed  variable one approach  uses  variable beam width starts   width   minimum   solution  found  beam  widened   procedure  repeated name  term beam search  coined  raj reddy carnegie mellon university uses  beam search   often used  maintain tractability  large systems  insufficient amount  memory  store  entire search tree  example   used  many machine translation systems  select  best translation  part  processed  many different ways  translating  words appear  top best translations according   sentence structures  kept   rest  discarded  translator  evaluates  translations according   given criterion choosing  translation  best keeps  goals  first use   beam search    harpy speech recognition system cmu extensions beam search   made complete  combining   depthfirst search resulting  beam stack search  depthfirst beam search   limited discrepancy search resulting  beam search using limited discrepancy backtracking bulb  resulting search algorithms  anytime algorithms  find good  likely suboptimal solutions quickly like beam search  backtrack  continue  find improved solutions  convergence   optimal solution\r\n"}
{"index":{"_id":23}}
{"conceptLabelTag":"unsupervised learning","conceptLabel":"unsupervised learning","conceptDescription":"unsupervised learning unsupervised machine learning   machine learning task  inferring  function  describe hidden structure  unlabeled data  classification  categorization   included   observations since  examples given   learner  unlabeled    objective evaluation   accuracy   structure   output   relevant algorithmwhich  one way  distinguishing unsupervised learning  supervised learning  reinforcement learning  central case  unsupervised learning   problem  density estimation  statistics though unsupervised learning encompasses many  problems  solutions involving summarizing  explaining key features   data approaches  unsupervised learning include unsupervised learning  neural networks  classical example  unsupervised learning   study   natural  artificial neural networks  subsumed  donald hebbs principle   neurons  fire together wire together  hebbian learning  connection  reinforced irrespective   error   exclusively  function   coincidence  action potentials   two neurons  similar version  modifies synaptic weights takes  account  time   action potentials spiketimingdependent plasticity  stdp hebbian learning   hypothesized  underlie  range  cognitive functions   pattern recognition  experiential learning among neural network models  selforganizing map som  adaptive resonance theory art  commonly used unsupervised learning algorithms  som   topographic organization   nearby locations   map represent inputs  similar properties  art model allows  number  clusters  vary  problem size  lets  user control  degree  similarity  members    clusters  means   userdefined constant called  vigilance parameter art networks  also used  many pattern recognition tasks   automatic target recognition  seismic signal processing  first version  art  art developed  carpenter  grossberg method  moments one   statistical approaches  unsupervised learning   method  moments   method  moments  unknown parameters  interest   model  related   moments  one   random variables  thus  unknown parameters can  estimated given  moments  moments  usually estimated  samples empirically  basic moments  first  second order moments   random vector  first order moment   mean vector   second order moment   covariance matrix   mean  zero higher order moments  usually represented using tensors    generalization  matrices  higher orders  multidimensional arrays  particular  method  moments  shown   effective  learning  parameters  latent variable models latent variable models  statistical models   addition   observed variables  set  latent variables also exists    observed  highly practical example  latent variable models  machine learning   topic modeling    statistical model  generating  words observed variables   document based   topic latent variable   document   topic modeling  words   document  generated according  different statistical parameters   topic   document  changed   shown  method  moments tensor decomposition techniques consistently recover  parameters   large class  latent variable models   assumptions  expectationmaximization algorithm em  also one    practical methods  learning latent variable models however  can get stuck  local optima     guaranteed   algorithm will converge   true unknown parameters   model alternatively   method  moments  global convergence  guaranteed   conditions examples behavioralbased detection  network security  become  good application area   combination  supervised  unsupervisedmachine learning     amount  data   human security analyst  analyze  impossible measured  terabytes per day  review  find patterns  anomalies according  giora engel cofounder  lightcyber   dark reading article  great promise machine learning holds   security industry   ability  detect advanced  unknown attacks particularly  leading  data breaches  basic premise    motivated attacker will find  way   network generally  compromising  users computer  network account  phishing social engineering  malware  security challenge  becomes finding  attacker   operational activities  include reconnaissance lateral movement command control  exfiltration  activitiesespecially reconnaissance  lateral movementstand  contrast   established baseline  normal  good activity   user  device   network  role  machine learning   create ongoing profiles  users  devices   find meaningful anomalies\r\n"}
{"index":{"_id":24}}
{"conceptLabelTag":"ensemble learning","conceptLabel":"ensemble learning","conceptDescription":"ensemble learning  statistics  machine learning ensemble methods use multiple learning algorithms  obtain better predictive performance    obtained     constituent learning algorithms alone unlike  statistical ensemble  statistical mechanics   usually infinite  machine learning ensemble refers    concrete finite set  alternative models  typically allows  much  flexible structure  exist among  alternatives overview supervised learning algorithms  commonly described  performing  task  searching   hypothesis space  find  suitable hypothesis  will make good predictions   particular problem even   hypothesis space contains hypotheses    wellsuited   particular problem  may   difficult  find  good one ensembles combine multiple hypotheses  form  hopefully better hypothesis  term ensemble  usually reserved  methods  generate multiple hypotheses using   base learner  broader term  multiple classifier systems also covers hybridization  hypotheses    induced    base learner evaluating  prediction   ensemble typically requires  computation  evaluating  prediction   single model  ensembles may  thought    way  compensate  poor learning algorithms  performing  lot  extra computation fast algorithms   decision trees  commonly used  ensemble methods  example random forest although slower algorithms can benefit  ensemble techniques  well  analogy ensemble techniques   used also  unsupervised learning scenarios  example  consensus clustering   anomaly detection ensemble theory  ensemble    supervised learning algorithm   can  trained   used  make predictions  trained ensemble therefore represents  single hypothesis  hypothesis however   necessarily contained within  hypothesis space   models     built thus ensembles can  shown    flexibility   functions  can represent  flexibility can  theory enable   overfit  training data    single model    practice  ensemble techniques especially bagging tend  reduce problems related  overfitting   training data empirically ensembles tend  yield better results     significant diversity among  models many ensemble methods therefore seek  promote diversity among  models  combine although perhaps nonintuitive  random algorithms like random decision trees can  used  produce  stronger ensemble   deliberate algorithms like entropyreducing decision trees using  variety  strong learning algorithms however   shown    effective  using techniques  attempt  dumbdown  models  order  promote diversity ensemble size   number  component classifiers   ensemble   great impact   accuracy  prediction    limited number  studies addressing  problem  priori determining  ensemble size   volume  velocity  big data streams make  even  crucial  online ensemble classifiers mostly statistical tests  used  determining  proper number  components  recently  theoretical framework suggested     ideal number  component classifiers   ensemble     less   number  classifiers  deteriorate  accuracy   called  law  diminishing returns  ensemble construction  theoretical framework shows  using   number  independent component classifiers  class labels gives  highest accuracy common types  ensembles bayes optimal classifier  bayes optimal classifier   classification technique    ensemble    hypotheses   hypothesis space  average   ensemble can outperform   hypothesis  given  vote proportional   likelihood   training dataset   sampled   system   hypothesis  true  facilitate training data  finite size  vote   hypothesis  also multiplied   prior probability   hypothesis  bayes optimal classifier can  expressed   following equation  formula   predicted class formula   set   possible classes formula   hypothesis space formula refers   probability  formula   training data   ensemble  bayes optimal classifier represents  hypothesis    necessarily  formula  hypothesis represented   bayes optimal classifier however   optimal hypothesis  ensemble space  space   possible ensembles consisting   hypotheses  formula unfortunately  bayes optimal classifier   practically implemented      simple  problems   several reasons   bayes optimal classifier   practically implemented bootstrap aggregating bagging bootstrap aggregating often abbreviated  bagging involves   model   ensemble vote  equal weight  order  promote model variance bagging trains  model   ensemble using  randomly drawn subset   training set   example  random forest algorithm combines random decision trees  bagging  achieve  high classification accuracy  interesting application  bagging  unsupervised learning  provided  boosting boosting involves incrementally building  ensemble  training  new model instance  emphasize  training instances  previous models misclassified   cases boosting   shown  yield better accuracy  bagging   also tends    likely  overfit  training data  far   common implementation  boosting  adaboost although  newer algorithms  reported  achieve better results bayesian parameter averaging bayesian parameter averaging bpa   ensemble technique  seeks  approximate  bayes optimal classifier  sampling hypotheses   hypothesis space  combining  using bayes law unlike  bayes optimal classifier bayesian model averaging bma can  practically implemented hypotheses  typically sampled using  monte carlo sampling technique   mcmc  example gibbs sampling may  used  draw hypotheses   representative   distribution formula    shown   certain circumstances  hypotheses  drawn   manner  averaged according  bayes law  technique   expected error   bounded     twice  expected error   bayes optimal classifier despite  theoretical correctness   technique early work showed experimental results suggesting   method promoted overfitting  performed worse compared  simpler ensemble techniques   bagging however  conclusions appear   based   misunderstanding   purpose  bayesian model averaging vs model combination additionally    considerable advances  theory  practice  bma recent rigorous proofs demonstrate  accuracy  bma  variable selection  estimation  highdimensional settings  provide empirical evidence highlighting  role  sparsityenforcing priors within  bma  alleviating overfitting bayesian model combination bayesian model combination bmc   algorithmic correction  bayesian model averaging bma instead  sampling  model   ensemble individually  samples   space  possible ensembles  model weightings drawn randomly   dirichlet distribution  uniform parameters  modification overcomes  tendency  bma  converge toward giving    weight   single model although bmc  somewhat  computationally expensive  bma  tends  yield dramatically better results  results  bmc   shown   better  average  statistical significance  bma  bagging  use  bayes law  compute model weights necessitates computing  probability   data given  model typically none   models   ensemble  exactly  distribution    training data  generated     correctly receive  value close  zero   term   work well   ensemble  big enough  sample  entire modelspace    rarely possible consequently  pattern   training data will cause  ensemble weight  shift toward  model   ensemble   closest   distribution   training data  essentially reduces   unnecessarily complex method   model selection  possible weightings   ensemble can  visualized  lying   simplex   vertex   simplex    weight  given   single model   ensemble bma converges toward  vertex   closest   distribution   training data  contrast bmc converges toward  point   distribution projects onto  simplex   words instead  selecting  one model   closest   generating distribution  seeks  combination  models   closest   generating distribution  results  bma can often  approximated  using crossvalidation  select  best model   bucket  models likewise  results  bmc may  approximated  using crossvalidation  select  best ensemble combination   random sampling  possible weightings bucket  models  bucket  models   ensemble    model selection algorithm  used  choose  best model   problem  tested   one problem  bucket  models can produce  better results   best model   set   evaluated across many problems  will typically produce much better results  average   model   set   common approach used  modelselection  crossvalidation selection sometimes called  bakeoff contest   described   following pseudocode crossvalidation selection can  summed   try     training set  pick  one  works best gating   generalization  crossvalidation selection  involves training another learning model  decide    models   bucket  bestsuited  solve  problem often  perceptron  used   gating model  can  used  pick  best model   can  used  give  linear weight   predictions   model   bucket   bucket  models  used   large set  problems  may  desirable  avoid training    models  take  long time  train landmark learning   metalearning approach  seeks  solve  problem  involves training   fast  imprecise algorithms   bucket   using  performance   algorithms  help determine  slow  accurate algorithm   likely   best stacking stacking sometimes called stacked generalization involves training  learning algorithm  combine  predictions  several  learning algorithms first     algorithms  trained using  available data   combiner algorithm  trained  make  final prediction using   predictions    algorithms  additional inputs   arbitrary combiner algorithm  used  stacking can theoretically represent    ensemble techniques described   article although  practice  singlelayer logistic regression model  often used   combiner stacking typically yields performance better   single one   trained models    successfully used   supervised learning tasks  unsupervised learning density estimation   also  used  estimate baggings error rate    reported  outperform bayesian modelaveraging  two topperformers   netflix competition utilized blending  may  considered    form  stacking\r\n"}
{"index":{"_id":25}}
{"conceptLabelTag":"novelty detection","conceptLabel":"novelty detection","conceptDescription":"novelty detection novelty detection   identification  new  unknown data   machine learning system    trained     previously aware    help  either statistical  machine learning based approaches novelty detection  one   fundamental requirements   good classification system  machine learning system can never  trained    possible object classes  hence  performance   network will  poor   classes   underrepresented   training set  good classification system must   ability  differentiate  known  unknown objects  testing   purpose different models  novelty detection   proposed novelty detection   hard problem  machine learning since  depends   statistics   already known information  generally applicable parameter method  outlier detection   highdimensional space   yet known novelty detection finds  variety  applications especially  signal processing computer vision pattern recognition data mining  robotics another important application   detection   disease  potential fault whose class may  underrepresented   training set  statistical approaches  novelty detection may  classified  parametric  nonparametric approaches parametric approaches assume  specific statistical distribution    gaussian distribution  data  statistical modeling based  data mean  covariance whereas nonparametric approaches   make  assumption   statistical properties  data\r\n"}
{"index":{"_id":26}}
{"conceptLabelTag":"recommender system","conceptLabel":"recommender system","conceptDescription":"recommender system recommender systems  recommendation systems sometimes replacing system   synonym   platform  engine   subclass  information filtering system  seek  predict  rating  preference   user  give   item recommender systems  become increasingly popular  recent years   utilized   variety  areas including movies music news books research articles search queries social tags  products  general   also recommender systems  experts collaborators jokes restaurants garments financial services life insurance romantic partners online dating  twitter pages overview recommender systems typically produce  list  recommendations  one  two ways  collaborative  contentbased filtering   personalitybased approach collaborative filtering approaches building  model   users past behavior items previously purchased  selected andor numerical ratings given   items  well  similar decisions made   users  model   used  predict items  ratings  items   user may   interest  contentbased filtering approaches utilize  series  discrete characteristics   item  order  recommend additional items  similar properties  approaches  often combined see hybrid recommender systems  differences  collaborative  contentbased filtering can  demonstrated  comparing two popular music recommender systems lastfm  pandora radio  type  system    strengths  weaknesses    example lastfm requires  large amount  information   user  order  make accurate recommendations    example   cold start problem   common  collaborative filtering systems  pandora needs  little information  get started   far  limited  scope  example  can  make recommendations   similar   original seed recommender systems   useful alternative  search algorithms since  help users discover items  might   found   interestingly enough recommender systems  often implemented using search engines indexing nontraditional data montaner provided  first overview  recommender systems   intelligent agent perspective adomavicius provided  new alternate overview  recommender systems herlocker provides  additional overview  evaluation techniques  recommender systems  beel et al discussed  problems  offline evaluations beel et al  also provided literature surveys  available research paper recommender systems  existing challenges approaches collaborative filtering one approach   design  recommender systems   wide use  collaborative filtering collaborative filtering methods  based  collecting  analyzing  large amount  information  users behaviors activities  preferences  predicting  users will like based   similarity   users  key advantage   collaborative filtering approach      rely  machine analyzable content  therefore   capable  accurately recommending complex items   movies without requiring  understanding   item  many algorithms   used  measuring user similarity  item similarity  recommender systems  example  knearest neighbor knn approach   pearson correlation  first implemented  allen collaborative filtering  based   assumption  people  agreed   past will agree   future    will like similar kinds  items   liked   past  building  model   users behavior  distinction  often made  explicit  implicit forms  data collection examples  explicit data collection include  following examples  implicit data collection include  following  recommender system compares  collected data  similar  dissimilar data collected  others  calculates  list  recommended items   user several commercial  noncommercial examples  listed   article  collaborative filtering systems one    famous examples  collaborative filtering  itemtoitem collaborative filtering people  buy x also buy y  algorithm popularized  amazoncoms recommender system  examples include collaborative filtering approaches often suffer  three problems cold start scalability  sparsity  particular type  collaborative filtering algorithm uses matrix factorization  lowrank matrix approximation technique collaborative filtering methods  classified  memorybased  model based collaborative filtering  wellknown example  memorybased approaches  userbased algorithm    modelbased approaches  kernelmapping recommender contentbased filtering another common approach  designing recommender systems  contentbased filtering contentbased filtering methods  based   description   item   profile   users preference   contentbased recommender system keywords  used  describe  items   user profile  built  indicate  type  item  user likes   words  algorithms try  recommend items   similar     user liked   past   examining   present  particular various candidate items  compared  items previously rated   user   bestmatching items  recommended  approach   roots  information retrieval  information filtering research  abstract  features   items   system  item presentation algorithm  applied  widely used algorithm   tfidf representation also called vector space representation  create  user profile  system mostly focuses  two types  information  model   users preference  history   users interaction   recommender system basically  methods use  item profile ie  set  discrete attributes  features characterizing  item within  system  system creates  contentbased profile  users based   weighted vector  item features  weights denote  importance   feature   user  can  computed  individually rated content vectors using  variety  techniques simple approaches use  average values   rated item vector   sophisticated methods use machine learning techniques   bayesian classifiers cluster analysis decision trees  artificial neural networks  order  estimate  probability   user  going  like  item direct feedback   user usually   form   like  dislike button can  used  assign higher  lower weights   importance  certain attributes using rocchio classification   similar techniques  key issue  contentbased filtering  whether  system  able  learn user preferences  users actions regarding one content source  use  across  content types   system  limited  recommending content    type   user  already using  value   recommendation system  significantly less    content types   services can  recommended  example recommending news articles based  browsing  news  useful    much  useful  music videos products discussions etc  different services can  recommended based  news browsing  previously detailed pandora radio   popular example   contentbased recommender system  plays music  similar characteristics     song provided   user   initial seed   also  large number  contentbased recommender systems aimed  providing movie recommendations    examples include rotten tomatoes internet movie database jinni rovi corporation  jaman document related recommender systems aim  providing document recommendations  knowledge workers public health professionals   studying recommender systems  personalize health education  preventative strategies hybrid recommender systems recent research  demonstrated   hybrid approach combining collaborative filtering  contentbased filtering    effective   cases hybrid approaches can  implemented  several ways  making contentbased  collaborativebased predictions separately   combining   adding contentbased capabilities   collaborativebased approach  vice versa   unifying  approaches  one model see   complete review  recommender systems several studies empirically compare  performance   hybrid   pure collaborative  contentbased methods  demonstrate   hybrid methods can provide  accurate recommendations  pure approaches  methods can also  used  overcome    common problems  recommender systems   cold start   sparsity problem netflix   good example   use  hybrid recommender systems  website makes recommendations  comparing  watching  searching habits  similar users ie collaborative filtering  well   offering movies  share characteristics  films   user  rated highly contentbased filtering  variety  techniques   proposed   basis  recommender systems collaborative contentbased knowledgebased  demographic techniques    techniques  known shortcomings    well known coldstart problem  collaborative  contentbased systems     new users   ratings   knowledge engineering bottleneck  knowledgebased approaches  hybrid recommender system  one  combines multiple techniques together  achieve  synergy    term hybrid recommender system  used   describe  recommender system  combines multiple recommendation techniques together  produce  output    reason  several different techniques    type    hybridized  example two different contentbased recommenders  work together   number  projects  investigated  type  hybrid newsdude  uses  naive bayes  knn classifiers   news recommendations  just one example seven hybridization techniques beyond accuracy typically research  recommender systems  concerned  finding   accurate recommendation algorithms however    number  factors   also important mobile recommender systems one growing area  research   area  recommender systems  mobile recommender systems   increasing ubiquity  internetaccessing smart phones   now possible  offer personalized contextsensitive recommendations    particularly difficult area  research  mobile data   complex  data  recommender systems often   deal    heterogeneous noisy requires spatial  temporal autocorrelation   validation  generality problems additionally mobile recommender systems suffer   transplantation problem recommendations may  apply   regions  instance    unwise  recommend  recipe   area     ingredients may   available one example   mobile recommender system  one  offers potentially profitable driving routes  taxi drivers   city  system takes input data   form  gps traces   routes  taxi drivers took  working  include location latitude  longitude time stamps  operational status   without passengers  uses  data  recommend  list  pickup points along  route   goal  optimizing occupancy times  profits  type  system  obviously locationdependent  since  must operate   handheld  embedded device  computation  energy requirements must remain low another example  mobile recommendation   bouneffouf et al developed  professional users using gps traces   user   agenda  suggests suitable information depending   situation  interests  system uses machine learning techniques  reasoning processes  order  dynamically adapt  mobile recommender system   evolution   users interest  author called  algorithm hybridgreedy mobile recommendation systems  also  successfully built using  web  data   source  structured information  good example   system  smartmuseum  system uses semantic modelling information retrieval  machine learning techniques  order  recommend content matching user interests even  presented  sparse  minimal user data riskaware recommender systems  majority  existing approaches  recommender systems focus  recommending   relevant content  users using contextual information    take  account  risk  disturbing  user  specific situation however  many applications   recommending personalized content   also important  consider  risk  upsetting  user     push recommendations  certain circumstances  instance   professional meeting early morning  late  night therefore  performance   recommender system depends  part   degree     incorporated  risk   recommendation process risk definition  risk  recommender systems   possibility  disturb   upset  user  leads   bad answer   user  response   challenges  authors  drars  dynamic riskaware recommender system  developed  dynamic risk sensitive recommendation system called drars dynamic riskaware recommender system  models  contextaware recommendation   bandit problem  system combines  contentbased technique   contextual bandit algorithm   shown  drars improves  upper confidence bound ucb policy  currently available best algorithm  calculating   optimal exploration value  maintain  tradeoff  exploration  exploitation based   risk level   current users situation  authors conducted experiments   industrial context  real data  real users   shown  taking  account  risk level  users situations significantly increased  performance   recommender systems  netflix prize one   key events  energized research  recommender systems   netflix prize   netflix sponsored  competition offering  grand prize    team   take  offered dataset   million movie ratings  return recommendations    accurate   offered   companys existing recommender system  competition energized  search  new   accurate algorithms  september  grand prize  us  given   bellkors pragmatic chaos team using tiebreaking rules   accurate algorithm  used  ensemble method  different algorithmic approaches blended   single prediction predictive accuracy  substantially improved  blending multiple predictors  experience    efforts   concentrated  deriving substantially different approaches rather  refining  single technique consequently  solution   ensemble  many methods many benefits accrued   web due   netflix project  teams  taken  technology  applied    markets  members   team  finished second place founded gravity rd  recommendation engine thats active   recsys community tell inc created  netflix projectderived solution  ecommerce websites  second contest  planned   ultimately canceled  response   ongoing lawsuit  concerns   federal trade commission performance measures evaluation  important  assessing  effectiveness  recommendation algorithms  commonly used metrics   mean squared error  root mean squared error  latter   used   netflix prize  information retrieval metrics   precision  recall  dcg  useful  assess  quality   recommendation method recently diversity novelty  coverage  also considered  important aspects  evaluation however many   classic evaluation measures  highly criticized often results  socalled offline evaluations   correlate  actually assessed usersatisfaction  authors conclude   suggest treating results  offline evaluations ie classic performance measures  skepticism multicriteria recommender systems multicriteria recommender systems mcrs can  defined  recommender systems  incorporate preference information upon multiple criteria instead  developing recommendation techniques based   single criterion values  overall preference  user u   item   systems try  predict  rating  unexplored items  u  exploiting preference information  multiple criteria  affect  overall preference value several researchers approach mcrs   multicriteria decision making mcdm problem  apply mcdm methods  techniques  implement mcrs systems see  chapter   extended introduction recommender system evaluation  measure  effectiveness  recommender systems  compare different approaches three types  evaluations  available user studies online evaluations ab tests  offline evaluations user studies  rather small scale   dozens  hundreds  users  presented recommendations created  different recommendation approaches    users judge  recommendations  best  ab tests recommendations  shown  typically thousands  users   real product   recommender system randomly picks  least two different recommendation approaches  generate recommendations  effectiveness  measured  implicit measures  effectiveness   conversion rate  clickthrough rate offline evaluations  based  historic data eg  dataset  contains information   users previously rated movies  effectiveness  recommendation approaches   measured based   well  recommendation approach can predict  users ratings   dataset   rating   explicit expression whether  user liked  movie    information   available   domains  instance   domain  citation recommender systems users typically   rate  citation  recommended article   cases offline evaluations may use implicit measures  effectiveness  instance  may  assumed   recommender system  effective   able  recommend  many articles  possible   contained   research articles reference list however  kind  offline evaluations  seen critical  many researchers  instance    shown  results  offline evaluations  low correlation  results  user studies  ab tests reproducibility  recommender system research  recent years    growing understanding   community  lots  previous research  little impact   practical application  recommender systems ekstrand konstan et al       renown researchers  recommender systems criticize    currently difficult  reproduce  extend recommender systems research results   evaluations   handled consistently konstan  adomavicius conclude   recommender systems research community  facing  crisis   significant number  papers present results  contribute little  collective knowledge often   research lacks  evaluation   properly judged  hence  provide meaningful contributions   consequence lots  research  recommender systes can  considered   reproducible hence operators  recommender systems find little guidance   current research  answering  question  recommendation approaches  use   recommender systems  researchers demonstrated  minor variations   recommendation algorithms  scenarios led  strong changes   effectiveness   recommender system  conclude  seven actions  necessary  improve  current situation survey  research fields  learn   find  common understanding  reproducibility identify  understand  determinants  affect reproducibility conduct  comprehensive experiments modernize publication practices foster  development  use  recommendation frameworks  establish bestpractice guidelines  recommendersystems research  reproducibility    considered   long time   recommendersystem community  aspects  much  considered recently  several workshops  conferences focusing  reproducibility  recommender system research kim falk practical recommender systems isbn \r\n"}
{"index":{"_id":27}}
{"conceptLabelTag":"variational message passing","conceptLabel":"variational message passing","conceptDescription":"variational message passing variational message passing vmp   approximate inference technique  continuous  discretevalued bayesian networks  conjugateexponential parents developed  john winn vmp  developed   means  generalizing  approximate variational methods used   techniques  latent dirichlet allocation  works  updating  approximate distribution   node  messages   nodes markov blanket likelihood lower bound given  set  hidden variables formula  observed variables formula  goal  approximate inference   lowerbound  probability   graphical model    configuration formula   probability distribution formula   defined later    define  lower bound     likelihood  simply  bound plus  relative entropy  formula  formula   relative entropy  nonnegative  function formula defined   indeed  lower bound   log likelihood   observation formula  distribution formula will   simpler character    formula  marginalizing  formula  intractable     simplest  graphical models  particular vmp uses  factorized distribution formula  formula   disjoint part   graphical model determining  update rule  likelihood estimate needs    large  possible    lower bound getting closer formula improves  approximation   log likelihood  substituting   factorized version  formula formula parameterized   hidden nodes formula    simply  negative relative entropy  formula  formula plus  terms independent  formula  formula  defined   formula   expectation   distributions formula except formula thus   set formula   formula  bound formula  maximized messages  variational message passing parents send  children  expectation   sufficient statistic  children send  parents  natural parameter  also requires messages   sent   coparents   node relationship  exponential families   nodes  vmp come  exponential families   parents  nodes  conjugate   children nodes  expectation   sufficient statistic can  computed   normalization factor vmp algorithm  algorithm begins  computing  expected value   sufficient statistics   vector    likelihood converges   stable value   usually accomplished  setting  small threshold value  running  algorithm   increases  less   threshold value   following   node constraints  every child must  conjugate   parent  limits  types  distributions  can  used   model  example  parents   gaussian distribution must   gaussian distribution corresponding   mean   gamma distribution corresponding   precision  one  formula   common parameterizations discrete variables can  dirichlet parents  poisson  exponential nodes must  gamma parents however   data can  modeled   manner vmp offers  generalized framework  providing inference\r\n"}
{"index":{"_id":28}}
{"conceptLabelTag":"rademacher complexity","conceptLabel":"rademacher complexity","conceptDescription":"rademacher complexity  computational learning theory machine learning  theory  computation rademacher complexity named  hans rademacher measures richness   class  realvalued functions  respect   probability distribution definitions rademacher complexity   set given  set formula  rademacher complexity    defined  follows  formula  independent random variables drawn   rademacher distribution ie formula  formula rademacher complexity   function class given  sample formula   class formula  realvalued functions defined   domain space formula  empirical rademacher complexity  formula given formula  defined   can also  written using  previous definition  formula denotes function composition ie let formula   probability distribution  formula  rademacher complexity   function class formula  respect  formula  sample size formula     expectation  taken   identically independently distributed iid sample formula generated according  formula examples formula contains  single vector eg formula  formula contains two vectors eg formula  using  rademacher complexity  rademacher complexity can  used  derive datadependent upperbounds   learnability  function classes intuitively  functionclass  smaller rademacher complexity  easier  learn bounding  representativeness  machine learning   desired    training set  represents  true distribution  samples  can  quantified using  notion  representativeness denote  p  probability distribution    samples  drawn denote  formula  set  hypotheses potential classifiers  denote  formula  corresponding set  error functions ie  every formula    function formula  maps  training sample featureslabel   error   classifier formula   sample  example    binary classification   error function   simple loss  formula   function  returns   training sample   formula  correct    training sample   formula  wrong define  representativeness   sample formula  respect  formula  formula  defined  smaller representativeness  better since  means   empirical error   classifier   training set   much higher   true error  expected representativeness   sample can  bounded   expected rademacher complexity   function class bounding  generalization error   rademacher complexity  small   possible  learn  hypothesis class h using empirical risk minimization  example  binary error function  every formula  probability  least formula  every hypothesis formula bounding  rademacher complexity since smaller rademacher complexity  better   useful   upper bounds   rademacher complexity  various function sets  following rules can  used  upperbound  rademacher complexity   set formula   vectors  formula  translated   constant vector formula  rada   change   vectors  formula  multiplied   scalar formula  rada  multiplied  formula kakadetewari lemma   vectors  formula  operated   lipschitz function  rada    multiplied   lipschitz constant   function  particular   vectors  formula  operated   contraction mapping  rada strictly decreases  rademacher complexity   convex hull  formula equals rada massart lemma  rademacher complexity   finite set grows logarithmically   set size formally let formula   set  formula vectors  formula  let formula   mean   vectors  formula   particular  formula   set  binary vectors  norm    formula  bounds related   vc dimension let formula   set family whose vc dimension  formula   known   growth function  formula  bounded   means   every set formula    formula elements formula  setfamily formula can  considered   set  binary vectors  formula substituting   massarts lemma gives   advanced techniques dudleys entropy bound  hausslers upper bound one can show  example   exists  constant formula    class  formulaindicator functions  vapnikchervonenkis dimension formula  rademacher complexity upperbounded  formula bounds related  linear classes  following bounds  related  linear operations  formula  constant set  formula vectors  formula define formula  set  dotproducts   vectors  formula  vectors   unit ball  define formula  set  dotproducts   vectors  formula  vectors   unit ball   norm  bounds related  covering numbers  following bound relates  rademacher complexity   set formula   external covering number  number  balls   given radius formula whose union contains formula  bound  attributed  dudley suppose formula   set  vectors whose length norm    formula   every integer formula  particular  formula lies   ddimensional subspace  formula  substituting    previous bound gives  following bound   rademacher complexity gaussian complexity gaussian complexity   similar complexity  similar physical meanings  can  obtained   rademacher complexity using  random variables formula instead  formula  formula  gaussian iid random variables  zeromean  variance ie formula\r\n"}
{"index":{"_id":29}}
{"conceptLabelTag":"inductive bias","conceptLabel":"inductive bias","conceptDescription":"inductive bias  inductive bias also known  learning bias   learning algorithm   set  assumptions   learner uses  predict outputs given inputs     encountered  machine learning one aims  construct algorithms   able  learn  predict  certain target output  achieve   learning algorithm  presented  training examples  demonstrate  intended relation  input  output values   learner  supposed  approximate  correct output even  examples     shown  training without  additional assumptions  problem   solved exactly since unseen situations might   arbitrary output value  kind  necessary assumptions   nature   target function  subsumed   phrase inductive bias  classical example   inductive bias  occams razor assuming   simplest consistent hypothesis   target function  actually  best  consistent means   hypothesis   learner yields correct outputs     examples    given   algorithm approaches    formal definition  inductive bias  based  mathematical logic   inductive bias   logical formula  together   training data logically entails  hypothesis generated   learner unfortunately  strict formalism fails  many practical cases   inductive bias can   given   rough description eg   case  neural networks     types  following   list  common inductive biases  machine learning algorithms shift  bias although  learning algorithms   static bias  algorithms  designed  shift  bias   acquire  data    avoid bias since  bias shifting process  must   bias\r\n"}
{"index":{"_id":30}}
{"conceptLabelTag":"expectation maximization algorithm","conceptLabel":"expectation maximization algorithm","conceptDescription":"expectationmaximization algorithm  statistics  expectationmaximization em algorithm   iterative method  find maximum likelihood  maximum  posteriori map estimates  parameters  statistical models   model depends  unobserved latent variables  em iteration alternates  performing  expectation e step  creates  function   expectation   loglikelihood evaluated using  current estimate   parameters   maximization m step  computes parameters maximizing  expected loglikelihood found   e step  parameterestimates   used  determine  distribution   latent variables   next e step history  em algorithm  explained  given  name   classic paper  arthur dempster nan laird  donald rubin  pointed    method   proposed many times  special circumstances  earlier authors   detailed treatment   em method  exponential families  published  rolf sundberg   thesis  several papers following  collaboration  per martinl f  anders martinl f  dempsterlairdrubin paper  generalized  method  sketched  convergence analysis   wider class  problems regardless  earlier inventions  innovative dempsterlairdrubin paper   journal   royal statistical society received  enthusiastic discussion   royal statistical society meeting  sundberg calling  paper brilliant  dempsterlairdrubin paper established  em method   important tool  statistical analysis  convergence analysis   dempsterlairdrubin paper  flawed   correct convergence analysis  published  cf jeff wu  wus proof established  em methods convergence outside   exponential family  claimed  dempsterlairdrubin introduction  em algorithm  used  find locally maximum likelihood parameters   statistical model  cases   equations   solved directly typically  models involve latent variables  addition  unknown parameters  known data observations   either missing values exist among  data   model can  formulated  simply  assuming  existence   unobserved data points  example  mixture model can  described  simply  assuming   observed data point   corresponding unobserved data point  latent variable specifying  mixture component    data point belongs finding  maximum likelihood solution typically requires taking  derivatives   likelihood function  respect    unknown values  parameters   latent variables  simultaneously solving  resulting equations  statistical models  latent variables   usually impossible instead  result  typically  set  interlocking equations    solution   parameters requires  values   latent variables  vice versa  substituting one set  equations    produces  unsolvable equation  em algorithm proceeds   observation   following   way  solve  two sets  equations numerically one can simply pick arbitrary values  one   two sets  unknowns use   estimate  second set  use  new values  find  better estimate   first set   keep alternating   two   resulting values  converge  fixed points   obvious   will work     can  proven    context      derivative   likelihood  arbitrarily close  zero   point   turn means   point  either  maximum   saddle point  general multiple maxima may occur   guarantee   global maximum will  found  likelihoods also  singularities   ie nonsensical maxima  example one   solutions  may  found  em   mixture model involves setting one   components   zero variance   mean parameter    component   equal  one   data points description given  statistical model  generates  set formula  observed data  set  unobserved latent data  missing values formula   vector  unknown parameters formula along   likelihood function formula  maximum likelihood estimate mle   unknown parameters  determined   marginal likelihood   observed data however  quantity  often intractable eg  formula   sequence  events    number  values grows exponentially   sequence length making  exact calculation   sum extremely difficult  em algorithm seeks  find  mle   marginal likelihood  iteratively applying  two steps  typical models   em  applied however   possible  apply em   sorts  models  motive   follows   value   parameters formula  known usually  value   latent variables formula can  found  maximizing  loglikelihood   possible values  formula either simply  iterating  formula    algorithm    viterbi algorithm  hidden markov models conversely   know  value   latent variables formula  can find  estimate   parameters formula fairly easily typically  simply grouping  observed data points according   value   associated latent variable  averaging  values   function   values   points   group  suggests  iterative algorithm   case   formula  formula  unknown  algorithm  just described monotonically approaches  local minimum   cost function   commonly called hard em  kmeans algorithm   example   class  algorithms however somewhat better methods exist rather  making  hard choice  formula given  current parameter values  averaging    set  data points associated   value  formula instead determine  probability   possible value  formula   data point   use  probabilities associated   value  formula  compute  weighted average   whole set  data points  resulting algorithm  commonly called soft em    type  algorithm normally associated  em  counts used  compute  weighted averages  called soft counts  opposed   hard counts used   hardemtype algorithm   kmeans  probabilities computed  formula  posterior probabilities     computed   e step  soft counts used  compute new parameter values    computed   m step properties speaking   expectation e step   bit   misnomer   calculated   first step   fixed datadependent parameters   function q   parameters  q  known   fully determined   maximized   second m step   em algorithm although  em iteration  increase  observed data ie marginal likelihood function  guarantee exists   sequence converges   maximum likelihood estimator  multimodal distributions  means   em algorithm may converge   local maximum   observed data likelihood function depending  starting values  variety  heuristic  metaheuristic approaches exist  escape  local maximum   randomrestart hill climbing starting  several different random initial estimates  applying simulated annealing methods em  especially useful   likelihood   exponential family  e step becomes  sum  expectations  sufficient statistics   m step involves maximizing  linear function    case   usually possible  derive closedform expression updates   step using  sundberg formula published  rolf sundberg using unpublished results  per martinl f  anders martinl f  em method  modified  compute maximum  posteriori map estimates  bayesian inference   original paper  dempster laird  rubin  methods exist  find maximum likelihood estimates   gradient descent conjugate gradient  variants   gaussnewton algorithm unlike em  methods typically require  evaluation  first andor second derivatives   likelihood function proof  correctness expectationmaximization works  improve formula rather  directly improving formula   shown  improvements   former imply improvements   latter   formula  nonzero probability formula  can write  take  expectation  possible values   unknown data formula   current parameter estimate formula  multiplying  sides  formula  summing  integrating  formula  lefthand side   expectation   constant   get  formula  defined   negated sum   replacing  last equation holds   value  formula including formula  subtracting  last equation   previous equation gives however gibbs inequality tells us  formula   can conclude   words choosing formula  improve formula beyond formula can  cause formula  decrease  formula    marginal likelihood   data  nondecreasing   maximizationmaximization procedure  em algorithm can  viewed  two alternating maximization steps     example  coordinate ascent consider  function  q   arbitrary probability distribution   unobserved data z  hq   entropy   distribution q  function can  written   formula   conditional distribution   unobserved data given  observed data formula  formula   kullbackleibler divergence   steps   em algorithm may  viewed  applications em  frequently used  data clustering  machine learning  computer vision  natural language processing two prominent instances   algorithm   baumwelch algorithm   insideoutside algorithm  unsupervised induction  probabilistic context grammars  psychometrics em  almost indispensable  estimating item parameters  latent abilities  item response theory models   ability  deal  missing data  observe unidentified variables em  becoming  useful tool  price  manage risk   portfolioref  em algorithm   faster variant ordered subset expectation maximization  also widely used  medical image reconstruction especially  positron emission tomography  single photon emission computed tomography see    faster variants  em  structural engineering  structural identification using expectation maximization stride algorithm   outputonly method  identifying natural vibration properties   structural system using sensor data see operational modal analysis filtering  smoothing em algorithms  kalman filter  typically used  online state estimation   minimumvariance smoother may  employed  offline  batch state estimation however  minimumvariance solutions require estimates   statespace model parameters em algorithms can  used  solving joint state  parameter estimation problems filtering  smoothing em algorithms arise  repeating  twostep procedure suppose   kalman filter  minimumvariance smoother operates  noisy measurements   singleinputsingleoutput system  updated measurement noise variance estimate can  obtained   maximum likelihood calculation  formula  scalar output estimates calculated   filter   smoother  n scalar measurements formula similarly   firstorder autoregressive process  updated process noise variance estimate can  calculated   formula  formula  scalar state estimates calculated   filter   smoother  updated model coefficient estimate  obtained via  convergence  parameter estimates      well studied variants  number  methods   proposed  accelerate  sometimes slow convergence   em algorithm    using conjugate gradient  modified newtons methods newtonraphson also em can  used  constrained estimation methods expectation conditional maximization ecm replaces  m step   sequence  conditional maximization cm steps    parameter  maximized individually conditionally    parameters remaining fixed  idea   extended  generalized expectation maximization gem algorithm    sought   increase   objective function f    e step  m step   alternative description gem   developed   distributed environment  shows promising results   also possible  consider  em algorithm   subclass   mm majorizeminimize  minorizemaximize depending  context algorithm  therefore use  machinery developed    general case em algorithm  qfunction used   em algorithm  based   log likelihood therefore   regarded   logem algorithm  use   log likelihood can  generalized     log likelihood ratio   log likelihood ratio   observed data can  exactly expressed  equality  using  qfunction   log likelihood ratio   divergence obtaining  qfunction   generalized e step  maximization   generalized m step  pair  called  em algorithm  contains  logem algorithm   subclass thus  em algorithm  yasuo matsuyama   exact generalization   logem algorithm  computation  gradient  hessian matrix  needed  em shows faster convergence   logem algorithm  choosing  appropriate  em algorithm leads   faster version   hidden markov model estimation algorithm hmm relation  variational bayes methods em   partially nonbayesian maximum likelihood method  final result gives  probability distribution   latent variables   bayesian style together   point estimate  either  maximum likelihood estimate   posterior mode  fully bayesian version   may  wanted giving  probability distribution    latent variables  bayesian approach  inference  simply  treat  another latent variable   paradigm  distinction   e  m steps disappears  using  factorized q approximation  described  variational bayes solving can iterate   latent variable now including  optimize  one   time now k steps per iteration  needed  k   number  latent variables  graphical models   easy     variables new q depends    markov blanket  local message passing can  used  efficient inference geometric interpretation  information geometry  e step   m step  interpreted  projections  dual affine connections called  econnection   mconnection  kullbackleibler divergence can also  understood   terms examples gaussian mixture let formula   sample  formula independent observations   mixture  two multivariate normal distributions  dimension formula  let formula   latent variables  determine  component    observation originates   aim   estimate  unknown parameters representing  mixing value   gaussians   means  covariances     incompletedata likelihood function    completedata likelihood function    formula   indicator function  formula   probability density function   multivariate normal  see  last equality      indicators formula  equal  zero except  one   equal  one  inner sum thus reduces  one term e step given  current estimate   parameters  conditional distribution   z  determined  bayes theorem    proportional height   normal density weighted    called  membership probabilities   normally considered  output   e step although     q function    e step corresponds   function  q  full conditional expectation   need   calculated  one step   appear  separate linear terms  can thus  maximized independently m step q  quadratic  form means  determining  maximizing values   relatively straightforward also  may   maximized independently since   appear  separate linear terms  begin consider    constraint     form   mle   binomial distribution    next estimates      form   weighted mle   normal distribution    symmetry termination conclude  iterative process  formula  formula   preset threshold generalization  algorithm illustrated  can  generalized  mixtures    two multivariate normal distributions truncated  censored regression  em algorithm   implemented   case   underlying linear regression model exists explaining  variation   quantity    values actually observed  censored  truncated versions   represented   model special cases   model include censored  truncated observations  one normal distribution alternatives em typically converges   local optimum  necessarily  global optimum   bound   convergence rate  general   possible   can  arbitrarily poor  high dimensions   can   exponential number  local optima hence  need exists  alternative methods  guaranteed learning especially   highdimensional setting alternatives  em exist  better guarantees  consistency   termed momentbased approaches   socalled spectral techniques momentbased approaches  learning  parameters   probabilistic model   increasing interest recently since  enjoy guarantees   global convergence  certain conditions unlike em   often plagued   issue  getting stuck  local optima algorithms  guarantees  learning can  derived   number  important models   mixture models hmms etc   spectral methods  spurious local optima occur   true parameters can  consistently estimated   regularity conditions\r\n"}
{"index":{"_id":31}}
{"conceptLabelTag":"weighted majority algorithm","conceptLabel":"weighted majority algorithm","conceptDescription":"weighted majority algorithm  machine learning weighted majority algorithm wma   metalearning algorithm used  construct  compound algorithm   pool  prediction algorithms     type  learning algorithms classifiers  even real human experts  algorithm assumes     prior knowledge   accuracy   algorithms   pool    sufficient reasons  believe  one   will perform well assume   problem   binary decision problem  construct  compound algorithm  positive weight  given     algorithms   pool  compound algorithm  collects weighted votes    algorithms   pool  gives  prediction    higher vote   compound algorithm makes  mistake  algorithms   pool  contributed   wrong predicting will  discounted   certain ratio   can  shown   upper bounds   number  mistakes made   given sequence  predictions   pool  algorithms formula   one algorithm  formula makes   formula mistakes   many variations   weighted majority algorithm  handle different situations like shifting targets infinite pools  randomized predictions  core mechanism remain similar   final performances   compound algorithm bounded   function   performance   specialist best performing algorithm   pool\r\n"}
{"index":{"_id":32}}
{"conceptLabelTag":"bayesian linear regression","conceptLabel":"bayesian linear regression","conceptDescription":"bayesian linear regression  statistics bayesian linear regression   approach  linear regression    statistical analysis  undertaken within  context  bayesian inference   regression model  errors    normal distribution    particular form  prior distribution  assumed explicit results  available   posterior probability distributions   models parameters model setup consider  standard linear regression problem    formula  specify  conditional distribution  formula given  formula predictor vector formula  formula   formula vector   formula  independent  identical normally distributed random variables  corresponds   following likelihood function  ordinary least squares solution   estimate  coefficient vector using  moorepenrose pseudoinverse  formula   formula design matrix  row     predictor vector formula  formula   column formulavector formula    frequentist approach   assumes    enough measurements  say something meaningful  formula   bayesian approach  data  supplemented  additional information   form   prior probability distribution  prior belief   parameters  combined   datas likelihood function according  bayes theorem  yield  posterior belief   parameters formula  formula  prior can take different functional forms depending   domain   information   available  priori  conjugate priors conjugate prior distribution   arbitrary prior distribution  may   analytical solution   posterior distribution   section  will consider  socalled conjugate prior    posterior distribution can  derived analytically  prior formula  conjugate   likelihood function      functional form  respect  formula  formula since  loglikelihood  quadratic  formula  loglikelihood  rewritten    likelihood becomes normal  formula write  likelihood  now rewritten    formula   number  regression coefficients  suggests  form   prior  formula   inversegamma distribution   notation introduced   inversegamma distribution article    density   formula distribution  formula  formula  formula  formula   prior values  formula  formula respectively equivalently  can also  described   scaled inverse chisquared distribution formula   conditional prior density formula   normal distribution   notation   normal distribution  conditional prior distribution  formula posterior distribution   prior now specified  posterior distribution can  expressed    rearrangement  posterior can  rewritten    posterior mean formula   parameter vector formula can  expressed  terms   least squares estimator formula   prior mean formula   strength   prior indicated   prior precision matrix formula  justify  formula  indeed  posterior mean  quadratic terms   exponential can  rearranged   quadratic form  formula now  posterior can  expressed   normal distribution times  inversegamma distribution therefore  posterior distribution can  parametrized  follows   two factors correspond   densities  formula  formula distributions   parameters   given   can  interpreted  bayesian learning   parameters  updated according   following equations model evidence  model evidence formula   probability   data given  model formula   also known   marginal likelihood    prior predictive density   model  defined   likelihood function formula   prior distribution   parameters ie formula  model evidence captures   single number  well   model explains  observations  model evidence   bayesian linear regression model presented   section can  used  compare competing linear models  bayesian model comparison  models may differ   number  values   predictor variables  well    priors   model parameters model complexity  already taken  account   model evidence   marginalizes   parameters  integrating formula   possible values  formula  formula  integral can  computed analytically   solution  given   following equation  formula denotes  gamma function    chosen  conjugate prior  marginal likelihood can also  easily computed  evaluating  following equality  arbitrary values  formula  formula note   equation  nothing   rearrangement  bayes theorem inserting  formulas   prior  likelihood   posterior  simplifying  resulting expression leads   analytic expression given   cases  general  may  impossible  impractical  derive  posterior distribution analytically however   possible  approximate  posterior   approximate bayesian inference method   monte carlo sampling  variational bayes  special case formula  called ridge regression  similar analysis can  performed   general case   multivariate regression  part   provides  bayesian estimation  covariance matrices see bayesian multivariate linear regression\r\n"}
{"index":{"_id":33}}
{"conceptLabelTag":"decision rules","conceptLabel":"decision rules","conceptDescription":"decision tree  decision tree   decision support tool  uses  treelike graph  model  decisions   possible consequences including chance event outcomes resource costs  utility   one way  display  algorithm decision trees  commonly used  operations research specifically  decision analysis  help identify  strategy  likely  reach  goal   also  popular tool  machine learning overview  decision tree   flowchartlike structure    internal node represents  test   attribute eg whether  coin flip comes  heads  tails  branch represents  outcome   test   leaf node represents  class label decision taken  computing  attributes  paths  root  leaf represents classification rules  decision analysis  decision tree   closely related influence diagram  used   visual  analytical decision support tool   expected values  expected utility  competing alternatives  calculated  decision tree consists  three types  nodes decision trees  commonly used  operations research  operations management   practice decisions    taken online   recall  incomplete knowledge  decision tree   paralleled   probability model   best choice model  online selection model algorithm another use  decision trees    descriptive means  calculating conditional probabilities decision trees influence diagrams utility functions   decision analysis tools  methods  taught  undergraduate students  schools  business health economics  public health   examples  operations research  management science methods decision tree building blocks decision tree elements drawn  left  right  decision tree   burst nodes splitting paths   sink nodes converging paths therefore used manually  can grow  big    often hard  draw fully  hand traditionally decision trees   created manually   aside example shows although increasingly specialized software  employed decision rules  decision tree can  linearized  decision rules   outcome   contents   leaf node   conditions along  path form  conjunction    clause  general  rules   form decision rules can  generated  constructing association rules   target variable   right  can also denote temporal  causal relations decision tree using flowchart symbols commonly  decision tree  drawn using flowchart symbols    easier  many  read  understand analysis example analysis can take  account  decision makers eg  companys preference  utility function  example  basic interpretation   situation    company prefers bs risk  payoffs  realistic risk preference coefficients greater  kin  range  risk aversion  company  need  model  third strategy neither   b influence diagram much   information   decision tree can  represented  compactly   influence diagram focusing attention   issues  relationships  events association rule induction decision trees can also  seen  generative models  induction rules  empirical data  optimal decision tree   defined   tree  accounts     data  minimizing  number  levels  questions several algorithms  generate  optimal trees   devised   id cls assistant  cart advantages  disadvantages among decision support tools decision trees  influence diagrams  several advantages decision trees disadvantages  decision trees\r\n"}
{"index":{"_id":34}}
{"conceptLabelTag":"bayesian network","conceptLabel":"bayesian network","conceptDescription":"bayesian network  bayesian network bayes network belief network bayesian model  probabilistic directed acyclic graphical model   probabilistic graphical model  type  statistical model  represents  set  random variables   conditional dependencies via  directed acyclic graph dag  example  bayesian network  represent  probabilistic relationships  diseases  symptoms given symptoms  network can  used  compute  probabilities   presence  various diseases formally bayesian networks  dags whose nodes represent random variables   bayesian sense  may  observable quantities latent variables unknown parameters  hypotheses edges represent conditional dependencies nodes    connected    path  one   variables      bayesian network represent variables   conditionally independent     node  associated   probability function  takes  input  particular set  values   nodes parent variables  gives  output  probability  probability distribution  applicable   variable represented   node  example  formula parent nodes represent formula boolean variables   probability function   represented   table  entries one entry     possible combinations   parents  true  false similar ideas may  applied  undirected  possibly cyclic graphs   markov networks efficient algorithms exist  perform inference  learning  bayesian networks bayesian networks  model sequences  variables eg speech signals  protein sequences  called dynamic bayesian networks generalizations  bayesian networks  can represent  solve decision problems  uncertainty  called influence diagrams example suppose    two events   cause grass   wet either  sprinkler     raining also suppose   rain   direct effect   use   sprinkler namely    rains  sprinkler  usually  turned    situation can  modeled   bayesian network shown   right  three variables  two possible values t  true  f  false  joint probability function    names   variables   abbreviated  g grass wet yesno s sprinkler turned  yesno  r raining yesno  model can answer questions like    probability    raining given  grass  wet  using  conditional probability formula  summing   nuisance variables using  expansion   joint probability function formula   conditional probabilities   conditional probability tables cpts stated   diagram one can evaluate  term   sums   numerator  denominator  example   numerical results subscripted   associated variable values      hand  wish  answer  interventional question    probability    rain given   wet  grass  answer   governed   postintervention joint distribution function formula obtained  removing  factor formula   preintervention distribution  expected  probability  rain  unaffected   action formula  moreover  wish  predict  impact  turning  sprinkler      term formula removed showing   action   effect   grass     rain  predictions may   feasible     variables  unobserved    policy evaluation problems  effect   action formula can still  predicted however whenever  criterion called backdoor  satisfied  states    set z  nodes can  observed  dseparates  blocks  backdoor paths  x  y  formula  backdoor path  one  ends   arrow  x sets  satisfy  backdoor criterion  called sufficient  admissible  example  set z r  admissible  predicting  effect  s t  g  r dseparate   backdoor path s r g however  s   observed     set  dseparates  path   effect  turning  sprinkler  s t   grass g   predicted  passive observations   say  pg dos t   identified  reflects  fact  lacking interventional data   determine   observed dependence  s  g  due   causal connection   spurious  determine whether  causal relation  identified   arbitrary bayesian network  unobserved variables one can use  three rules  docalculus  test whether   terms can  removed   expression   relation thus confirming   desired quantity  estimable  frequency data using  bayesian network can save considerable amounts  memory   dependencies   joint distribution  sparse  example  naive way  storing  conditional probabilities  twovalued variables   table requires storage space  formula values   local distributions   variable depends    three parent variables  bayesian network representation  needs  store   formula values one advantage  bayesian networks     intuitively easier   human  understand  sparse set  direct dependencies  local distributions  complete joint distributions inference  learning   three main inference tasks  bayesian networks inferring unobserved variables   bayesian network   complete model   variables   relationships  can  used  answer probabilistic queries    example  network can  used  find  updated knowledge   state   subset  variables   variables  evidence variables  observed  process  computing  posterior distribution  variables given evidence  called probabilistic inference  posterior gives  universal sufficient statistic  detection applications  one wants  choose values   variable subset  minimize  expected loss function  instance  probability  decision error  bayesian network can thus  considered  mechanism  automatically applying bayes theorem  complex problems   common exact inference methods  variable elimination  eliminates  integration  summation  nonobserved nonquery variables one  one  distributing  sum   product clique tree propagation  caches  computation   many variables can  queried  one time  new evidence can  propagated quickly  recursive conditioning  andor search  allow   spacetime tradeoff  match  efficiency  variable elimination  enough space  used    methods  complexity   exponential   networks treewidth   common approximate inference algorithms  importance sampling stochastic mcmc simulation minibucket elimination loopy belief propagation generalized belief propagation  variational methods parameter learning  order  fully specify  bayesian network  thus fully represent  joint probability distribution   necessary  specify   node x  probability distribution  x conditional upon xs parents  distribution  x conditional upon  parents may   form   common  work  discrete  gaussian distributions since  simplifies calculations sometimes  constraints   distribution  known one can  use  principle  maximum entropy  determine  single distribution  one   greatest entropy given  constraints analogously   specific context   dynamic bayesian network one commonly specifies  conditional distribution   hidden states temporal evolution  maximize  entropy rate   implied stochastic process often  conditional distributions include parameters   unknown  must  estimated  data sometimes using  maximum likelihood approach direct maximization   likelihood    posterior probability  often complex    unobserved variables  classical approach   problem   expectationmaximization algorithm  alternates computing expected values   unobserved variables conditional  observed data  maximizing  complete likelihood  posterior assuming  previously computed expected values  correct  mild regularity conditions  process converges  maximum likelihood  maximum posterior values  parameters   fully bayesian approach  parameters   treat parameters  additional unobserved variables   compute  full posterior distribution   nodes conditional upon observed data   integrate   parameters  approach can  expensive  lead  large dimension models   practice classical parametersetting approaches   common structure learning   simplest case  bayesian network  specified   expert    used  perform inference   applications  task  defining  network   complex  humans   case  network structure   parameters   local distributions must  learned  data automatically learning  graph structure   bayesian network   challenge pursued within machine learning  basic idea goes back   recovery algorithm developed  rebane  pearl  rests   distinction   three possible types  adjacent triplets allowed   directed acyclic graph dag type  type represent   dependencies formula  formula  independent given formula   therefore indistinguishable type however can  uniquely identified since formula  formula  marginally independent    pairs  dependent thus   skeletons  graphs stripped  arrows   three triplets  identical  directionality   arrows  partially identifiable   distinction applies  formula  formula  common parents except  one must first condition   parents algorithms   developed  systematically determine  skeleton   underlying graph   orient  arrows whose directionality  dictated   conditional independencies observed  alternative method  structural learning uses optimization based search  requires  scoring function   search strategy  common scoring function  posterior probability   structure given  training data like  bic   bdeu  time requirement   exhaustive search returning  structure  maximizes  score  superexponential   number  variables  local search strategy makes incremental changes aimed  improving  score   structure  global search algorithm like markov chain monte carlo can avoid getting trapped  local minima friedman et al discuss using mutual information  variables  finding  structure  maximizes      restricting  parent candidate set  k nodes  exhaustively searching therein  particularly fast method  exact bn learning   cast  problem   optimisation problem  solve  using integer programming acyclicity constraints  added   integer program ip  solving   form  cutting planes  method can handle problems    variables  order  deal  problems  thousands  variables   necessary  use  different approach one   first sample one ordering   find  optimal bn structure  respect   ordering  implies working   search space   possible orderings   convenient    smaller   space  network structures multiple orderings   sampled  evaluated  method   proven    best available  literature   number  variables  huge another method consists  focusing   subclass  decomposable models    mle   closed form    possible  discover  consistent structure  hundreds  variables  previously noted learning bayesian networks  bounded treewidth  necessary  allow exact tractable inference since  worstcase inference complexity  exponential   treewidth k   exponential time hypothesis yet   global property   graph  considerably increases  difficulty   learning process   context   possible  use  concept  ktree  effective learning statistical introduction given data formula  parameter formula  simple bayesian analysis starts   prior probability prior formula  likelihood formula  compute  posterior probability formula often  prior  formula depends  turn   parameters formula    mentioned   likelihood   prior formula must  replaced   likelihood formula   prior formula   newly introduced parameters formula  required resulting   posterior probability    simplest example   hierarchical bayes model  process may  repeated  example  parameters formula may depend  turn  additional parameters formula  will require   prior eventually  process must terminate  priors    depend    unmentioned parameters introductory examples suppose   measured  quantities formulaeach  normally distributed errors  known standard deviation formula suppose   interested  estimating  formula  approach    estimate  formula using  maximum likelihood approach since  observations  independent  likelihood factorizes   maximum likelihood estimate  simply however   quantities  related    example  may think   individual formula    drawn   underlying distribution   relationship destroys  independence  suggests   complex model eg  improper priors formulaflat formulaflatformula  formula    identified model ie  exists  unique solution   models parameters   posterior distributions   individual formula will tend  move  shrink away   maximum likelihood estimates towards  common mean  shrinkage   typical behavior  hierarchical bayes models restrictions  priors  care  needed  choosing priors   hierarchical model particularly  scale variables  higher levels   hierarchy    variable formula   example  usual priors    jeffreys prior often   work   posterior distribution will  improper  normalizable  estimates made  minimizing  expected loss will  inadmissible definitions  concepts   several equivalent definitions   bayesian network    following let g ve   directed acyclic graph  dag  let x x   set  random variables indexed  v factorization definition x   bayesian network  respect  g   joint probability density function  respect   product measure can  written   product   individual density functions conditional   parent variables formula  pav   set  parents  v ie  vertices pointing directly  v via  single edge   set  random variables  probability   member   joint distribution can  calculated  conditional probabilities using  chain rule given  topological ordering  x  follows formula compare    definition   can  written  formula   formula    parent  formula  difference   two expressions   conditional independence   variables     nondescendants given  values   parent variables local markov property x   bayesian network  respect  g   satisfies  local markov property  variable  conditionally independent   nondescendants given  parent variables  dev   set  descendants  v dev   set  nondescendants  v  can also  expressed  terms similar   first definition  note   set  parents   subset   set  nondescendants   graph  acyclic developing bayesian networks  develop  bayesian network  often first develop  dag g    believe x satisfies  local markov property  respect  g sometimes   done  creating  causal dag   ascertain  conditional probability distributions   variable given  parents  g  many cases  particular   case   variables  discrete   define  joint distribution  x    product   conditional distributions  x   bayesian network  respect  g markov blanket  markov blanket   node   set  nodes consisting   parents  children    parents   children  markov blanket renders  node independent   rest   network  joint distribution   variables   markov blanket   node  sufficient knowledge  calculating  distribution   node x   bayesian network  respect  g  every node  conditionally independent    nodes   network given  markov blanket dseparation  definition can  made  general  defining  dseparation  two nodes  d stands  directional let p   trail  node u  v  trail   loop undirected path  two nodes ie  direction  edges  ignored  constructing  path   edges may   direction  p  said   dseparated   set  nodes z     following conditions holds thus u  v  said   dseparated  z   trails    dseparated  u  v   dseparated   called dconnected x   bayesian network  respect  g    two nodes u v  z   set  dseparates u  v  markov blanket   minimal set  nodes  dseparates node v    nodes hierarchical models  term hierarchical model  sometimes considered  particular type  bayesian network    formal definition sometimes  term  reserved  models  three   levels  random variables  times   reserved  models  latent variables  general however  moderately complex bayesian network  usually termed hierarchical causal networks although bayesian networks  often used  represent causal relationships  need    case  directed edge  u  v   require  x  causally dependent  x   demonstrated   fact  bayesian networks   graphs  equivalent    impose exactly   conditional independence requirements  causal network   bayesian network   explicit requirement   relationships  causal  additional semantics   causal networks specify    node x  actively caused     given state x  action written  dox x   probability density function changes   one   network obtained  cutting  links   parents  x  x  setting x   caused value x using  semantics one can predict  impact  external interventions  data obtained prior  intervention inference complexity  approximation algorithms   working  stanford university  large bioinformatic applications greg cooper proved  exact inference  bayesian networks  nphard  result prompted  surge  research  approximation algorithms   aim  developing  tractable approximation  probabilistic inference  paul dagum  michael luby proved two surprising results   complexity  approximation  probabilistic inference  bayesian networks first  proved     tractable deterministic algorithm  can approximate probabilistic inference  within  absolute error second  proved     tractable randomized algorithm  can approximate probabilistic inference  within  absolute error  confidence probability greater      time dan roth proved  exact inference  bayesian networks   fact pcomplete  thus  hard  counting  number  satisfying assignments   cnf formula   approximate inference even  bayesian networks  restricted architecture  nphard  practical terms  complexity results suggested   bayesian networks  rich representations  ai  machine learning applications  use  large realworld applications  need   tempered  either topological structural constraints   na ve bayes networks   restrictions   conditional probabilities  bounded variance algorithm   first provable fast approximation algorithm  efficiently approximate probabilistic inference  bayesian networks  guarantees   error approximation  powerful algorithm required  minor restriction   conditional probabilities   bayesian network   bounded away  zero  one  pn  pn   polynomial   number  nodes   network n applications bayesian networks  used  modelling beliefs  computational biology  bioinformatics gene regulatory networks protein structure gene expression analysis learning epistasis  gwas data sets medicine biomonitoring document classification information retrieval semantic search image processing data fusion decision support systems engineering sports betting gaming law study design  risk analysis   texts applying bayesian networks  bioinformatics  financial  marketing informatics history  term bayesian networks  coined  judea pearl   emphasize three aspects   late s judea pearls text probabilistic reasoning  intelligent systems  richard e neapolitans text probabilistic reasoning  expert systems summarized  properties  bayesian networks  established bayesian networks   field  study informal variants   networks  first used  legal scholar john henry wigmore   form  wigmore charts  analyse trial evidence  another variant called path diagrams  developed   geneticist sewall wright  used  social  behavioral sciences mostly  linear parametric models\r\n"}
{"index":{"_id":35}}
{"conceptLabelTag":"compressed sensing","conceptLabel":"compressed sensing","conceptDescription":"compressed sensing compressed sensing also known  compressive sensing compressive sampling  sparse sampling   signal processing technique  efficiently acquiring  reconstructing  signal  finding solutions  underdetermined linear systems   based   principle   optimization  sparsity   signal can  exploited  recover   far fewer samples  required   shannonnyquist sampling theorem   two conditions   recovery  possible  first one  sparsity  requires  signal   sparse   domain  second one  incoherence   applied   isometric property   sufficient  sparse signals overview  common goal   engineering field  signal processing   reconstruct  signal   series  sampling measurements  general  task  impossible     way  reconstruct  signal   times   signal   measured nevertheless  prior knowledge  assumptions   signal  turns    possible  perfectly reconstruct  signal   series  measurements  time engineers  improved  understanding   assumptions  practical    can  generalized  early breakthrough  signal processing   nyquistshannon sampling theorem  states    signals highest frequency  less  half   sampling rate   signal can  reconstructed perfectly  main idea    prior knowledge  constraints   signals frequencies fewer samples  needed  reconstruct  signal around emmanuel cand s terence tao  david donoho proved  given knowledge   signals sparsity  signal may  reconstructed  even fewer samples   sampling theorem requires  idea   basis  compressed sensing history compressed sensing relies  l techniques  several  scientific fields  used historically  statistics  least squares method  complemented   formulanorm   introduced  laplace following  introduction  linear programming  dantzigs simplex algorithm  formulanorm  used  computational statistics  statistical theory  formulanorm  used  george w brown  later writers  medianunbiased estimators   used  peter j huber  others working  robust statistics  formulanorm  also used  signal processing  example   s  seismologists constructed images  reflective layers within  earth based  data    seem  satisfy  nyquistshannon criterion   used  matching pursuit   lasso estimator  robert tibshirani   basis pursuit    theoretical results describing   algorithms recovered sparse solutions   required type  number  measurements  suboptimal  subsequently greatly improved  compressed sensing  first glance compressed sensing might seem  violate  sampling theorem  compressed sensing depends   sparsity   signal  question    highest frequency    misconception   sampling theorem guarantees perfect reconstruction given sufficient  necessary conditions  sampling method fundamentally different  classical fixedrate sampling  violate  sampling theorem sparse signals  high frequency components can  highly undersampled using compressed sensing compared  classical fixedrate sampling method underdetermined linear system  underdetermined system  linear equations   unknowns  equations  generally   infinite number  solutions  order  choose  solution    system one must impose extra constraints  conditions   smoothness  appropriate  compressed sensing one adds  constraint  sparsity allowing  solutions    small number  nonzero coefficients   underdetermined systems  linear equations   sparse solution however     unique sparse solution   underdetermined system   compressed sensing framework allows  recovery   solution solution reconstruction method compressed sensing takes advantage   redundancy  many interesting signalsthey   pure noise  particular many signals  sparse    contain many coefficients close   equal  zero  represented   domain     insight used  many forms  lossy compression compressed sensing typically starts  taking  weighted linear combination  samples also called compressive measurements   basis different   basis    signal  known   sparse  results found  emmanuel cand s justin romberg terence tao  david donoho showed   number   compressive measurements can  small  still contain nearly   useful information therefore  task  converting  image back   intended domain involves solving  underdetermined matrix equation since  number  compressive measurements taken  smaller   number  pixels   full image however adding  constraint   initial signal  sparse enables one  solve  underdetermined system  linear equations  leastsquares solution   problems   minimize  formula normthat  minimize  amount  energy   system   usually simple mathematically involving   matrix multiplication   pseudoinverse   basis sampled  however  leads  poor results  many practical applications    unknown coefficients  nonzero energy  enforce  sparsity constraint  solving   underdetermined system  linear equations one can minimize  number  nonzero components   solution  function counting  number  nonzero components   vector  called  formula norm  david donoho cand s et al proved   many problems   probable   formula norm  equivalent   formula norm   technical sense  equivalence result allows one  solve  formula problem   easier   formula problem finding  candidate   smallest formula norm can  expressed relatively easily   linear program   efficient solution methods already exist  measurements may contain  finite amount  noise basis pursuit denoising  preferred  linear programming since  preserves sparsity   face  noise  can  solved faster   exact linear program total variation based cs reconstruction motivation  applications role  tv regularization total variation can  seen   nonnegative realvalued functional defined   space  realvalued functions   case  functions  one variable    space  integrable functions   case  functions  several variables  signals especially total variation refers   integral   absolute gradient   signal  signal  image reconstruction   applied  total variation regularization   underlying principle   signals  excessive details  high total variation   removing  details  retaining important information   edges  reduce  total variation   signal  make  signal subject closer   original signal   problem   purpose  signal  image reconstruction formula minimization models  used  approaches also include  leastsquares    discussed    article  methods  extremely slow  return  notsoperfect reconstruction   signal  current cs regularization models attempt  address  problem  incorporating sparsity priors   original image one     total variation tv conventional tv approaches  designed  give piecewise constant solutions    include  discussed ahead constrained lminimization  uses  iterative scheme  method though fast subsequently leads  oversmoothing  edges resulting  blurred image edges tv methods  iterative reweighting   implemented  reduce  influence  large gradient value magnitudes   images    used  computed tomography ct reconstruction   method known  edgepreserving total variation however  gradient magnitudes  used  estimation  relative penalty weights   data fidelity  regularization terms  method   robust  noise  artifacts  accurate enough  cs imagesignal reconstruction  therefore fails  preserve smaller structures recent progress   problem involves using  iteratively directional tv refinement  cs reconstruction  method   stages  first stage  estimate  refine  initial orientation field   defined   noisy pointwise initial estimate  edgedetection   given image   second stage  cs reconstruction model  presented  utilizing directional tv regularizer  details   tvbased approaches iteratively reweighted l minimization edgepreserving tv  iterative model using directional orientation field  tv  provided  existing approaches iteratively reweighted formula minimization   cs reconstruction models using constrained formula minimization larger coefficients  penalized heavily   formula norm   proposed    weighted formulation  formula minimization designed   democratically penalize nonzero coefficients  iterative algorithm  used  constructing  appropriate weights  iteration requires solving one formula minimization problem  finding  local minimum   concave penalty function   closely resembles  formula norm  additional parameter usually  avoid  sharp transitions   penalty function curve  introduced   iterative equation  ensure stability     zero estimate  one iteration   necessarily lead   zero estimate   next iteration  method essentially involves using  current solution  computing  weights   used   next iteration advantages  disadvantages early iterations may find inaccurate sample estimates however  method will downsample    later stage  give  weight   smaller nonzero signal estimates one   disadvantages   need  defining  valid starting point   global minimum might   obtained every time due   concavity   function another disadvantage    method tends  uniformly penalize  image gradient irrespective   underlying image structures  causes oversmoothing  edges especially   low contrast regionssubsequently leading  loss  low contrast informationthe advantages   method include reduction   sampling rate  sparse signals reconstruction   image   robust   removal  noise   artifacts  use    iterations  can also help  recovering images  sparse gradients   figure shown  p refers   firststep   iterative reconstruction process   projection matrix p   fanbeam geometry   constrained   data fidelity term  may contain noise  artifacts   regularization  performed  minimization  p  solved   conjugate gradient least squares method p refers   second step   iterative reconstruction process wherein  utilizes  edgepreserving total variation regularization term  remove noise  artifacts  thus improve  quality   reconstructed imagesignal  minimization  p  done   simple gradient descent method convergence  determined  testing   iteration  image positivity  checking  formula   case  formula note  formula refers   different xray linear attenuation coefficients  different voxels   patient image edgepreserving total variation tv based compressed sensing    iterative ct reconstruction algorithm  edgepreserving tv regularization  reconstruct ct images  highly undersampled data obtained  low dose ct  low current levels milliampere  order  reduce  imaging dose one   approaches used   reduce  number  xray projections acquired   scanner detectors however  insufficient projection data   used  reconstruct  ct image can cause streaking artifacts furthermore using  insufficient projections  standard tv algorithms end  making  problem underdetermined  thus leading  infinitely many possible solutions   method  additional penalty weighted function  assigned   original tv norm  allows  easier detection  sharp discontinuities  intensity   images  thereby adapt  weight  store  recovered edge information   process  signalimage reconstruction  parameter formula controls  amount  smoothing applied   pixels   edges  differentiate    nonedge pixels  value  formula  changed adaptively based   values   histogram   gradient magnitude    certain percentage  pixels  gradient values larger  formula  edgepreserving total variation term thus becomes sparser   speeds   implementation  twostep iteration process known  forwardbackward splitting algorithm  used  optimization problem  split  two subproblems    solved   conjugate gradient least squares method   simple gradient descent method respectively  method  stopped   desired convergence   achieved    maximum number  iterations  reached advantages  disadvantages    disadvantages   method   absence  smaller structures   reconstructed image  degradation  image resolution  edge preserving tv algorithm however requires fewer iterations   conventional tv algorithm analyzing  horizontal  vertical intensity profiles   reconstructed images  can  seen    sharp s  edge points  negligible minor fluctuation  nonedge points thus  method leads  low relative error  higher correlation  compared   tv method  also effectively suppresses  removes  form  image noise  image artifacts   streaking iterative model using  directional orientation field  directional total variation  prevent oversmoothing  edges  texture details   obtain  reconstructed cs image   accurate  robust  noise  artifacts  method  used first  initial estimate   noisy pointwise orientation field   image formula formula  obtained  noisy orientation field  defined    can  refined   later stage  reduce  noise influences  orientation field estimationa coarse orientation field estimation   introduced based  structure tensor   formulated  formula  formula refers   structure tensor related   image pixel point ij  standard deviation formula formula refers   gaussian kernel formula  standard deviation formula formula refers   manually defined parameter   image formula    edge detection  insensitive  noise formula refers   gradient   image formula  formula refers   tensor product obtained  using  gradient  structure tensor obtained  convolved   gaussian kernel formula  improve  accuracy   orientation estimate  formula  set  high values  account   unknown noise levels  every pixel ij   image  structure tensor j   symmetric  positive semidefinite matrix convolving   pixels   image  formula gives orthonormal eigen vectors    formula matrix points   direction   dominant orientation   largest contrast  points   direction   structure orientation   smallest contrast  orientation field coarse initial estimation formula  defined  formula  estimate  accurate  strong edges however  weak edges   regions  noise  reliability decreases  overcome  drawback  refined orientation model  defined    data term reduces  effect  noise  improves accuracy   second penalty term   lnorm   fidelity term  ensures accuracy  initial coarse estimation  orientation field  introduced   directional total variation optimization model  cs reconstruction   equation formula formula   objective signal  needs   recovered y   corresponding measurement vector d   iterative refined orientation field  formula   cs measurement matrix  method undergoes   iterations ultimately leading  convergenceformula   orientation field approximate estimation   reconstructed image formula   previous iteration  order  check  convergence   subsequent optical performance  previous iteration  used   two vector fields represented  formula  formula formula refers   multiplication  respective horizontal  vertical vector elements  formula  formula followed   subsequent addition  equations  reduced   series  convex minimization problems    solved   combination  variable splitting  augmented lagrangian fftbased fast solver   closed form solution methods  augmented lagrangian  considered equivalent   split bregman iteration  ensures convergence   method  orientation field d  defined   equal  formula  formula define  horizontal  vertical estimates  formula  augmented lagrangian method   orientation field formula involves initializing formula   finding  approximate minimizer  formula  respect   variables  lagrangian multipliers   updated   iterative process  stopped  convergence  achieved   iterative directional total variation refinement model  augmented lagrangian method involves initializing formula  formula  newly introduced variables  formula formula formula formula formula formula  formula formula formula   lagrangian multipliers  formula   iteration  approximate minimizer  formula  respect  variables formula  calculated     field refinement model  lagrangian multipliers  updated   iterative process  stopped  convergence  achieved   orientation field refinement model  lagrangian multipliers  updated   iterative process  follows formula formula   iterative directional total variation refinement model  lagrangian multipliers  updated  follows formula formula  formula  positive constants advantages  disadvantages based  peak signaltonoise ratio psnr  structural similarity index ssim metrics  known groundtruth images  testing performance   concluded  iterative directional total variation   better reconstructed performance   noniterative methods  preserving edge  texture areas  orientation field refinement model plays  major role   improvement  performance   increases  number  directionless pixels   flat area  enhancing  orientation field consistency   regions  edges applications  field  compressive sensing  related  several topics  signal processing  computational mathematics   underdetermined linearsystems group testing heavy hitters sparse coding multiplexing sparse sampling  finite rate  innovation  broad scope  generality  enabled several innovative csenhanced approaches  signal processing  compression solution  inverse problems design  radiating systems radar  throughthewall imaging  antenna characterization imaging techniques   strong affinity  compressive sensing include coded aperture  computational photography implementations  compressive sensing  hardware  different technology readiness levels  available conventional cs reconstruction uses sparse signals usually sampled   rate less   nyquist sampling rate  reconstruction  constrained formula minimization one   earliest applications    approach   reflection seismology  used sparse reflected signals  bandlimited data  tracking changes  subsurface layers   lasso model came  prominence   s   statistical method  selection  sparse models  method   used  computational harmonic analysis  sparse signal representation  overcomplete dictionaries     applications include incoherent sampling  radar pulses  work  boyd et al  applied  lasso model  selection  sparse models towards analog  digital converters  current ones use  sampling rate higher   nyquist rate along   quantized shannon representation   involve  parallel architecture    polarity   analog signal changes   high rate followed  digitizing  integral   end   timeinterval  obtain  converted digital signal photography compressed sensing  used   mobile phone camera sensor  approach allows  reduction  image acquisition energy per image   much   factor    cost  complex decompression algorithms  computation may require  offdevice implementation compressed sensing  used  singlepixel cameras  rice university bell labs employed  technique   lensless singlepixel camera  takes stills using repeated snapshots  randomly chosen apertures   grid image quality improves   number  snapshots  generally requires  small fraction   data  conventional imaging  eliminating lensfocusrelated aberrations holography compressed sensing can  used  improve image reconstruction  holography  increasing  number  voxels one can infer   single hologram   also used  image retrieval  undersampled measurements  optical  millimeterwave holography facial recognition compressed sensing   used  facial recognition applications magnetic resonance imaging compressed sensing   used  shorten magnetic resonance imaging scanning sessions  conventional hardware reconstruction methods include compressed sensing addresses  issue  high scan time  enabling faster acquisition  measuring fewer fourier coefficients  produces  highquality image  relatively lower scan time another application also discussed ahead   ct reconstruction  fewer xray projections compressed sensing   case removes  high spatial gradient parts mainly image noise  artifacts  holds tremendous potential  one can obtain highresolution ct images  low radiation doses  lower currentma settings network tomography compressed sensing  showed outstanding results   application  network tomography  network management network delay estimation  network congestion detection can   modeled  underdetermined systems  linear equations   coefficient matrix   network routing matrix moreover   internet network routing matrices usually satisfy  criterion  using compressed sensing shortwaveinfrared cameras commercial shortwaveinfrared cameras based upon compressed sensing  available  cameras  light sensitivity  m  m   wavelengths invisible   human eye aperture synthesis  radio astronomy   field  radio astronomy compressed sensing   proposed  deconvolving  interferometric image  fact  h gbom clean algorithm     use   deconvolution  radio images since  similar  compressed sensings matching pursuit algorithm transmission electron microscopy compressed sensing combined   moving aperture   used  increase  acquisition rate  images   transmission electron microscope  scanning mode compressive sensing combined  random scanning   electron beam  enabled  faster acquisition  less electron dose  allows  imaging  electron beam sensitive materials\r\n"}
{"index":{"_id":36}}
{"conceptLabelTag":"markov chain monte carlo","conceptLabel":"markov chain monte carlo","conceptDescription":"markov chain monte carlo  statistics markov chain monte carlo mcmc methods   class  algorithms  sampling   probability distribution based  constructing  markov chain    desired distribution   equilibrium distribution  state   chain   number  steps   used   sample   desired distribution  quality   sample improves   function   number  steps random walk monte carlo methods make   large subclass  mcmc methods classification random walk monte carlo methods multidimensional integrals   mcmc method  used  approximating  multidimensional integral  ensemble  walkers move around randomly   point   walker steps  integrand value   point  counted towards  integral  walker  may make  number  tentative steps around  area looking   place   reasonably high contribution   integral  move  next random walk monte carlo methods   kind  random simulation  monte carlo method however whereas  random samples   integrand used   conventional monte carlo integration  statistically independent  used  mcmc methods  correlated  markov chain  constructed    way     integrand   equilibrium distribution examples examples  random walk monte carlo methods include  following  mcmc methods trainingbased mcmc unlike    current mcmc methods  ignore  previous trials using  new algorithm  mcmc algorithm  able  use  previous steps  generate  next candidate  trainingbased algorithm  able  speedup  mcmc algorithm   order  magnitude interacting mcmc methodologies   class  mean field particle methods  obtaining random samples   sequence  probability distributions   increasing level  sampling complexity  probabilistic models include path space state models  increasing time horizon posterior distributions wrt sequence  partial observations increasing constraint level sets  conditional distributions decreasing temperature schedules associated   boltzmanngibbs distributions  many others  principle  mcmc sampler can  turned   interacting mcmc sampler  interacting mcmc samplers can  interpreted   way  run  parallel  sequence  mcmc samplers  instance interacting simulated annealing algorithms  based  independent metropolishastings moves interacting sequentially   selectionresampling type mechanism  contrast  traditional mcmc methods  precision parameter   class  interacting mcmc samplers   related   number  interacting mcmc samplers  advanced particle methodologies belong   class  feynmankac particle models also called sequential monte carlo  particle filter methods  bayesian inference  signal processing communities interacting mcmc methods can also  interpreted   mutationselection genetic particle algorithm  mcmc mutations markov chain quasimonte carlo mcqmc  advantage  lowdiscrepancy sequences  lieu  random numbers  simple independent monte carlo sampling  well known  procedure known  quasimonte carlo method qmc yields  integration error  decays   superior rate   obtained  iid sampling   koksmahlawka inequality empirically  allows  reduction   estimation error  convergence time   order  magnitude reducing correlation  sophisticated methods use various ways  reducing  correlation  successive samples  algorithms may  harder  implement   usually exhibit faster convergence ie fewer steps   accurate result examples examples  nonrandom walk mcmc methods include  following convergence usually    hard  construct  markov chain   desired properties   difficult problem   determine  many steps  needed  converge   stationary distribution within  acceptable error  good chain will  rapid mixing  stationary distribution  reached quickly starting   arbitrary position  standard empirical method  assess convergence   run several independent simulated markov chains  check   ratio  interchain  intrachain variances    parameters sampled  close  typically mcmc sampling can  approximate  target distribution    always  residual effect   starting position  sophisticated mcmcbased algorithms   coupling   past can produce exact samples   cost  additional computation   unbounded though finite  expectation running time many random walk monte carlo methods move around  equilibrium distribution  relatively small steps   tendency   steps  proceed    direction  methods  easy  implement  analyze  unfortunately  can take  long time   walker  explore    space  walker will often double back  cover ground already covered software several software programs provide mcmc sampling capabilities  example\r\n"}
{"index":{"_id":37}}
{"conceptLabelTag":"perceptron","conceptLabel":"perceptron","conceptDescription":"perceptron  machine learning  perceptron   algorithm  supervised learning  binary classifiers functions  can decide whether  input represented   vector  numbers belongs   specific class      type  linear classifier ie  classification algorithm  makes  predictions based   linear predictor function combining  set  weights   feature vector  algorithm allows  online learning    processes elements   training set one   time  perceptron algorithm dates back   late s  first implementation  custom hardware  one   first artificial neural networks   produced history  perceptron algorithm  invented    cornell aeronautical laboratory  frank rosenblatt funded   united states office  naval research  perceptron  intended    machine rather   program    first implementation   software   ibm   subsequently implemented  custombuilt hardware   mark perceptron  machine  designed  image recognition    array  photocells randomly connected   neurons weights  encoded  potentiometers  weight updates  learning  performed  electric motors   press conference organized   us navy rosenblatt made statements   perceptron  caused  heated controversy among  fledgling ai community based  rosenblatts statements  new york times reported  perceptron    embryo   electronic computer   navy expects will  able  walk talk see write reproduce    conscious   existence although  perceptron initially seemed promising   quickly proved  perceptrons    trained  recognise many classes  patterns  caused  field  neural network research  stagnate  many years    recognised   feedforward neural network  two   layers also called  multilayer perceptron  far greater processing power  perceptrons  one layer also called  single layer perceptron single layer perceptrons   capable  learning linearly separable patterns   famous book entitled perceptrons  marvin minsky  seymour papert showed    impossible   classes  network  learn  xor function   often believed   also conjectured incorrectly   similar result  hold   multilayer perceptron network however    true   minsky  papert already knew  multilayer perceptrons  capable  producing  xor function see  page  perceptrons book   information three years later stephen grossberg published  series  papers introducing networks capable  modelling differential contrastenhancing  xor functions  papers  published   see eg nevertheless  oftenmiscited minskypapert text caused  significant decline  interest  funding  neural network research  took ten  years  neural network research experienced  resurgence   s  text  reprinted   perceptrons expanded edition   errors   original text  shown  corrected  kernel perceptron algorithm  already introduced   aizerman et al margin bounds guarantees  given   perceptron algorithm   general nonseparable case first  freund  schapire   recently  mohri  rostamizadeh  extend previous results  give new l bounds definition   modern sense  perceptron   algorithm  learning  binary classifier  function  maps  input formula  realvalued vector   output value formula  single binary value    vector  realvalued weights formula   dot product formula  m   number  inputs   perceptron    bias  bias shifts  decision boundary away   origin    depend   input value  value  formula   used  classify  either  positive   negative instance   case   binary classification problem  formula  negative   weighted combination  inputs must produce  positive value greater  formula  order  push  classifier neuron   threshold spatially  bias alters  position though   orientation   decision boundary  perceptron learning algorithm   terminate   learning set   linearly separable   vectors   linearly separable learning will never reach  point   vectors  classified properly   famous example   perceptrons inability  solve problems  linearly nonseparable vectors   boolean exclusiveor problem  solution spaces  decision boundaries   binary functions  learning behaviors  studied   reference   context  neural networks  perceptron   artificial neuron using  heaviside step function   activation function  perceptron algorithm  also termed  singlelayer perceptron  distinguish    multilayer perceptron    misnomer    complicated neural network   linear classifier  singlelayer perceptron   simplest feedforward neural network learning algorithm    example   learning algorithm   singlelayer perceptron  multilayer perceptrons   hidden layer exists  sophisticated algorithms   backpropagation must  used alternatively methods    delta rule can  used   function  nonlinear  differentiable although  one  will work  well  multiple perceptrons  combined   artificial neural network  output neuron operates independently    others thus learning  output can  considered  isolation definitions  first define  variables  show  values   features  follows  represent  weights  show  timedependence  formula  use unlike  linear classification algorithms   logistic regression    need   learning rate   perceptron algorithm    multiplying  update   constant simply rescales  weights  never changes  sign   prediction steps  algorithm updates  weights  steps   b  weights  immediately applied   pair   training set  subsequently updated rather  waiting   pairs   training set  undergone  steps convergence  perceptron   linear classifier therefore  will never get   state    input vectors classified correctly   training set   linearly separable ie   positive examples can   separated   negative examples   hyperplane   case  approximate solution will  gradually approached   standard learning algorithm  instead learning will fail completely hence  linear separability   training set   known  priori one   training variants    used    training set  linearly separable   perceptron  guaranteed  converge     upper bound   number  times  perceptron will adjust  weights   training suppose   input vectors   two classes can  separated   hyperplane   margin formula ie  exists  weight vector formula   bias term   formula   formula  formula   formula  also let denote  maximum norm   input vector novikoff proved    case  perceptron algorithm converges  making formula updates  idea   proof    weight vector  always adjusted   bounded amount   direction      negative dot product  thus can  bounded      number  changes   weight vector   can also  bounded      exists  unknown satisfactory weight vector  every change makes progress   unknown direction   positive amount  depends    input vector   perceptron algorithm  guaranteed  converge   solution   case   linearly separable training set  may still pick  solution  problems may admit many solutions  varying quality  perceptron  optimal stability nowadays better known   linear support vector machine  designed  solve  problem variants  pocket algorithm  ratchet gallant solves  stability problem  perceptron learning  keeping  best solution seen  far   pocket  pocket algorithm  returns  solution   pocket rather   last solution  can  used also  nonseparable data sets   aim   find  perceptron   small number  misclassifications however  solutions appear purely stochastically  hence  pocket algorithm neither approaches  gradually   course  learning    guaranteed  show  within  given number  learning steps  maxover algorithm wendemuth  robust   sense   will converge regardless  prior knowledge  linear separability   data set   linear separable case  will solve  training problem  desired even  optimal stability maximum margin   classes  nonseparable data sets  will return  solution   small number  misclassifications   cases  algorithm gradually approaches  solution   course  learning without memorizing previous states  without stochastic s convergence   global optimality  separable data sets   local optimality  nonseparable data sets  separable problems perceptron training can also aim  finding  largest separating margin   classes  socalled perceptron  optimal stability can  determined  means  iterative training  optimization schemes    minover algorithm krauth  mezard   adatron anlauf  biehl adatron uses  fact   corresponding quadratic optimization problem  convex  perceptron  optimal stability together   kernel trick   conceptual foundations   support vector machine  formulaperceptron  used  preprocessing layer  fixed random weights  thresholded output units  enabled  perceptron  classify analogue patterns  projecting    binary space  fact   projection space  sufficiently high dimension patterns can become linearly separable another way  solve nonlinear problems without using multiple layers   use higher order networks sigmapi unit   type  network  element   input vector  extended   pairwise combination  multiplied inputs second order  can  extended   norder network    kept  mind however   best classifier   necessarily   classifies   training data perfectly indeed     prior constraint   data come  equivariant gaussian distributions  linear separation   input space  optimal   nonlinear solution  overfitted  linear classification algorithms include winnow support vector machine  logistic regression multiclass perceptron like   techniques  training linear classifiers  perceptron generalizes naturally  multiclass classification   input formula   output formula  drawn  arbitrary sets  feature representation function formula maps  possible inputoutput pair   finitedimensional realvalued feature vector    feature vector  multiplied   weight vector formula  now  resulting score  used  choose among many possible outputs learning  iterates   examples predicting  output   leaving  weights unchanged   predicted output matches  target  changing       update becomes  multiclass feedback formulation reduces   original perceptron  formula   realvalued vector formula  chosen  formula  formula  certain problems inputoutput representations  features can  chosen   formula can  found efficiently even though formula  chosen    large  even infinite set  recent years perceptron training  become popular   field  natural language processing   tasks  partofspeech tagging  syntactic parsing collins \r\n"}
{"index":{"_id":38}}
{"conceptLabelTag":"temporal difference learning","conceptLabel":"temporal difference learning","conceptDescription":"temporal difference learning temporal difference td learning   predictionbased machine learning method   primarily  used   reinforcement learning problem   said    combination  monte carlo ideas  dynamic programming dp ideas td resembles  monte carlo method   learns  sampling  environment according   policy   related  dynamic programming techniques   approximates  current estimate based  previously learned estimates  process known  bootstrapping  td learning algorithm  related   temporal difference model  animal learning   prediction method td learning considers  subsequent predictions  often correlated   sense  standard supervised predictive learning one learns   actually observed values  prediction  made    observation  available  prediction mechanism  adjusted  better match  observation  elucidated  richard sutton  core idea  td learning   one adjusts predictions  match   accurate predictions   future  procedure   form  bootstrapping  illustrated   following example mathematically speaking    standard   td approach one  try  optimize  cost function related   error   predictions   expectation   random variable ez however    standard approach one   sense assumes ez z  actual observed value   td approach  use  model   particular case  reinforcement learning    major application  td methods z   total return  ez  given   bellman equation   return mathematical formulation let formula   reward return  time step t let formula   correct prediction   equal   discounted sum   future reward  discounting  done  powers  factor  formula   reward  distant time step  less important  formula  formula can  expanded  changing  index    start  thus  reward   difference   correct prediction   current prediction tdlambda tdlambda   learning algorithm invented  richard s sutton based  earlier work  temporal difference learning  arthur samuel  algorithm  famously applied  gerald tesauro  create tdgammon  program  learned  play  game  backgammon   level  expert human players  lambda formula parameter refers   trace decay parameter  formula higher settings lead  longer lasting traces    larger proportion  credit   reward can  given   distant states  actions  formula  higher  formula producing parallel learning  monte carlo rl algorithms td algorithm  neuroscience  td algorithm  also received attention   field  neuroscience researchers discovered   firing rate  dopamine neurons   ventral tegmental area vta  substantia nigra snc appear  mimic  error function   algorithm  error function reports back  difference   estimated reward   given state  time step   actual reward received  larger  error function  larger  difference   expected  actual reward    paired   stimulus  accurately reflects  future reward  error can  used  associate  stimulus   future reward dopamine cells appear  behave   similar manner  one experiment measurements  dopamine cells  made  training  monkey  associate  stimulus   reward  juice initially  dopamine cells increased firing rates   monkey received juice indicating  difference  expected  actual rewards  time  increase  firing back propagated   earliest reliable stimulus   reward   monkey  fully trained    increase  firing rate upon presentation   predicted reward continually  firing rate   dopamine cells decreased  normal activation   expected reward   produced  mimics closely   error function  td  used  reinforcement learning  relationship   model  potential neurological function  produced research attempting  use td  explain many aspects  behavioral research   also  used  study conditions   schizophrenia   consequences  pharmacological manipulations  dopamine  learning\r\n"}
{"index":{"_id":39}}
{"conceptLabelTag":"supervised learning","conceptLabel":"supervised learning","conceptDescription":"supervised learning supervised learning   machine learning task  inferring  function   training data consist   set  training examples  supervised learning  example   pair consisting   input object typically  vector   desired output value also called  supervisory signal  supervised learning algorithm analyzes  training data  produces  inferred function  can  used  mapping new examples  optimal scenario will allow   algorithm  correctly determine  class labels  unseen instances  requires  learning algorithm  generalize   training data  unseen situations   reasonable way see inductive bias  parallel task  human  animal psychology  often referred   concept learning overview  order  solve  given problem  supervised learning one   perform  following steps  wide range  supervised learning algorithms  available    strengths  weaknesses    single learning algorithm  works best   supervised learning problems see    lunch theorem   four major issues  consider  supervised learning biasvariance tradeoff  first issue   tradeoff  bias  variance imagine    available several different  equally good training data sets  learning algorithm  biased   particular input formula   trained     data sets   systematically incorrect  predicting  correct output  formula  learning algorithm  high variance   particular input formula   predicts different output values  trained  different training sets  prediction error   learned classifier  related   sum   bias   variance   learning algorithm generally    tradeoff  bias  variance  learning algorithm  low bias must  flexible    can fit  data well    learning algorithm   flexible  will fit  training data set differently  hence  high variance  key aspect  many supervised learning methods     able  adjust  tradeoff  bias  variance either automatically   providing  biasvariance parameter   user can adjust function complexity  amount  training data  second issue   amount  training data available relative   complexity   true function classifier  regression function   true function  simple   inflexible learning algorithm  high bias  low variance will  able  learn    small amount  data    true function  highly complex eg   involves complex interactions among many different input features  behaves differently  different parts   input space   function will   learnable    large amount  training data  using  flexible learning algorithm  low bias  high variance dimensionality   input space  third issue   dimensionality   input space   input feature vectors   high dimension  learning problem can  difficult even   true function  depends   small number   features     many extra dimensions can confuse  learning algorithm  cause    high variance hence high input dimensionality typically requires tuning  classifier   low variance  high bias  practice   engineer can manually remove irrelevant features   input data   likely  improve  accuracy   learned function  addition   many algorithms  feature selection  seek  identify  relevant features  discard  irrelevant ones    instance    general strategy  dimensionality reduction  seeks  map  input data   lowerdimensional space prior  running  supervised learning algorithm noise   output values  fourth issue   degree  noise   desired output values  supervisory target variables   desired output values  often incorrect   human error  sensor errors   learning algorithm   attempt  find  function  exactly matches  training examples attempting  fit  data  carefully leads  overfitting  can overfit even     measurement errors stochastic noise   function   trying  learn   complex   learning model    situation  part   target function    modeled corrupts  training data  phenomenon   called deterministic noise  either type  noise  present   better  go   higher bias lower variance estimator  practice   several approaches  alleviate noise   output values   early stopping  prevent overfitting  well  detecting  removing  noisy training examples prior  training  supervised learning algorithm   several algorithms  identify noisy training examples  removing  suspected noisy training examples prior  training  decreased generalization error  statistical significance  factors  consider  factors  consider  choosing  applying  learning algorithm include  following  considering  new application  engineer can compare multiple learning algorithms  experimentally determine  one works best   problem  hand see cross validation tuning  performance   learning algorithm can   timeconsuming given fixed resources   often better  spend  time collecting additional training data   informative features     spend extra time tuning  learning algorithms   widely used learning algorithms  support vector machines linear regression logistic regression naive bayes linear discriminant analysis decision trees knearest neighbor algorithm  neural networks multilayer perceptron  supervised learning algorithms work given  set  formula training examples   form formula   formula   feature vector   ith example  formula   label ie class  learning algorithm seeks  function formula  formula   input space  formula   output space  function formula   element   space  possible functions formula usually called  hypothesis space   sometimes convenient  represent formula using  scoring function formula   formula  defined  returning  formula value  gives  highest score formula let formula denote  space  scoring functions although formula  formula can   space  functions many learning algorithms  probabilistic models  formula takes  form   conditional probability model formula  formula takes  form   joint probability model formula  example naive bayes  linear discriminant analysis  joint probability models whereas logistic regression   conditional probability model   two basic approaches  choosing formula  formula empirical risk minimization  structural risk minimization empirical risk minimization seeks  function  best fits  training data structural risk minimize includes  penalty function  controls  biasvariance tradeoff   cases   assumed   training set consists   sample  independent  identically distributed pairs formula  order  measure  well  function fits  training data  loss function formula  defined  training example formula  loss  predicting  value formula  formula  risk formula  function formula  defined   expected loss  formula  can  estimated   training data  empirical risk minimization  empirical risk minimization  supervised learning algorithm seeks  function formula  minimizes formula hence  supervised learning algorithm can  constructed  applying  optimization algorithm  find formula  formula   conditional probability distribution formula   loss function   negative log likelihood formula  empirical risk minimization  equivalent  maximum likelihood estimation  formula contains many candidate functions   training set   sufficiently large empirical risk minimization leads  high variance  poor generalization  learning algorithm  able  memorize  training examples without generalizing well   called overfitting structural risk minimization structural risk minimization seeks  prevent overfitting  incorporating  regularization penalty   optimization  regularization penalty can  viewed  implementing  form  occams razor  prefers simpler functions   complex ones  wide variety  penalties   employed  correspond  different definitions  complexity  example consider  case   function formula   linear function   form  popular regularization penalty  formula    squared euclidean norm   weights also known   formula norm  norms include  formula norm formula   formula norm    number  nonzero formulas  penalty will  denoted  formula  supervised learning optimization problem   find  function formula  minimizes  parameter formula controls  biasvariance tradeoff  formula  gives empirical risk minimization  low bias  high variance  formula  large  learning algorithm will  high bias  low variance  value  formula can  chosen empirically via cross validation  complexity penalty   bayesian interpretation   negative log prior probability  formula formula   case formula   posterior probabability  formula generative training  training methods described   discriminative training methods   seek  find  function formula  discriminates well   different output values see discriminative model   special case  formula   joint probability distribution   loss function   negative log likelihood formula  risk minimization algorithm  said  perform generative training  formula can  regarded   generative model  explains   data  generated generative training algorithms  often simpler   computationally efficient  discriminative training algorithms   cases  solution can  computed  closed form   naive bayes  linear discriminant analysis generalizations  supervised learning   several ways    standard supervised learning problem can  generalized\r\n"}
{"index":{"_id":40}}
{"conceptLabelTag":"linear regression","conceptLabel":"linear regression","conceptDescription":"linear regression  statistics linear regression   approach  modeling  relationship   scalar dependent variable y  one   explanatory variables  independent variables denoted x  case  one explanatory variable  called simple linear regression    one explanatory variable  process  called multiple linear regression  term  distinct  multivariate linear regression  multiple correlated dependent variables  predicted rather   single scalar variable  linear regression  relationships  modeled using linear predictor functions whose unknown model parameters  estimated   data  models  called linear models  commonly  conditional mean  y given  value  x  assumed    affine function  x less commonly  median    quantile   conditional distribution  y given x  expressed   linear function  x like  forms  regression analysis linear regression focuses   conditional probability distribution  y given x rather    joint probability distribution  y  x    domain  multivariate analysis linear regression   first type  regression analysis   studied rigorously    used extensively  practical applications    models  depend linearly   unknown parameters  easier  fit  models   nonlinearly related   parameters    statistical properties   resulting estimators  easier  determine linear regression  many practical uses  applications fall  one   following two broad categories linear regression models  often fitted using  least squares approach   may also  fitted   ways    minimizing  lack  fit    norm   least absolute deviations regression   minimizing  penalized version   least squares loss function   ridge regression lnorm penalty  lasso lnorm penalty conversely  least squares approach can  used  fit models    linear models thus although  terms least squares  linear model  closely linked    synonymous introduction given  data set formula  n statistical units  linear regression model assumes   relationship   dependent variable y   pvector  regressors x  linear  relationship  modeled   disturbance term  error variable  unobserved random variable  adds noise   linear relationship   dependent variable  regressors thus  model takes  form  denotes  transpose   x   inner product  vectors x  often  n equations  stacked together  written  vector form    remarks  terminology  general use example consider  situation   small ball   tossed    air    measure  heights  ascent h  various moments  time t physics tells us  ignoring  drag  relationship can  modeled   determines  initial velocity   ball  proportional   standard gravity   due  measurement errors linear regression can  used  estimate  values     measured data  model  nonlinear   time variable    linear   parameters    take regressors x x x t t  model takes   standard form assumptions standard linear regression models  standard estimation techniques make  number  assumptions   predictor variables  response variables   relationship numerous extensions   developed  allow    assumptions   relaxed ie reduced   weaker form    cases eliminated entirely  methods  general enough   can relax multiple assumptions      cases  can  achieved  combining different extensions generally  extensions make  estimation procedure  complex  timeconsuming  may also require  data  order  produce  equally precise model  following   major assumptions made  standard linear regression models  standard estimation techniques eg ordinary least squares beyond  assumptions several  statistical properties   data strongly influence  performance  different estimation methods interpretation  fitted linear regression model can  used  identify  relationship   single predictor variable x   response variable y     predictor variables   model  held fixed specifically  interpretation    expected change  y   oneunit change  x    covariates  held fixedthat   expected value   partial derivative  y  respect  x   sometimes called  unique effect  x  y  contrast  marginal effect  x  y can  assessed using  correlation coefficient  simple linear regression model relating x  y  effect   total derivative  y  respect  x care must  taken  interpreting regression results     regressors may  allow  marginal changes   dummy variables   intercept term  others   held fixed recall  example   introduction    impossible  hold t fixed     time change  value  t   possible   unique effect can  nearly zero even   marginal effect  large  may imply    covariate captures   information  x     variable    model    contribution  x   variation  y conversely  unique effect  x can  large   marginal effect  nearly zero   happen    covariates explained  great deal   variation  y   mainly explain variation   way   complementary    captured  x   case including   variables   model reduces  part   variability  y   unrelated  x thereby strengthening  apparent relationship  x  meaning   expression held fixed may depend    values   predictor variables arise   experimenter directly sets  values   predictor variables according   study design  comparisons  interest may literally correspond  comparisons among units whose predictor variables   held fixed   experimenter alternatively  expression held fixed can refer   selection  takes place   context  data analysis   case  hold  variable fixed  restricting  attention   subsets   data  happen    common value   given predictor variable     interpretation  held fixed  can  used   observational study  notion   unique effect  appealing  studying  complex system  multiple interrelated components influence  response variable   cases  can literally  interpreted   causal effect   intervention   linked   value   predictor variable however    argued   many cases multiple regression analysis fails  clarify  relationships   predictor variables   response variable   predictors  correlated       assigned following  study design  commonality analysis may  helpful  disentangling  shared  unique impacts  correlated independent variables extensions numerous extensions  linear regression   developed  allow      assumptions underlying  basic model   relaxed simple  multiple regression   simplest case   single scalar predictor variable x   single scalar response variable y  known  simple linear regression  extension  multiple andor vectorvalued predictor variables denoted   capital x  known  multiple linear regression also known  multivariable linear regression nearly  realworld regression models involve multiple predictors  basic descriptions  linear regression  often phrased  terms   multiple regression model note however    cases  response variable y  still  scalar another term multivariate linear regression refers  cases  y   vector ie    general linear regression general linear models  general linear model considers  situation   response variable y    scalar   vector conditional linearity  eyx bx  still assumed   matrix b replacing  vector   classical linear regression model multivariate analogues  ordinary leastsquares ols  generalized leastsquares gls   developed general linear models  also called multivariate linear models       multivariable linear models also called multiple linear models heteroscedastic models various models   created  allow  heteroscedasticity ie  errors  different response variables may  different variances  example weighted least squares   method  estimating linear regression models   response variables may  different error variances possibly  correlated errors see also weighted linear least squares  generalized least squares heteroscedasticityconsistent standard errors   improved method  use  uncorrelated  potentially heteroscedastic errors generalized linear models generalized linear models glms   framework  modeling  response variable y   bounded  discrete   used  example generalized linear models allow   arbitrary link function g  relates  mean   response variable   predictors ie ey gx  link function  often related   distribution   response   particular  typically   effect  transforming   formula range   linear predictor   range   response variable  common examples  glms  single index models allow  degree  nonlinearity   relationship  x  y  preserving  central role   linear predictor x    classical linear regression model  certain conditions simply applying ols  data   singleindex model will consistently estimate    proportionality constant hierarchical linear models hierarchical linear models  multilevel regression organizes  data   hierarchy  regressions  example    regressed  b  b  regressed  c   often used   variables  interest   natural hierarchical structure    educational statistics  students  nested  classrooms classrooms  nested  schools  schools  nested   administrative grouping    school district  response variable might   measure  student achievement    test score  different covariates   collected   classroom school  school district levels errorsinvariables errorsinvariables models  measurement error models extend  traditional linear regression model  allow  predictor variables x   observed  error  error causes standard estimators   become biased generally  form  bias   attenuation meaning   effects  biased toward zero estimation methods  large number  procedures   developed  parameter estimation  inference  linear regression  methods differ  computational simplicity  algorithms presence   closedform solution robustness  respect  heavytailed distributions  theoretical assumptions needed  validate desirable statistical properties   consistency  asymptotic efficiency     common estimation techniques  linear regression  summarized   discussion  statistics  numerical analysis  problem  numerical methods  linear least squares   important one  linear regression models  one    important types  model   formal statistical models   exploration  data sets  majority  statistical computer packages contain facilities  regression analysis  make use  linear least squares computations hence   appropriate  considerable effort   devoted   task  ensuring   computations  undertaken efficiently   due regard  numerical precision individual statistical analyses  seldom undertaken  isolation  rather  part   sequence  investigatory steps    topics involved  considering numerical methods  linear least squares relate   point thus important topics can  fitting  linear models  least squares often   always arises   context  statistical analysis  can therefore  important  considerations  computational efficiency   problems extend     auxiliary quantities required   analyses    restricted   formal solution   linear least squares problem matrix calculations like  others  affected  rounding errors  early summary   effects regarding  choice  computational methods  matrix inversion  provided  wilkinson using linear algebra  follows  one can find  best approximation  another function  minimizing  area  two functions  continuous function formula  formula   function formula  formula   subspace  formula  within  subspace formula due   frequent difficulty  evaluating integrands involving absolute value one can instead define   adequate criterion  obtaining  least squares approximation function formula  formula  respect   inner product space formula   formula  equivalently formula can thus  written  vector form   words  least squares approximation  formula   function formula closest  formula  terms   inner product formula furthermore  can  applied   theorem applications  linear regression linear regression  widely used  biological behavioral  social sciences  describe possible relationships  variables  ranks  one    important tools used   disciplines trend line  trend line represents  trend  longterm movement  time series data   components   accounted   tells whether  particular data set say gdp oil prices  stock prices  increased  decreased   period  time  trend line  simply  drawn  eye   set  data points   properly  position  slope  calculated using statistical techniques like linear regression trend lines typically  straight lines although  variations use higher degree polynomials depending   degree  curvature desired   line trend lines  sometimes used  business analytics  show changes  data  time    advantage   simple trend lines  often used  argue   particular action  event   training   advertising campaign caused observed changes   point  time    simple technique    require  control group experimental design   sophisticated analysis technique however  suffers   lack  scientific validity  cases   potential changes can affect  data epidemiology early evidence relating tobacco smoking  mortality  morbidity came  observational studies employing regression analysis  order  reduce spurious correlations  analyzing observational data researchers usually include several variables   regression models  addition   variable  primary interest  example suppose    regression model   cigarette smoking   independent variable  interest   dependent variable  lifespan measured  years researchers might include socioeconomic status   additional independent variable  ensure   observed effect  smoking  lifespan   due   effect  education  income however   never possible  include  possible confounding variables   empirical analysis  example  hypothetical gene might increase mortality  also cause people  smoke    reason randomized controlled trials  often able  generate  compelling evidence  causal relationships  can  obtained using regression analyses  observational data  controlled experiments   feasible variants  regression analysis   instrumental variables regression may  used  attempt  estimate causal relationships  observational data finance  capital asset pricing model uses linear regression  well   concept  beta  analyzing  quantifying  systematic risk   investment  comes directly   beta coefficient   linear regression model  relates  return   investment   return   risky assets economics linear regression   predominant empirical tool  economics  example   used  predict consumption spending fixed investment spending inventory investment purchases   countrys exports spending  imports  demand  hold liquid assets labor demand  labor supply environmental science linear regression finds application   wide range  environmental science applications  canada  environmental effects monitoring program uses statistical analyses  fish  benthic surveys  measure  effects  pulp mill  metal mine effluent   aquatic ecosystem\r\n"}
{"index":{"_id":41}}
{"conceptLabelTag":"computational learning theory","conceptLabel":"computational learning theory","conceptDescription":"computational learning theory  computer science computational learning theory  just learning theory   subfield  artificial intelligence devoted  studying  design  analysis  machine learning algorithms overview theoretical results  machine learning mainly deal   type  inductive learning called supervised learning  supervised learning  algorithm  given samples   labeled   useful way  example  samples might  descriptions  mushrooms   labels   whether    mushrooms  edible  algorithm takes  previously labeled samples  uses   induce  classifier  classifier   function  assigns labels  samples including  samples   never  previously seen   algorithm  goal   supervised learning algorithm   optimize  measure  performance   minimizing  number  mistakes made  new samples  addition  performance bounds computational learning theory studies  time complexity  feasibility  learning  computational learning theory  computation  considered feasible   can  done  polynomial time   two kinds  time complexity results negative results often rely  commonly believed  yet unproven assumptions     several different approaches  computational learning theory  differences  based  making assumptions   inference principles used  generalize  limited data  includes different definitions  probability see frequency probability bayesian probability  different assumptions   generation  samples  different approaches include computational learning theory  led  several practical algorithms  example pac theory inspired boosting vc theory led  support vector machines  bayesian inference led  belief networks  judea pearl  description     publications  given  important publications  machine learning\r\n"}
{"index":{"_id":42}}
{"conceptLabelTag":"expectation propagation","conceptLabel":"expectation propagation","conceptDescription":"expectation propagation expectation propagation ep   technique  bayesian machine learning ep finds approximations   probability distribution  uses  iterative approach  leverages  factorization structure   target distribution  differs   bayesian approximation approaches   variational bayesian methods\r\n"}
{"index":{"_id":43}}
{"conceptLabelTag":"naive bayes classifier","conceptLabel":"naive bayes classifier","conceptDescription":"naive bayes classifier  machine learning naive bayes classifiers   family  simple probabilistic classifiers based  applying bayes theorem  strong naive independence assumptions   features naive bayes   studied extensively since  s   introduced   different name   text retrieval community   early s  remains  popular baseline method  text categorization  problem  judging documents  belonging  one category      spam  legitimate sports  politics etc  word frequencies   features  appropriate preprocessing   competitive   domain   advanced methods including support vector machines  also finds application  automatic medical diagnosis naive bayes classifiers  highly scalable requiring  number  parameters linear   number  variables featurespredictors   learning problem maximumlikelihood training can  done  evaluating  closedform expression  takes linear time rather   expensive iterative approximation  used  many  types  classifiers   statistics  computer science literature naive bayes models  known   variety  names including simple bayes  independence bayes   names reference  use  bayes theorem   classifiers decision rule  naive bayes   necessarily  bayesian method introduction naive bayes   simple technique  constructing classifiers models  assign class labels  problem instances represented  vectors  feature values   class labels  drawn   finite set     single algorithm  training  classifiers   family  algorithms based   common principle  naive bayes classifiers assume   value   particular feature  independent   value    feature given  class variable  example  fruit may  considered    apple    red round   cm  diameter  naive bayes classifier considers    features  contribute independently   probability   fruit   apple regardless   possible correlations   color roundness  diameter features   types  probability models naive bayes classifiers can  trained  efficiently   supervised learning setting  many practical applications parameter estimation  naive bayes models uses  method  maximum likelihood   words one can work   naive bayes model without accepting bayesian probability  using  bayesian methods despite  naive design  apparently oversimplified assumptions naive bayes classifiers  worked quite well  many complex realworld situations   analysis   bayesian classification problem showed    sound theoretical reasons   apparently implausible efficacy  naive bayes classifiers still  comprehensive comparison   classification algorithms  showed  bayes classification  outperformed   approaches   boosted trees  random forests  advantage  naive bayes     requires  small number  training data  estimate  parameters necessary  classification probabilistic model abstractly naive bayes   conditional probability model given  problem instance   classified represented   vector formula representing  features independent variables  assigns   instance probabilities    possible outcomes  classes formula  problem    formulation     number  features  large    feature can take   large number  values  basing   model  probability tables  infeasible  therefore reformulate  model  make   tractable using bayes theorem  conditional probability can  decomposed   plain english using bayesian probability terminology   equation can  written   practice   interest    numerator   fraction   denominator   depend  formula   values   features formula  given    denominator  effectively constant  numerator  equivalent   joint probability model  can  rewritten  follows using  chain rule  repeated applications   definition  conditional probability now  naive conditional independence assumptions come  play assume   feature formula  conditionally independent  every  feature formula  formula given  category formula  means  thus  joint model can  expressed   means     independence assumptions  conditional distribution   class variable formula    evidence formula   scaling factor dependent   formula    constant   values   feature variables  known constructing  classifier   probability model  discussion  far  derived  independent feature model    naive bayes probability model  naive bayes classifier combines  model   decision rule one common rule   pick  hypothesis    probable   known   maximum  posteriori  map decision rule  corresponding classifier  bayes classifier   function  assigns  class label formula    follows parameter estimation  event models  classs prior may  calculated  assuming equiprobable classes ie priors number  classes   calculating  estimate   class probability   training set ie prior   given class number  samples   class total number  samples  estimate  parameters   features distribution one must assume  distribution  generate nonparametric models   features   training set  assumptions  distributions  features  called  event model   naive bayes classifier  discrete features like  ones encountered  document classification include spam filtering multinomial  bernoulli distributions  popular  assumptions lead  two distinct models   often confused gaussian naive bayes  dealing  continuous data  typical assumption    continuous values associated   class  distributed according   gaussian distribution  example suppose  training data contains  continuous attribute formula  first segment  data   class   compute  mean  variance  formula   class let formula   mean   values  formula associated  class c  let formula   variance   values  formula associated  class c suppose   collected  observation value formula   probability distribution  formula given  class formula formula can  computed  plugging formula   equation   normal distribution parameterized  formula  formula   another common technique  handling continuous values   use binning  discretize  feature values  obtain  new set  bernoullidistributed features  literature  fact suggests    necessary  apply naive bayes       discretization may throw away discriminative information multinomial naive bayes   multinomial event model samples feature vectors represent  frequencies   certain events   generated   multinomial formula  formula   probability  event occurs   multinomials   multiclass case  feature vector formula    histogram  formula counting  number  times event  observed   particular instance    event model typically used  document classification  events representing  occurrence   word   single document see bag  words assumption  likelihood  observing  histogram  given   multinomial naive bayes classifier becomes  linear classifier  expressed  logspace  formula  formula   given class  feature value never occur together   training data   frequencybased probability estimate will  zero   problematic   will wipe   information    probabilities    multiplied therefore   often desirable  incorporate  smallsample correction called pseudocount   probability estimates    probability  ever set   exactly zero  way  regularizing naive bayes  called laplace smoothing   pseudocount  one  lidstone smoothing   general case rennie et al discuss problems   multinomial assumption   context  document classification  possible ways  alleviate  problems including  use  tfidf weights instead  raw term frequencies  document length normalization  produce  naive bayes classifier   competitive  support vector machines bernoulli naive bayes   multivariate bernoulli event model features  independent booleans binary variables describing inputs like  multinomial model  model  popular  document classification tasks  binary term occurrence features  used rather  term frequencies  formula   boolean expressing  occurrence  absence   th term   vocabulary   likelihood   document given  class formula  given   formula   probability  class formula generating  term formula  event model  especially popular  classifying short texts    benefit  explicitly modelling  absence  terms note   naive bayes classifier   bernoulli event model       multinomial nb classifier  frequency counts truncated  one semisupervised parameter estimation given  way  train  naive bayes classifier  labeled data  possible  construct  semisupervised training algorithm  can learn   combination  labeled  unlabeled data  running  supervised learning algorithm   loop convergence  determined based  improvement   model likelihood formula  formula denotes  parameters   naive bayes model  training algorithm   instance    general expectationmaximization algorithm em  prediction step inside  loop   estep  em   retraining  naive bayes   mstep  algorithm  formally justified   assumption   data  generated   mixture model   components   mixture model  exactly  classes   classification problem discussion despite  fact   farreaching independence assumptions  often inaccurate  naive bayes classifier  several properties  make  surprisingly useful  practice  particular  decoupling   class conditional feature distributions means   distribution can  independently estimated   onedimensional distribution  helps alleviate problems stemming   curse  dimensionality    need  data sets  scale exponentially   number  features  naive bayes often fails  produce  good estimate   correct class probabilities  may    requirement  many applications  example  naive bayes classifier will make  correct map decision rule classification  long   correct class   probable    class   true regardless  whether  probability estimate  slightly  even grossly inaccurate   manner  overall classifier can  robust enough  ignore serious deficiencies   underlying naive probability model  reasons   observed success   naive bayes classifier  discussed   literature cited  relation  logistic regression   case  discrete inputs indicator  frequency features  discrete events naive bayes classifiers form  generativediscriminative pair  multinomial logistic regression classifiers  naive bayes classifier can  considered  way  fitting  probability model  optimizes  joint likelihood formula  logistic regression fits   probability model  optimize  conditional formula  link   two can  seen  observing   decision function  naive bayes   binary case can  rewritten  predict class formula   odds  formula exceed   formula expressing   logspace gives  lefthand side   equation   logodds  logit  quantity predicted   linear model  underlies logistic regression since naive bayes  also  linear model   two discrete event models  can  reparametrised   linear function formula obtaining  probabilities    matter  applying  logistic function  formula    multiclass case  softmax function discriminative classifiers  lower asymptotic error  generative ones however research  ng  jordan  shown    practical cases naive bayes can outperform logistic regression   reaches  asymptotic error faster examples gender classification problem classify whether  given person   male   female based   measured features  features include height weight  foot size training example training set   classifier created   training set using  gaussian distribution assumption   given variances  unbiased sample variances lets say   equiprobable classes  pmale pfemale  prior probability distribution might  based   knowledge  frequencies   larger population   frequency   training set testing    sample   classified   male  female  wish  determine  posterior  greater male  female   classification  male  posterior  given    classification  female  posterior  given   evidence also termed normalizing constant may  calculated however given  sample  evidence   constant  thus scales  posteriors equally  therefore   affect classification  can  ignored  now determine  probability distribution   sex   sample  formula  formula   parameters  normal distribution    previously determined   training set note   value greater   ok     probability density rather   probability  height   continuous variable since posterior numerator  greater   female case  predict  sample  female document classification    worked example  naive bayesian classification   document classification problem consider  problem  classifying documents   content  example  spam  nonspam emails imagine  documents  drawn   number  classes  documents  can  modeled  sets  words   independent probability   ith word   given document occurs   document  class c can  written    probability   given document d contains    words formula given  class c   question   desire  answer     probability   given document d belongs   given class c   words   formula now  definition  bayes theorem manipulates    statement  probability  terms  likelihood assume   moment     two mutually exclusive classes s  s eg spam   spam   every element email   either one     using  bayesian result   can write dividing one    gives  can  refactored  thus  probability ratio ps d ps d can  expressed  terms   series  likelihood ratios  actual probability ps d can  easily computed  log ps d ps d based   observation  ps d ps d taking  logarithm    ratios    technique  loglikelihood ratios   common technique  statistics   case  two mutually exclusive alternatives    example  conversion   loglikelihood ratio   probability takes  form   sigmoid curve see logit  details finally  document can  classified  follows   spam  formula  e formula otherwise    spam\r\n"}
{"index":{"_id":44}}
{"conceptLabelTag":"reinforcement learning","conceptLabel":"reinforcement learning","conceptDescription":"reinforcement learning reinforcement learning   area  machine learning inspired  behaviorist psychology concerned   software agents   take actions   environment    maximize  notion  cumulative reward  problem due   generality  studied  many  disciplines   game theory control theory operations research information theory simulationbased optimization multiagent systems swarm intelligence statistics  genetic algorithms   operations research  control literature  field  reinforcement learning methods  studied  called approximate dynamic programming  problem   studied   theory  optimal control though  studies  concerned   existence  optimal solutions   characterization     learning  approximation aspects  economics  game theory reinforcement learning may  used  explain  equilibrium may arise  bounded rationality  machine learning  environment  typically formulated   markov decision process mdp  many reinforcement learning algorithms   context utilize dynamic programming techniques  main difference   classical techniques  reinforcement learning algorithms    latter   need knowledge   mdp   target large mdps  exact methods become infeasible reinforcement learning differs  standard supervised learning   correct inputoutput pairs  never presented  suboptimal actions explicitly corrected     focus  online performance  involves finding  balance  exploration  uncharted territory  exploitation  current knowledge  exploration vs exploitation tradeoff  reinforcement learning    thoroughly studied   multiarmed bandit problem   finite mdps introduction  basic reinforcement learning model consists   rules  often stochastic  observation typically involves  scalar immediate reward associated   last transition  many works  agent  also assumed  observe  current environmental state   case  talk  full observability whereas   opposing case  talk  partial observability sometimes  set  actions available   agent  restricted eg   spend  money    possess  reinforcement learning agent interacts   environment  discrete time steps   time formula  agent receives  observation formula  typically includes  reward formula   chooses  action formula   set  actions available   subsequently sent   environment  environment moves   new state formula   reward formula associated   transition formula  determined  goal   reinforcement learning agent   collect  much reward  possible  agent can choose  action   function   history   can even randomize  action selection   agents performance  compared     agent  acts optimally   beginning  difference  performance gives rise   notion  regret note   order  act near optimally  agent must reason   long term consequences   actions  order  maximize  future income   better go  school now although  immediate monetary reward associated   might  negative thus reinforcement learning  particularly wellsuited  problems  include  longterm versus shortterm reward tradeoff    applied successfully  various problems including robot control elevator scheduling telecommunications backgammon checkers  go alphago two components make reinforcement learning powerful  use  samples  optimize performance   use  function approximation  deal  large environments thanks   two key components reinforcement learning can  used  large environments     following situations  first two   problems   considered planning problems since  form   model  available   last one   considered   genuine learning problem however   reinforcement learning methodology  planning problems   converted  machine learning problems exploration  reinforcement learning problem  described requires clever exploration mechanisms randomly selecting actions without reference   estimated probability distribution  known  give rise   poor performance  case  small finite mdps  relatively well understood  now however due   lack  algorithms   provably scale well   number  states  scale  problems  infinite state spaces  practice people resort  simple exploration methods one  method  formulagreedy   agent chooses  action   believes   best longterm effect  probability formula   chooses  action uniformly  random otherwise  formula   tuning parameter   sometimes changed either according   fixed schedule making  agent explore less  time goes   adaptively based   heuristics algorithms  control learning even   issue  exploration  disregarded  even   state  observable   assume  now   problem remains  find   actions  good based  past experience criterion  optimality  simplicity assume   moment   problem studied  episodic  episode ending   terminal state  reached assume    matter  course  actions  agent takes termination  inevitable   mild regularity conditions  expectation   total reward   welldefined   policy   initial distribution   states   policy refers   mapping  assigns  probability distribution   actions   possible histories given  fixed initial distribution formula  can thus assign  expected return formula  policy formula   random variable formula denotes  return   defined   formula   reward received   formulath transition  initial state  sampled  random  formula  actions  selected  policy formula  formula denotes  random time   terminal state  reached ie  time   episode terminates   case  nonepisodic problems  return  often discounted giving rise   total expected discounted reward criterion  formula   socalled discountfactor since  undiscounted return   special case   discounted return  now   will assume discounting although  looks innocent enough discounting   fact problematic  one cares  online performance    discounting makes  initial time steps  important since  learning agent  likely  make mistakes   first  steps   life starts  uninformed learning algorithm can achieve nearoptimal performance  discounting even   class  environments  restricted    finite mdps    mean though  given enough time  learning agent  figure   act nearoptimally  time  restarted  problem    specify  algorithm  can  used  find  policy  maximum expected return   theory  mdps   known  without loss  generality  search can  restricted   set   socalled stationary policies  policy  called stationary   actiondistribution returned   depends    last state visited   part   observation history   agent   simplifying assumption  fact  search can   restricted  deterministic stationary policies  deterministic stationary policy  one  deterministically selects actions based   current state since   policy can  identified   mapping   set  states   set  actions  policies can  identified   mappings   loss  generality brute force  brute force approach entails  following two steps one problem      number  policies can  extremely large  even infinite another   variance   returns might  large   case  large number  samples will  required  accurately estimate  return   policy  problems can  ameliorated   assume  structure  perhaps allow samples generated  one policy  influence  estimates made  another  two main approaches  achieving   value function estimation  direct policy search value function approaches value function approaches attempt  find  policy  maximizes  return  maintaining  set  estimates  expected returns   policy usually either  current onpolicy   optimal offpolicy one  methods rely   theory  mdps  optimality  defined   sense   stronger    one  policy  called optimal   achieves  best expected return   initial state ie initial distributions play  role   definition  one can always find  optimal policy amongst stationary policies  define optimality   formal manner define  value   policy formula   formula stands   random return associated  following formula   initial state formula define formula   maximum possible value  formula  formula  allowed  change  policy  achieves  optimal values   state  called optimal clearly  policy   optimal   strong sense  also optimal   sense   maximizes  expected return formula since formula  formula   state randomly sampled   distribution formula although statevalues suffice  define optimality  will prove   useful  define actionvalues given  state formula  action formula   policy formula  actionvalue   pair formula  formula  defined   now formula stands   random return associated  first taking action formula  state formula  following formula thereafter   wellknown   theory  mdps   someone gives us formula   optimal policy  can always choose optimal actions  thus act optimally  simply choosing  action   highest value   state  actionvalue function    optimal policy  called  optimal actionvalue function   denoted  formula  summary  knowledge   optimal actionvalue function alone suffices  know   act optimally assuming full knowledge   mdp   two basic approaches  compute  optimal actionvalue function value iteration  policy iteration  algorithms compute  sequence  functions formula formula  converge  formula computing  functions involves computing expectations   whole statespace   impractical     smallest finite mdps never mind  case   mdp  unknown  reinforcement learning methods  expectations  approximated  averaging  samples  one uses function approximation techniques  cope   need  represent value functions  large stateaction spaces monte carlo methods  simplest monte carlo methods can  used   algorithm  mimics policy iteration policy iteration consists  two steps policy evaluation  policy improvement  monte carlo methods  used   policy evaluation step   step given  stationary deterministic policy formula  goal   compute  function values formula   good approximation     stateaction pairs formula assume  simplicity   mdp  finite   fact  table representing  actionvalues fits   memory  assume   problem  episodic    episode  new one starts   random initial state   estimate   value   given stateaction pair formula can  computed  simply averaging  sampled returns  originated  formula  time given enough time  procedure can thus construct  precise estimate formula   actionvalue function formula  finishes  description   policy evaluation step   policy improvement step    done   standard policy iteration algorithm  next policy  obtained  computing  greedy policy  respect  formula given  state formula  new policy returns  action  maximizes formula  practice one often avoids computing  storing  new policy  uses lazy evaluation  defer  computation   maximizing actions     actually needed   problems   procedure   follows temporal difference methods  first issue  easily corrected  allowing  procedure  change  policy      states   values settle however good  sounds  may  problematic   might prevent convergence still  current algorithms implement  idea giving rise   class  generalized policy iteration algorithm  note  passing  actor critic methods belong   category  second issue can  corrected within  algorithm  allowing trajectories  contribute   stateaction pair    may also help   extent   third problem although  better solution  returns  high variance   suttons temporal difference td methods   based   recursive bellman equation note   computation  td methods can  incremental    transition  memory  changed   transition  thrown away  batch   transitions  collected    estimates  computed  based   large number  transitions batch methods  prime example     leastsquares temporal difference method due  may use  information   samples better whereas incremental methods    choice  batch methods become infeasible due   high computational  memory complexity  addition  exist methods  try  unify  advantages   two approaches methods based  temporal differences also overcome  second  last issue  order  address  last issue mentioned   previous section function approximation methods  used  linear function approximation one starts   mapping formula  assigns  finitedimensional vector   stateaction pair   action values   stateaction pair formula  obtained  linearly combining  components  formula   weights formula  algorithms  adjust  weights instead  adjusting  values associated   individual stateaction pairs however linear function approximation     choice  recently methods based  ideas  nonparametric statistics  can  seen  construct   features   explored  far  discussion  restricted   policy iteration can  used   basis   designing reinforcement learning algorithms equally importantly value iteration can also  used   starting point giving rise   qlearning algorithm   many variants  problem  methods  use actionvalues    may need highly precise estimates   competing action values  can  hard  obtain   returns  noisy though  problem  mitigated   extent  temporal difference methods   one uses  socalled compatible function approximation method  work remains   done  increase generality  efficiency another problem specific  temporal difference methods comes   reliance   recursive bellman equation  temporal difference methods   socalled formula parameter formula  allows one  continuously interpolate  montecarlo methods    rely   bellman equations   basic temporal difference methods  rely entirely   bellman equations  can thus  effective  palliating  issue direct policy search  alternative method  find  good policy   search directly   subset   policy space   case  problem becomes  instance  stochastic optimization  two approaches available  gradientbased  gradient methods gradientbased methods giving rise   socalled policy gradient methods start   mapping   finitedimensional parameter space   space  policies given  parameter vector formula let formula denote  policy associated  formula define  performance function   mild conditions  function will  differentiable   function   parameter vector formula   gradient  formula  known one  use gradient ascent since  analytic expression   gradient   available one must rely   noisy estimate   estimate can  constructed  many ways giving rise  algorithms like williams reinforce method   also known   likelihood ratio method   simulationbased optimization literature policy gradient methods  received  lot  attention   last couple  years eg   remain  active field  overview  policy search methods   context  robotics   given  deisenroth neumann  peters  issue  many   methods    may get stuck  local optima    based  local search  large class  methods avoids relying  gradient information  include simulated annealing crossentropy search  methods  evolutionary computation many gradient methods can achieve  theory    limit  global optimum   number  cases   indeed demonstrated remarkable performance  issue  policy search methods    may converge slowly   information based    act  noisy  example  happens   episodic problems  trajectories  long   variance   returns  large  argued beforehand valuefunction based methods  rely  temporal differences might help   case  recent years several actorcritic algorithms   proposed following  idea   demonstrated  perform well  various problems theory  theory  small finite mdps  quite mature   asymptotic  finitesample behavior   algorithms  wellunderstood  mentioned beforehand algorithms  provably good online performance addressing  exploration issue  known  theory  large mdps needs  work efficient exploration  largely untouched except   case  bandit problems although finitetime performance bounds appeared  many algorithms   recent years  bounds  expected   rather loose  thus  work  needed  better understand  relative advantages  well   limitations   algorithms  incremental algorithm asymptotic convergence issues   settled recently new incremental temporaldifferencebased algorithms  appeared  converge   much wider set  conditions   previously possible  example  used  arbitrary smooth function approximation current research current research topics include adaptive methods  work  fewer   parameters   large number  conditions addressing  exploration problem  large mdps largescale empirical evaluations learning  acting  partial information eg using predictive state representation modular  hierarchical reinforcement learning improving existing valuefunction  policy search methods algorithms  work well  large  continuous action spaces transfer learning lifelong learning efficient samplebased planning eg based  montecarlo tree search multiagent  distributed reinforcement learning  also  topic  interest  current research   also  growing interest  real life applications  reinforcement learning successes  reinforcement learning  listed  reinforcement learning algorithms   td learning  also  investigated   model  dopaminebased learning   brain   model  dopaminergic projections   substantia nigra   basal ganglia function   prediction error reinforcement learning  also  used   part   model  human skill learning especially  relation   interaction  implicit  explicit learning  skill acquisition  first publication   application       many followup studies   multiple applications  reinforcement learning  generate models  train   play video games   atari games   models reinforcement learning finds  actions   best reward   play  method   widely used method  combination  deep neural networks  teach computers  play atari video games inverse reinforcement learning  inverse reinforcement learning irl  reward function  given instead one tries  extract  reward function given  observed behavior   expert  idea   mimic  observed behavior   often optimal  close  optimal  apprenticeship learning one assumes   expert demonstrating  ideal behavior  tries  recover  policy directly using  observations   expert literature conferences journals  reinforcement learning papers  published   major machine learning  ai conferences icml nips aaai ijcai uai ai  statistics  journals jair jmlr machine learning journal ieee tciaig  theory papers  published  colt  alt however many papers appear  robotics conferences iros icra   agent conference aamas operations researchers publish  papers   informs conference   example   operation research   mathematics  operations research journals control researchers publish  papers   cdc  acc conferences  eg   journals ieee transactions  automatic control  automatica although applied works tend   published   specialized journals  winter simulation conference also publishes many relevant papers    papers also published   major conferences   neural networks fuzzy  evolutionary computation communities  annual ieee symposium titled approximate dynamic programming  reinforcement learning adprl   biannual european workshop  reinforcement learning ewrl  two regularly held meetings  rl researchers meet\r\n"}
{"index":{"_id":45}}
{"conceptLabelTag":"dimensionality reduction","conceptLabel":"dimensionality reduction","conceptDescription":"dimensionality reduction  machine learning  statistics dimensionality reduction  dimension reduction   process  reducing  number  random variables  consideration via obtaining  set  principal variables  can  divided  feature selection  feature extraction feature selection feature selection approaches try  find  subset   original variables also called features  attributes   three strategies filter eg information gain  wrapper eg search guided  accuracy approaches  embedded features  selected  add   removed  building  model based   prediction errors see also combinatorial optimization problems   cases data analysis   regression  classification can  done   reduced space  accurately    original space feature extraction feature extraction transforms  data   highdimensional space   space  fewer dimensions  data transformation may  linear   principal component analysis pca  many nonlinear dimensionality reduction techniques also exist  multidimensional data tensor representation can  used  dimensionality reduction  multilinear subspace learning principal component analysis pca  main linear technique  dimensionality reduction principal component analysis performs  linear mapping   data   lowerdimensional space    way   variance   data   lowdimensional representation  maximized  practice  covariance  sometimes  correlation matrix   data  constructed   eigen vectors   matrix  computed  eigen vectors  correspond   largest eigenvalues  principal components can now  used  reconstruct  large fraction   variance   original data moreover  first  eigen vectors can often  interpreted  terms   largescale physical behavior   system  original space  dimension   number  points   reduced  data loss  hopefully retaining   important variance   space spanned    eigenvectors kernel pca principal component analysis can  employed   nonlinear way  means   kernel trick  resulting technique  capable  constructing nonlinear mappings  maximize  variance   data  resulting technique  entitled kernel pca graphbased kernel pca  prominent nonlinear techniques include manifold learning techniques   isomap locally linear embedding lle hessian lle laplacian eigenmaps  local tangent space alignment ltsa  techniques construct  lowdimensional data representation using  cost function  retains local properties   data  can  viewed  defining  graphbased kernel  kernel pca  recently techniques   proposed  instead  defining  fixed kernel try  learn  kernel using semidefinite programming   prominent example    technique  maximum variance unfolding mvu  central idea  mvu   exactly preserve  pairwise distances  nearest neighbors   inner product space  maximizing  distances  points    nearest neighbors  alternative approach  neighborhood preservation    minimization   cost function  measures differences  distances   input  output spaces important examples   techniques include classical multidimensional scaling   identical  pca isomap  uses geodesic distances   data space diffusion maps  use diffusion distances   data space tdistributed stochastic neighbor embedding tsne  minimizes  divergence  distributions  pairs  points  curvilinear component analysis  different approach  nonlinear dimensionality reduction    use  autoencoders  special kind  feedforward neural networks   bottleneck hidden layer  training  deep encoders  typically performed using  greedy layerwise pretraining eg using  stack  restricted boltzmann machines   followed   finetuning stage based  backpropagation linear discriminant analysis lda linear discriminant analysis lda   generalization  fishers linear discriminant  method used  statistics pattern recognition  machine learning  find  linear combination  features  characterizes  separates two   classes  objects  events generalized discriminant analysis gda gda deals  nonlinear discriminant analysis using kernel function operator  underlying theory  close   support vector machines svm insofar   gda method provides  mapping   input vectors  highdimensional feature space similar  lda  objective  gda   find  projection   features   lower dimensional space  maximizing  ratio  betweenclass scatter  withinclass scatter dimension reduction  highdimensional datasets ie  number  dimensions   dimension reduction  usually performed prior  applying  knearest neighbors algorithm knn  order  avoid  effects   curse  dimensionality feature extraction  dimension reduction can  combined  one step using principal component analysis pca linear discriminant analysis lda  canonical correlation analysis cca techniques   preprocessing step followed  clustering  knn  feature vectors  reduceddimension space  machine learning  process  also called lowdimensional embedding  veryhighdimensional datasets eg  performing similarity search  live video streams dna data  highdimensional time series running  fast approximate knn search using locality sensitive hashing random projection sketches   highdimensional similarity search techniques   vldb toolbox might    feasible option applications  dimensionality reduction technique   sometimes used  neuroscience  maximally informative dimensions  finds  lowerdimensional representation   dataset    much information  possible   original data  preserved\r\n"}
{"index":{"_id":46}}
{"conceptLabelTag":"feature extraction","conceptLabel":"feature extraction","conceptDescription":"feature extraction  machine learning pattern recognition   image processing feature extraction starts   initial set  measured data  builds derived values features intended   informative  nonredundant facilitating  subsequent learning  generalization steps    cases leading  better human interpretations feature extraction  related  dimensionality reduction   input data   algorithm   large   processed    suspected   redundant eg   measurement   feet  meters   repetitiveness  images presented  pixels   can  transformed   reduced set  features also named  feature vector determining  subset   initial features  called feature selection  selected features  expected  contain  relevant information   input data    desired task can  performed  using  reduced representation instead   complete initial data general feature extraction involves reducing  amount  resources required  describe  large set  data  performing analysis  complex data one   major problems stems   number  variables involved analysis   large number  variables generally requires  large amount  memory  computation power also  may cause  classification algorithm  overfit  training samples  generalize poorly  new samples feature extraction   general term  methods  constructing combinations   variables  get around  problems  still describing  data  sufficient accuracy  best results  achieved   expert constructs  set  applicationdependent features  process called feature engineering nevertheless    expert knowledge  available general dimensionality reduction techniques may help  include image processing one  important area  application  image processing   algorithms  used  detect  isolate various desired portions  shapes features   digitized image  video stream   particularly important   area  optical character recognition feature extraction  software many data analysis software packages provide  feature extraction  dimension reduction common numerical programming environments   matlab scilab numpy   r language provide    simpler feature extraction techniques eg principal component analysis via builtin commands  specific algorithms  often available  publicly available scripts  thirdparty addons\r\n"}
{"index":{"_id":47}}
{"conceptLabelTag":"markov model","conceptLabel":"markov model","conceptDescription":"markov model  probability theory  markov model   stochastic model used  model randomly changing systems    assumed  future states depend    current state    events  occurred      assumes  markov property generally  assumption enables reasoning  computation   model   otherwise  intractable   reason   fields  predictive modelling  probabilistic forecasting   desirable   given model  exhibit  markov property introduction   four common markov models used  different situations depending  whether every sequential state  observable    whether  system    adjusted   basis  observations made markov chain  simplest markov model   markov chain  models  state   system   random variable  changes  time   context  markov property suggests   distribution   variable depends    distribution  previous state  example use   markov chain  markov chain monte carlo  uses  markov property  prove   particular method  performing  random walk will sample   joint distribution hidden markov model  hidden markov model   markov chain    state   partially observable   words observations  related   state   system    typically insufficient  precisely determine  state several wellknown algorithms  hidden markov models exist  example given  sequence  observations  viterbi algorithm will compute  mostlikely corresponding sequence  states  forward algorithm will compute  probability   sequence  observations   baumwelch algorithm will estimate  starting probabilities  transition function   observation function   hidden markov model one common use   speech recognition   observed data   speech audio waveform   hidden state   spoken text   example  viterbi algorithm finds   likely sequence  spoken words given  speech audio markov decision process  markov decision process   markov chain   state transitions depend   current state   action vector   applied   system typically  markov decision process  used  compute  policy  actions  will maximize  utility  respect  expected rewards   closely related  reinforcement learning  can  solved  value iteration  related methods partially observable markov decision process  partially observable markov decision process pomdp   markov decision process    state   system   partially observed pomdps  known   np complete  recent approximation techniques  made  useful   variety  applications   controlling simple agents  robots markov random field  markov random field  markov network may  considered    generalization   markov chain  multiple dimensions   markov chain state depends    previous state  time whereas   markov random field  state depends   neighbors    multiple directions  markov random field may  visualized   field  graph  random variables   distribution   random variable depends   neighboring variables     connected  specifically  joint distribution   random variable   graph can  computed   product   clique potentials    cliques   graph  contain  random variable modeling  problem   markov random field  useful   implies   joint distributions   vertex   graph may  computed   manner hierarchical markov models hierarchical markov models can  applied  categorize human behavior  various levels  abstraction  example  series  simple observations    persons location   room can  interpreted  determine  complex information     task  activity  person  performing two kinds  hierarchical markov models   hierarchical hidden markov model   abstract hidden markov model    used  behavior recognition  certain conditional independence properties  different levels  abstraction   model allow  faster learning  inference\r\n"}
{"index":{"_id":48}}
{"conceptLabelTag":"competitive learning","conceptLabel":"competitive learning","conceptDescription":"competitive learning competitive learning   form  unsupervised learning  artificial neural networks   nodes compete   right  respond   subset   input data  variant  hebbian learning competitive learning works  increasing  specialization   node   network   well suited  finding clusters within data models  algorithms based   principle  competitive learning include vector quantization  selforganizing maps kohonen maps principles   three basic elements   competitive learning rule accordingly  individual neurons   network learn  specialize  ensembles  similar patterns     become feature detectors  different classes  input patterns  fact  competitive networks recode sets  correlated inputs  one    output neurons essentially removes  redundancy  representation    essential part  processing  biological sensory systems architecture  implementation competitive learning  usually implemented  neural networks  contain  hidden layer   commonly known  competitive layer every competitive neuron  described   vector  weights formula  calculates  similarity measure   input data formula   weight vector formula  every input vector  competitive neurons compete     see  one      similar   particular input vector  winner neuron m sets  output formula     competitive neurons set  output formula usually  order  measure similarity  inverse   euclidean distance  used formula   input vector formula   weight vector formula example algorithm    simple competitive learning algorithm  find three clusters within  input data setup let  set  sensors  feed  three different nodes   every node  connected  every sensor let  weights   node gives   sensors  set randomly   let  output   node   sum    sensors  sensors signal strength  multiplied   weight   net  shown  input  node   highest output  deemed  winner  input  classified   within  cluster corresponding   node  winner updates    weights moving weight   connections  gave  weaker signals   connections  gave  stronger signals thus   data  received  node converges   centre   cluster    come  represent  activates  strongly  inputs   cluster   weakly  inputs   clusters\r\n"}
{"index":{"_id":49}}
{"conceptLabelTag":"cluster analysis","conceptLabel":"cluster analysis","conceptDescription":"cluster analysis cluster analysis  clustering   task  grouping  set  objects    way  objects    group called  cluster   similar   sense  another         groups clusters    main task  exploratory data mining   common technique  statistical data analysis used  many fields including machine learning pattern recognition image analysis information retrieval bioinformatics data compression  computer graphics cluster analysis    one specific algorithm   general task   solved  can  achieved  various algorithms  differ significantly   notion   constitutes  cluster    efficiently find  popular notions  clusters include groups  small distances among  cluster members dense areas   data space intervals  particular statistical distributions clustering can therefore  formulated   multiobjective optimization problem  appropriate clustering algorithm  parameter settings including values    distance function  use  density threshold   number  expected clusters depend   individual data set  intended use   results cluster analysis      automatic task   iterative process  knowledge discovery  interactive multiobjective optimization  involves trial  failure   often necessary  modify data preprocessing  model parameters   result achieves  desired properties besides  term clustering    number  terms  similar meanings including automatic classification numerical taxonomy botryology  greek grape  typological analysis  subtle differences  often   usage   results   data mining  resulting groups   matter  interest  automatic classification  resulting discriminative power   interest cluster analysis  originated  anthropology  driver  kroeber   introduced  psychology  zubin   robert tryon   famously used  cattell beginning   trait theory classification  personality psychology definition  notion   cluster   precisely defined   one   reasons     many clustering algorithms    common denominator  group  data objects however different researchers employ different cluster models      cluster models  different algorithms can  given  notion   cluster  found  different algorithms varies significantly   properties understanding  cluster models  key  understanding  differences   various algorithms typical cluster models include  clustering  essentially  set   clusters usually containing  objects   data set additionally  may specify  relationship   clusters     example  hierarchy  clusters embedded    clusterings can  roughly distinguished    also finer distinctions possible  example algorithms clustering algorithms can  categorized based   cluster model  listed   following overview will  list   prominent examples  clustering algorithms    possibly  published clustering algorithms   provide models   clusters  can thus  easily  categorized  overview  algorithms explained   can  found   list  statistics algorithms    objectively correct clustering algorithm     noted clustering    eye   beholder   appropriate clustering algorithm   particular problem often needs   chosen experimentally unless    mathematical reason  prefer one cluster model  another    noted   algorithm   designed  one kind  model   chance   data set  contains  radically different kind  model  example kmeans  find nonconvex clusters connectivitybased clustering hierarchical clustering connectivity based clustering also known  hierarchical clustering  based   core idea  objects   related  nearby objects   objects farther away  algorithms connect objects  form clusters based   distance  cluster can  described largely   maximum distance needed  connect parts   cluster  different distances different clusters will form  can  represented using  dendrogram  explains   common name hierarchical clustering comes   algorithms   provide  single partitioning   data set  instead provide  extensive hierarchy  clusters  merge     certain distances   dendrogram  yaxis marks  distance    clusters merge   objects  placed along  xaxis    clusters dont mix connectivity based clustering   whole family  methods  differ   way distances  computed apart   usual choice  distance functions  user also needs  decide   linkage criterion since  cluster consists  multiple objects   multiple candidates  compute  distance   use popular choices  known  singlelinkage clustering  minimum  object distances complete linkage clustering  maximum  object distances  upgma unweighted pair group method  arithmetic mean also known  average linkage clustering furthermore hierarchical clustering can  agglomerative starting  single elements  aggregating   clusters  divisive starting   complete data set  dividing   partitions  methods will  produce  unique partitioning   data set   hierarchy    user still needs  choose appropriate clusters     robust towards outliers  will either show   additional clusters  even cause  clusters  merge known  chaining phenomenon  particular  singlelinkage clustering   general case  complexity  formula  agglomerative clustering  formula  divisive clustering  makes   slow  large data sets   special cases optimal efficient methods  complexity formula  known slink  singlelinkage  clink  completelinkage clustering   data mining community  methods  recognized   theoretical foundation  cluster analysis  often considered obsolete   however provide inspiration  many later methods   density based clustering centroidbased clustering  centroidbased clustering clusters  represented   central vector  may  necessarily   member   data set   number  clusters  fixed  k kmeans clustering gives  formal definition   optimization problem find  formula cluster centers  assign  objects   nearest cluster center    squared distances   cluster  minimized  optimization problem   known   nphard  thus  common approach   search   approximate solutions  particularly well known approximative method  lloyds algorithm often actually referred   kmeans algorithm   however  find  local optimum   commonly run multiple times  different random initializations variations  kmeans often include  optimizations  choosing  best  multiple runs  also restricting  centroids  members   data set kmedoids choosing medians kmedians clustering choosing  initial centers less randomly kmeans  allowing  fuzzy cluster assignment fuzzy cmeans  kmeanstype algorithms require  number  clusters formula   specified  advance   considered   one   biggest drawbacks   algorithms furthermore  algorithms prefer clusters  approximately similar size   will always assign  object   nearest centroid  often leads  incorrectly cut borders    clusters    surprising   algorithm optimized cluster centers  cluster borders kmeans   number  interesting theoretical properties first  partitions  data space   structure known   voronoi diagram second   conceptually close  nearest neighbor classification     popular  machine learning third  can  seen   variation  model based classification  lloyds algorithm   variation   expectationmaximization algorithm   model discussed  distributionbased clustering  clustering model  closely related  statistics  based  distribution models clusters can  easily  defined  objects belonging  likely    distribution  convenient property   approach    closely resembles  way artificial data sets  generated  sampling random objects   distribution   theoretical foundation   methods  excellent  suffer  one key problem known  overfitting unless constraints  put   model complexity   complex model will usually  able  explain  data better  makes choosing  appropriate model complexity inherently difficult one prominent method  known  gaussian mixture models using  expectationmaximization algorithm   data set  usually modelled   fixed  avoid overfitting number  gaussian distributions   initialized randomly  whose parameters  iteratively optimized  fit better   data set  will converge   local optimum  multiple runs may produce different results  order  obtain  hard clustering objects  often  assigned   gaussian distribution   likely belong   soft clusterings    necessary distributionbased clustering produces complex models  clusters  can capture correlation  dependence  attributes however  algorithms put  extra burden   user  many real data sets  may   concisely defined mathematical model eg assuming gaussian distributions   rather strong assumption   data densitybased clustering  densitybased clustering clusters  defined  areas  higher density   remainder   data set objects   sparse areas   required  separate clusters  usually considered   noise  border points   popular density based clustering method  dbscan  contrast  many newer methods  features  welldefined cluster model called densityreachability similar  linkage based clustering   based  connecting points within certain distance thresholds however   connects points  satisfy  density criterion   original variant defined   minimum number   objects within  radius  cluster consists   densityconnected objects  can form  cluster   arbitrary shape  contrast  many  methods plus  objects   within  objects range another interesting property  dbscan    complexity  fairly low  requires  linear number  range queries   database    will discover essentially   results   deterministic  core  noise points    border points   run therefore    need  run  multiple times optics   generalization  dbscan  removes  need  choose  appropriate value   range parameter formula  produces  hierarchical result related    linkage clustering deliclu densitylinkclustering combines ideas  singlelinkage clustering  optics eliminating  formula parameter entirely  offering performance improvements  optics  using  rtree index  key drawback  dbscan  optics    expect  kind  density drop  detect cluster borders moreover   detect intrinsic cluster structures   prevalent   majority  real life data  variation  dbscan endbscan efficiently detects  kinds  structures  data sets   example overlapping gaussian distributions  common use case  artificial data  cluster borders produced   algorithms will often look arbitrary   cluster density decreases continuously   data set consisting  mixtures  gaussians  algorithms  nearly always outperformed  methods   em clustering   able  precisely model  kind  data meanshift   clustering approach   object  moved   densest area   vicinity based  kernel density estimation eventually objects converge  local maxima  density similar  kmeans clustering  density attractors can serve  representatives   data set  meanshift can detect arbitraryshaped clusters similar  dbscan due   expensive iterative procedure  density estimation meanshift  usually slower  dbscan  kmeans recent developments  recent years considerable effort   put  improving  performance  existing algorithms among   clarans ng  han  birch zhang et al   recent need  process larger  larger data sets also known  big data  willingness  trade semantic meaning   generated clusters  performance   increasing  led   development  preclustering methods   canopy clustering  can process huge data sets efficiently   resulting clusters  merely  rough prepartitioning   data set   analyze  partitions  existing slower methods   kmeans clustering various  approaches  clustering   tried   seed based clustering  highdimensional data many   existing methods fail due   curse  dimensionality  renders particular distance functions problematic  highdimensional spaces  led  new clustering algorithms  highdimensional data  focus  subspace clustering    attributes  used  cluster models include  relevant attributes   cluster  correlation clustering  also looks  arbitrary rotated correlated subspace clusters  can  modeled  giving  correlation   attributes examples   clustering algorithms  clique  subclu ideas  densitybased clustering methods  particular  dbscanoptics family  algorithms   adopted  subspace clustering hisc hierarchical subspace clustering  dish  correlation clustering hico hierarchical correlation clustering c using correlation connectivity  eric exploring hierarchical densitybased correlation clusters several different clustering systems based  mutual information   proposed one  marina meils variation  information metric another provides hierarchical clustering using genetic algorithms  wide range  different fitfunctions can  optimized including mutual information also message passing algorithms  recent development  computer science  statistical physics  led   creation  new types  clustering algorithms evaluation  assessment evaluation  clustering results sometimes  referred   cluster validation    several suggestions   measure  similarity  two clusterings   measure can  used  compare  well different data clustering algorithms perform   set  data  measures  usually tied   type  criterion  considered  assessing  quality   clustering method internal evaluation   clustering result  evaluated based   data   clustered    called internal evaluation  methods usually assign  best score   algorithm  produces clusters  high similarity within  cluster  low similarity  clusters one drawback  using internal criteria  cluster evaluation   high scores   internal measure   necessarily result  effective information retrieval applications additionally  evaluation  biased towards algorithms  use   cluster model  example kmeans clustering naturally optimizes object distances   distancebased internal criterion will likely overrate  resulting clustering therefore  internal evaluation measures  best suited  get  insight  situations  one algorithm performs better  another   shall  imply  one algorithm produces  valid results  another validity  measured    index depends   claim   kind  structure exists   data set  algorithm designed   kind  models   chance   data set contains  radically different set  models    evaluation measures  radically different criterion  example kmeans clustering can  find convex clusters  many evaluation indexes assume convex clusters   data set  nonconvex clusters neither  use  kmeans    evaluation criterion  assumes convexity  sound  following methods can  used  assess  quality  clustering algorithms based  internal criterion external evaluation  external evaluation clustering results  evaluated based  data    used  clustering   known class labels  external benchmarks  benchmarks consist   set  preclassified items   sets  often created  expert humans thus  benchmark sets can  thought    gold standard  evaluation  types  evaluation methods measure  close  clustering    predetermined benchmark classes however   recently  discussed whether   adequate  real data    synthetic data sets   factual ground truth since classes can contain internal structure  attributes present may  allow separation  clusters   classes may contain anomalies additionally   knowledge discovery point  view  reproduction  known knowledge may  necessarily   intended result   special scenario  constrained clustering  meta information   class labels  used already   clustering process  holdout  information  evaluation purposes  nontrivial  number  measures  adapted  variants used  evaluate classification tasks  place  counting  number  times  class  correctly assigned   single data point known  true positives  pair counting metrics assess whether  pair  data points   truly    cluster  predicted      cluster    measures  quality   cluster algorithm using external criterion include cluster tendency  measure cluster tendency   measure   degree clusters exist   data   clustered  may  performed   initial test  attempting clustering one way      compare  data  random data  average random data    clusters\r\n"}
{"index":{"_id":50}}
{"conceptLabelTag":"linear discriminant analysis","conceptLabel":"linear discriminant analysis","conceptDescription":"linear discriminant analysis linear discriminant analysis lda   generalization  fishers linear discriminant  method used  statistics pattern recognition  machine learning  find  linear combination  features  characterizes  separates two   classes  objects  events  resulting combination may  used   linear classifier   commonly  dimensionality reduction  later classification lda  closely related  analysis  variance anova  regression analysis  also attempt  express one dependent variable   linear combination   features  measurements however anova uses categorical independent variables   continuous dependent variable whereas discriminant analysis  continuous independent variables   categorical dependent variable ie  class label logistic regression  probit regression   similar  lda  anova    also explain  categorical variable   values  continuous independent variables   methods  preferable  applications     reasonable  assume   independent variables  normally distributed    fundamental assumption   lda method lda  also closely related  principal component analysis pca  factor analysis     look  linear combinations  variables  best explain  data lda explicitly attempts  model  difference   classes  data pca    hand   take  account  difference  class  factor analysis builds  feature combinations based  differences rather  similarities discriminant analysis  also different  factor analysis       interdependence technique  distinction  independent variables  dependent variables also called criterion variables must  made lda works   measurements made  independent variables   observation  continuous quantities  dealing  categorical independent variables  equivalent technique  discriminant correspondence analysis lda  two classes consider  set  observations formula also called features attributes variables  measurements   sample   object  event  known class formula  set  samples  called  training set  classification problem    find  good predictor   class formula   sample    distribution  necessarily   training set given   observation formula lda approaches  problem  assuming   conditional probability density functions formula  formula   normally distributed  mean  covariance parameters formula  formula respectively   assumption  bayes optimal solution   predict points     second class   log   likelihood ratios    threshold t   without   assumptions  resulting classifier  referred   qda quadratic discriminant analysis lda instead makes  additional simplifying homoscedasticity assumption ie   class covariances  identical  formula    covariances  full rank   case several terms cancel    decision criterion becomes  threshold   dot product   threshold constant c   means   criterion   input formula    class formula  purely  function   linear combination   known observations   often useful  see  conclusion  geometrical terms  criterion   input formula    class formula  purely  function  projection  multidimensionalspace point formula onto vector formula thus   consider  direction   words  observation belongs  formula  corresponding formula  located   certain side   hyperplane perpendicular  formula  location   plane  defined   threshold c canonical discriminant analysis  k classes canonical discriminant analysis cda finds axes k canonical coordinates k   number  classes  best separate  categories  linear functions  uncorrelated  define  effect  optimal k space   ndimensional cloud  data  best separates  projections   space   k groups see multiclass lda  details  fishers linear discriminant  terms fishers linear discriminant  lda  often used interchangeably although fishers original article actually describes  slightly different discriminant    make    assumptions  lda   normally distributed classes  equal class covariances suppose two classes  observations  means formula  covariances formula   linear combination  features formula will  means formula  variances formula  formula fisher defined  separation   two distributions    ratio   variance   classes   variance within  classes  measure    sense  measure   signaltonoise ratio   class labelling  can  shown   maximum separation occurs    assumptions  lda  satisfied   equation  equivalent  lda  sure  note   vector formula   normal   discriminant hyperplane   example   two dimensional problem  line  best divides  two groups  perpendicular  formula generally  data points   discriminated  projected onto formula   threshold  best separates  data  chosen  analysis   onedimensional distribution    general rule   threshold however  projections  points   classes exhibit approximately   distributions  good choice    hyperplane  projections   two means formula  formula   case  parameter c  threshold condition formula can  found explicitly otsus method  related  fishers linear discriminant   created  binarize  histogram  pixels   grayscale image  optimally picking  blackwhite threshold  minimizes intraclass variance  maximizes interclass variance withinbetween grayscales assigned  black  white pixel classes multiclass lda   case      two classes  analysis used   derivation   fisher discriminant can  extended  find  subspace  appears  contain    class variability  generalization  due  c r rao suppose    c classes   mean formula    covariance formula   scatter  class variability may  defined   sample covariance   class means  formula   mean   class means  class separation   direction formula   case will  given   means   formula   eigenvector  formula  separation will  equal   corresponding eigenvalue  formula  diagonalizable  variability  features will  contained   subspace spanned   eigenvectors corresponding   c largest eigenvalues since formula   rank c    eigenvectors  primarily used  feature reduction   pca  eigenvectors corresponding   smaller eigenvalues will tend    sensitive   exact choice  training data    often necessary  use regularisation  described   next section  classification  required instead  dimension reduction    number  alternative techniques available  instance  classes may  partitioned   standard fisher discriminant  lda used  classify  partition  common example    one   rest   points  one class  put  one group  everything else      lda applied  will result  c classifiers whose results  combined another common method  pairwise classification   new classifier  created   pair  classes giving cc classifiers  total   individual classifiers combined  produce  final classification incremental lda  typical implementation   lda technique requires    samples  available  advance however   situations   entire data set   available   input data  observed   stream   case   desirable   lda feature extraction    ability  update  computed lda features  observing  new samples without running  algorithm   whole data set  example  many realtime applications   mobile robotics  online face recognition   important  update  extracted lda features  soon  new observations  available  lda feature extraction technique  can update  lda features  simply observing new samples   incremental lda algorithm   idea   extensively studied   last two decades catterjee  roychowdhury proposed  incremental selforganized lda algorithm  updating  lda features   work demir  ozmehmet proposed online local learning algorithms  updating lda features incrementally using errorcorrecting   hebbian learning rules later aliyari et al derived fast incremental algorithms  update  lda features  observing  new samples practical use  practice  class means  covariances   known  can however  estimated   training set either  maximum likelihood estimate   maximum  posteriori estimate may  used  place   exact value    equations although  estimates   covariance may  considered optimal   sense    mean   resulting discriminant obtained  substituting  values  optimal   sense even   assumption  normally distributed classes  correct another complication  applying lda  fishers discriminant  real data occurs   number  measurements   sample ie  dimensionality   data vector exceeds  number  samples   class   case  covariance estimates    full rank     inverted    number  ways  deal   one   use  pseudo inverse instead   usual matrix inverse    formulae however better numeric stability may  achieved  first projecting  problem onto  subspace spanned  formula another strategy  deal  small sample size   use  shrinkage estimator   covariance matrix  can  expressed mathematically   formula   identity matrix  formula   shrinkage intensity  regularisation parameter  leads   framework  regularized discriminant analysis  shrinkage discriminant analysis also  many practical cases linear discriminants   suitable lda  fishers discriminant can  extended  use  nonlinear classification via  kernel trick   original observations  effectively mapped   higher dimensional nonlinear space linear classification   nonlinear space   equivalent  nonlinear classification   original space   commonly used example     kernel fisher discriminant lda can  generalized  multiple discriminant analysis  c becomes  categorical variable  n possible states instead   two analogously   classconditional densities formula  normal  shared covariances  sufficient statistic  formula   values  n projections    subspace spanned   n means affine projected   inverse covariance matrix  projections can  found  solving  generalized eigenvalue problem   numerator   covariance matrix formed  treating  means   samples   denominator   shared covariance matrix see multiclass lda   details applications  addition   examples given  lda  applied  positioning  product management bankruptcy prediction  bankruptcy prediction based  accounting ratios   financial variables linear discriminant analysis   first statistical method applied  systematically explain  firms entered bankruptcy vs survived despite limitations including known nonconformance  accounting ratios   normal distribution assumptions  lda edward altmans model  still  leading model  practical applications face recognition  computerised face recognition  face  represented   large number  pixel values linear discriminant analysis  primarily used   reduce  number  features    manageable number  classification    new dimensions   linear combination  pixel values  form  template  linear combinations obtained using fishers linear discriminant  called fisher faces   obtained using  related principal component analysis  called eigenfaces marketing  marketing discriminant analysis   often used  determine  factors  distinguish different types  customers andor products   basis  surveys   forms  collected data logistic regression   methods  now  commonly used  use  discriminant analysis  marketing can  described   following steps biomedical studies  main application  discriminant analysis  medicine   assessment  severity state   patient  prognosis  disease outcome  example  retrospective analysis patients  divided  groups according  severity  disease mild moderate  severe form  results  clinical  laboratory analyses  studied  order  reveal variables   statistically different  studied groups using  variables discriminant functions  built  help  objectively classify disease   future patient  mild moderate  severe form  biology similar principles  used  order  classify  define groups  different biological objects  example  define phage types  salmonella enteritidis based  fourier transform infrared spectra  detect animal source  escherichia coli studying  virulence factors etc earth science  method can  used  separate  alteration zones  example  different data  various zones  available discriminate analysis can find  pattern within  data  classify  effectively\r\n"}
{"index":{"_id":51}}
{"conceptLabelTag":"committee machine","conceptLabel":"committee machine","conceptDescription":"committee machine  committee machine   type  artificial neural network using  divide  conquer strategy    responses  multiple neural networks experts  combined   single response  combined response   committee machine  supposed   superior     constituent experts compare  ensembles  classifiers types static structures   class  committee machines  responses  several predictors experts  combined  means   mechanism    involve  input signal hence  designation static  category includes  following methods  ensemble averaging outputs  different predictors  linearly combined  produce  overall output  boosting  weak algorithm  converted  one  achieves arbitrarily high accuracy dynamic structures   second class  committee machines  input signal  directly involved  actuating  mechanism  integrates  outputs   individual experts   overall output hence  designation dynamic   two kinds  dynamic structures  mixture  experts  individual responses   experts  nonlinearly combined  means   single gating network  hierarchical mixture  experts  individual responses   individual experts  nonlinearly combined  means  several gating networks arranged   hierarchical fashion\r\n"}
{"index":{"_id":52}}
{"conceptLabelTag":"relevance vector machine","conceptLabel":"relevance vector machine","conceptDescription":"relevance vector machine  mathematics  relevance vector machine rvm   machine learning technique  uses bayesian inference  obtain parsimonious solutions  regression  probabilistic classification  rvm   identical functional form   support vector machine  provides probabilistic classification   actually equivalent   gaussian process model  covariance function  formula   kernel function usually gaussian formula   variances   prior   weight vector formula  formula   input vectors   training set compared    support vector machines svm  bayesian formulation   rvm avoids  set   parameters   svm  usually require crossvalidationbased postoptimizations however rvms use  expectation maximization emlike learning method   therefore  risk  local minima   unlike  standard sequential minimal optimization smobased algorithms employed  svms   guaranteed  find  global optimum   convex problem  relevance vector machine  patented   united states  microsoft\r\n"}
{"index":{"_id":53}}
{"conceptLabelTag":"activation function","conceptLabel":"activation function","conceptDescription":"activation function  computational networks  activation function   node defines  output   node given  input  set  inputs  standard computer chip circuit can  seen   digital network  activation functions  can     depending  input   similar   behavior   linear perceptron  neural networks however    nonlinear activation function  allows  networks  compute nontrivial problems using   small number  nodes  artificial neural networks  function  also called transfer function    confused   linear systems transfer function functions  biologically inspired neural networks  activation function  usually  abstraction representing  rate  action potential firing   cell   simplest form  function  binarythat  either  neuron  firing    function looks like formula  formula   heaviside step function   case  large number  neurons must  used  computation beyond linear separation  categories  line  positive slope may also  used  reflect  increase  firing rate  occurs  input current increases  function      form formula  formula   slope  activation function  linear  therefore    problems   binary function  addition networks constructed using  model  unstable convergence  neuron inputs along favored paths tend  increase without bound   function   normalizable  problems mentioned  can  handled  using  normalizable sigmoid activation function one realistic model stays  zero  input current  received   point  firing frequency increases quickly  first  gradually approaches  asymptote  firing rate mathematically  looks like formula   hyperbolic tangent function can also  replaced   sigmoid function  behavior  realistically reflected   neuron  neurons  physically fire faster   certain rate  model runs  problems however  computational networks     differentiable  requirement  order  calculate backpropagation  final model    used  multilayer perceptrons   sigmoidal activation function   form   hyperbolic tangent two forms   function  commonly used formula whose range  normalized    formula  vertically translated  normalize    latter model  often considered  biologically realistic   runs  theoretical  experimental difficulties  certain types  computational problems alternative structures  special class  activation functions known  radial basis functions rbfs  used  rbf networks   extremely efficient  universal function approximators  activation functions can take many forms    usually found  one  three functions  formula   vector representing  function center  formula  formula  parameters affecting  spread   radius support vector machines svms can effectively utilize  class  activation functions  includes  sigmoids  rbfs   case  input  transformed  reflect  decision boundary hyperplane based    training inputs called support vectors formula  activation function   hidden layer   machines  referred    inner product kernel formula  support vectors  represented   centers  rbfs   kernel equal   activation function   take  unique form   perceptron   formula  formula must satisfy certain conditions  convergence  machines can also accept arbitraryorder polynomial activation functions  activation function  types comparison  activation functions  desirable properties   activation function include  following table compares  properties  several activation functions   functions  one fold   previous layer  layers  following table lists activation functions    functions   single fold   previous layer  layers\r\n"}
{"index":{"_id":54}}
{"conceptLabelTag":"hinge loss","conceptLabel":"hinge loss","conceptDescription":"hinge loss  machine learning  hinge loss   loss function used  training classifiers  hinge loss  used  maximummargin classification  notably  support vector machines svms   intended output   classifier score  hinge loss   prediction  defined  note     raw output   classifiers decision function   predicted class label  instance  linear svms formula  formula   parameters   hyperplane  formula   point  classify  can  seen       sign meaning predicts  right class  formula  hinge loss formula     opposite sign formula increases linearly  onesided error extensions  svms  commonly extended  multiclass classification   onevsall  onevsone fashion  exists  true multiclass version   hinge loss due  crammer  singer defined   linear classifier   structured prediction  hinge loss can   extended  structured output spaces structured svms  margin rescaling use  following variant  denotes  svms parameters  joint feature function   hamming loss optimization  hinge loss   convex function  many   usual convex optimizers used  machine learning can work      differentiable    subgradient  respect  model parameters   linear svm  score function formula   given  however since  derivative   hinge loss  formula  nondeterministic smoothed versions may  preferred  optimization   rennie  srebros   quadratically smoothed suggested  zhang  modified huber loss   special case   loss function  formula\r\n"}
{"index":{"_id":55}}
{"conceptLabelTag":"topic model","conceptLabel":"topic model","conceptDescription":"topic model  machine learning  natural language processing  topic model   type  statistical model  discovering  abstract topics  occur   collection  documents topic modeling   frequently used textmining tool  discovery  hidden semantic structures   text body intuitively given   document    particular topic one  expect particular words  appear   document   less frequently dog  bone will appear  often  documents  dogs cat  meow will appear  documents  cats     will appear equally    document typically concerns multiple topics  different proportions thus   document    cats   dogs   probably   times  dog words  cat words  topics produced  topic modeling techniques  clusters  similar words  topic model captures  intuition   mathematical framework  allows examining  set  documents  discovering based   statistics   words     topics might     documents balance  topics  topic models  also referred   probabilistic topic models  refers  statistic algorithms  discovering  latent semantic structures   extensive text body   age  information  amount   written material  encounter  day  simply beyond  processing capacity topic models can help  organize  offer insights  us  understand large collections  unstructured text bodies originally developed   textmining tool topic models   used  detect instructive structures  data   genetic information images  networks  also  applications   fields   bioinformatics history  early topic model  described  papadimitriou raghavan tamaki  vempala  another one called probabilistic latent semantic indexing plsi  created  thomas hofmann  latent dirichlet allocation lda perhaps   common topic model currently  use   generalization  plsi developed  david blei andrew ng  michael  jordan  allowing documents    mixture  topics  topic models  generally extensions  lda   pachinko allocation  improves  lda  modeling correlations  topics  addition   word correlations  constitute topics case studies templetons survey  work  topic modeling   humanities grouped previous work  synchronic  diachronic approaches  synchronic approaches identify topics   certain time  example jockers used topic modelling  classify bloggers writing   day  digital humanities  identify  topics  wrote    day meeks modeled texts   humanities computingdigital humanities genre  identify selfdefinitions  scholars working  digital humanities  visualize networks  researchers  topics drouin examined proust  identify topics  show    graphical network diachronic approaches include block  newmans determination  temporal dynamics  topics   pennsylvania gazette  grifths steyvers use topic modeling  abstract   journal pnas  identify topics  rose  fell  popularity   nelson   analyzing change  topics  time   richmond timesdispatch  understand social  political changes  continuities  richmond   american civil war yang torget  mihalcea applied topic modeling methods  newspapers  blevins   topic modeling martha ballards diary  identify thematic trends across  year diary mimno used topic modelling  journals  classical philology  archaeology spanning years  look   topics   journals change  time    journals become  different  similar  time algorithms  practice researchers attempt  fit appropriate model parameters   data corpus using one  several heuristics  maximum likelihood fit  recent survey  blei describes  suite  algorithms several groups  researchers starting  papadimitriou et al  attempted  design algorithms  probable guarantees assuming   data  actually generated   model  question  try  design algorithms  probably find  model   used  create  data techniques used  include singular value decomposition svd   method  moments   algorithm based upon nonnegative matrix factorization nmf  introduced  also generalizes  topic models  correlations among topics\r\n"}
{"index":{"_id":56}}
{"conceptLabelTag":"information retrieval","conceptLabel":"information retrieval","conceptDescription":"information retrieval information retrieval ir   activity  obtaining information resources relevant   information need   collection  information resources searches can  based  fulltext   contentbased indexing automated information retrieval systems  used  reduce    called information overload many universities  public libraries use ir systems  provide access  books journals   documents web search engines    visible ir applications overview  information retrieval process begins   user enters  query   system queries  formal statements  information needs  example search strings  web search engines  information retrieval  query   uniquely identify  single object   collection instead several objects may match  query perhaps  different degrees  relevancy  object   entity   represented  information   content collection  database user queries  matched   database information however  opposed  classical sql queries   database  information retrieval  results returned may  may  match  query  results  typically ranked  ranking  results   key difference  information retrieval searching compared  database searching depending   application  data objects may   example text documents images audio mind maps  videos often  documents    kept  stored directly   ir system   instead represented   system  document surrogates  metadata  ir systems compute  numeric score   well  object   database matches  query  rank  objects according   value  top ranking objects   shown   user  process may   iterated   user wishes  refine  query history  idea  using computers  search  relevant pieces  information  popularized   article   may think  vannevar bush    appear  bush  inspired  patents   statistical machine filed  emanuel goldberg   s  s  searched  documents stored  film  first description   computer searching  information  described  holmstrom  detailing  early mention   univac computer automated information retrieval systems  introduced   s one even featured   romantic comedy desk set   s  first large information retrieval research group  formed  gerard salton  cornell   s several different retrieval techniques   shown  perform well  small text corpora    cranfield collection several thousand documents largescale retrieval systems    lockheed dialog system came  use early   s   us department  defense along   national institute  standards  technology nist cosponsored  text retrieval conference trec  part   tipster text program  aim     look   information retrieval community  supplying  infrastructure   needed  evaluation  text retrieval methodologies    large text collection  catalyzed research  methods  scale  huge corpora  introduction  web search engines  boosted  need   large scale retrieval systems even  model types  effectively retrieving relevant documents  ir strategies  documents  typically transformed   suitable representation  retrieval strategy incorporates  specific model   document representation purposes  picture   right illustrates  relationship   common models   picture  models  categorized according  two dimensions  mathematical basis   properties   model performance  correctness measures  evaluation   information retrieval system   process  assessing  well  system meets  information needs   users traditional evaluation metrics designed  boolean retrieval  topk retrieval include precision  recall many  measures  evaluating  performance  information retrieval systems  also  proposed  general measurement considers  collection  documents   searched   search query  common measures described  assume  ground truth notion  relevancy every document  known   either relevant  nonrelevant   particular query  practice queries may  illposed   may  different shades  relevancy virtually  modern evaluation metrics eg mean average precision discounted cumulative gain  designed  ranked retrieval without  explicit rank cutoff taking  account  relative order   documents retrieved   search engines  giving  weight  documents returned  higher ranks  mathematical symbols used   formulas  mean precision precision   fraction   documents retrieved   relevant   users information need  binary classification precision  analogous  positive predictive value precision takes  retrieved documents  account  can also  evaluated   given cutoff rank considering   topmost results returned   system  measure  called precision  n  pn note   meaning  usage  precision   field  information retrieval differs   definition  accuracy  precision within  branches  science  statistics recall recall   fraction   documents   relevant   query   successfully retrieved  binary classification recall  often called sensitivity   can  looked    probability   relevant document  retrieved   query   trivial  achieve recall   returning  documents  response   query therefore recall alone   enough  one needs  measure  number  nonrelevant documents also  example  computing  precision fallout  proportion  nonrelevant documents   retrieved    nonrelevant documents available  binary classification fallout  closely related  specificity   equal  formula  can  looked    probability   nonrelevant document  retrieved   query   trivial  achieve fallout   returning zero documents  response   query fscore fmeasure  weighted harmonic mean  precision  recall  traditional fmeasure  balanced fscore    also known   formula measure  recall  precision  evenly weighted  general formula  nonnegative real formula  two  commonly used f measures   formula measure  weights recall twice  much  precision   formula measure  weights precision twice  much  recall  fmeasure  derived  van rijsbergen   formula measures  effectiveness  retrieval  respect   user  attaches formula times  much importance  recall  precision   based  van rijsbergens effectiveness measure formula  relationship  fmeasure can   better single metric  compared  precision  recall  precision  recall give different information  can complement    combined  one   excels     fmeasure will reflect  average precision precision  recall  singlevalue metrics based   whole list  documents returned   system  systems  return  ranked sequence  documents   desirable  also consider  order    returned documents  presented  computing  precision  recall  every position   ranked sequence  documents one can plot  precisionrecall curve plotting precision formula   function  recall formula average precision computes  average value  formula   interval  formula  formula    area   precisionrecall curve  integral   practice replaced   finite sum  every position   ranked sequence  documents  formula   rank   sequence  retrieved documents formula   number  retrieved documents formula   precision  cutoff formula   list  formula   change  recall  items formula  formula  finite sum  equivalent   formula   indicator function equaling   item  rank formula   relevant document zero otherwise note   average    relevant documents   relevant documents  retrieved get  precision score  zero  authors choose  interpolate  formula function  reduce  impact  wiggles   curve  example  pascal visual object classes challenge  benchmark  computer vision object detection computes average precision  averaging  precision   set  evenly spaced recall levels  formula   interpolated precision  takes  maximum precision   recalls greater  formula  alternative   derive  analytical formula function  assuming  particular parametric distribution   underlying decision values  example  binormal precisionrecall curve can  obtained  assuming decision values   classes  follow  gaussian distribution precision  k  modern webscale information retrieval recall   longer  meaningful metric  many queries  thousands  relevant documents   users will  interested  reading    precision  k documents pk  still  useful metric eg p  precision  corresponds   number  relevant results   first search results page  fails  take  account  positions   relevant documents among  top k another shortcoming     query  fewer relevant results  k even  perfect system will   score less    easier  score manually since   top k results need   examined  determine    relevant   rprecision rprecision requires knowing  documents   relevant   query  number  relevant documents formula  used   cutoff  calculation   varies  query  query  example    documents relevant  red   corpus r rprecision  red looks   top documents returned counts  number   relevant formula turns    relevancy fraction formula precision  equal  recall   rth position empirically  measure  often highly correlated  mean average precision mean average precision mean average precision   set  queries   mean   average precision scores   query  q   number  queries discounted cumulative gain dcg uses  graded relevance scale  documents   result set  evaluate  usefulness  gain   document based   position   result list  premise  dcg   highly relevant documents appearing lower   search result list   penalized   graded relevance value  reduced logarithmically proportional   position   result  dcg accumulated   particular rank position formula  defined  since result set may vary  size among different queries  systems  compare performances  normalised version  dcg uses  ideal dcg   end  sorts documents   result list  relevance producing  ideal dcg  position p formula  normalizes  score  ndcg values   queries can  averaged  obtain  measure   average performance   ranking algorithm note    perfect ranking algorithm  formula will      formula producing  ndcg   ndcg calculations   relative values   interval     crossquery comparable visualization visualizations  information retrieval performance include\r\n"}
{"index":{"_id":57}}
{"conceptLabelTag":"GSP Algorithm","conceptLabel":"GSP Algorithm","conceptDescription":"gsp algorithm gsp algorithm generalized sequential pattern algorithm   algorithm used  sequence mining  algorithms  solving sequence mining problems  mostly based    priori levelwise algorithm one way  use  levelwise paradigm   first discover   frequent items   levelwise fashion  simply means counting  occurrences   singleton elements   database   transactions  filtered  removing  nonfrequent items   end   step  transaction consists    frequent elements  originally contained  modified database becomes  input   gsp algorithm  process requires one pass   whole database gsp algorithm makes multiple database passes   first pass  single items sequences  counted   frequent items  set  candidate sequences  formed  another pass  made  identify  frequency  frequent sequences  used  generate  candidate sequences   process  repeated    frequent sequences  found   two main steps   algorithm algorithm f  set  frequent sequence   algorithm looks like  apriori algorithm one main difference  however  generation  candidate sets let us assume   two frequent sequences  items involved   sequences   b  ac respectively  candidate generation   usual apriori style  give  b c   itemset    present context  get  following sequences   result  joining   sequences  candidategeneration phase takes   account  gsp algorithm discovers frequent sequences allowing  time constraints   maximum gap  minimum gap among  sequence elements moreover  supports  notion   sliding window ie   time interval within  items  observed  belonging    event even   originate  different events see also sequence mining\r\n"}
{"index":{"_id":58}}
{"conceptLabelTag":"causality","conceptLabel":"causality","conceptDescription":"causality causality also referred   causation  cause  effect   agency  efficacy  connects one process  cause  another process  state  effect   first  understood   partly responsible   second   second  dependent   first  general  process  many causes   said   causal factors     lie   past  effect can  turn   cause  many  effects although retrocausality  sometimes referred   thought experiments  hypothetical analyses causality  generally accepted   temporally bound   causes always precede  dependent effects although   contexts   economics  may coincide  time see instrumental variable     dealt  econometrically causality   abstraction  indicates   world progresses  basic  concept     apt   explanation   concepts  progression   something   explained  others  basic  concept  like   agency  efficacy   reason  leap  intuition may  needed  grasp  accordingly causality  built   conceptual structure  ordinary language  aristotelian philosophy  word cause  also used  mean explanation  answer    question including aristotles material formal efficient  final causes   cause   explanans   explanandum   case failure  recognize  different kinds  cause   considered can lead  futile debate  aristotles four explanatory modes  one nearest   concerns   present article   efficient one  topic remains  staple  contemporary philosophy  studying  meaning  causality semantics traditionally appeal   chicken   egg causality dilemma ie  came first  chicken   egg   allocates  constituent elements  cause  effect  link   joins    concept metaphysics  nature  cause  effect   concern   subject known  metaphysics ontology  general metaphysical question  cause  effect   kind  entity can   cause   kind  entity can   effect one viewpoint   question   cause  effect   one    kind  entity  causality  asymmetric relation      say   make good sense grammatically  say either    cause  b  effect  b   cause    effect though  one   two can  actually true   view one opinion proposed   metaphysical principle  process philosophy   every cause  every effect  respectively  process event becoming  happening  example   tripping   step   cause   breaking  ankle  effect another view   causes  effects  states  affairs   exact natures   entities  less restrictively defined   process philosophy another viewpoint   question    classical one   cause   effect can   different kinds  entity  example  aristotles efficient causal explanation  action can   cause   enduring object   effect  example  generative actions   parents can  regarded   efficient cause  socrates   effect socrates  regarded   enduring object  philosophical tradition called  substance  distinct   action epistemology since causality   subtle metaphysical notion considerable effort  needed  establish knowledge    particular empirical circumstances geometrical significance causality   properties  antecedence  contiguity   topological   ingredients  spacetime geometry  developed  alfred robb  properties allow  derivation   notions  time  space max jammer writes  einstein postulate opens  way   straightforward construction   causal topology  minkowski space causal efficacy propagates  faster  light thus  notion  causality  metaphysically prior   notions  time  space necessary  sufficient causes causes  often distinguished  two types necessary  sufficient  third type  causation  requires neither necessity  sufficiency       contributes   effect  called  contributory cause j l mackie argues  usual talk  cause  fact refers  inus conditions insufficient  nonredundant parts   condition    unnecessary  sufficient   occurrence   effect  example   short circuit   cause   house burning  consider  collection  events  short circuit  proximity  flammable material   absence  firefighters together   unnecessary  sufficient   houses burning  since many  collections  events certainly   led   house burning   example shooting  house   flamethrower   presence  oxygen   forth within  collection  short circuit   insufficient since  short circuit      caused  fire  nonredundant   fire    happened without  everything else  equal part   condition    unnecessary  sufficient   occurrence   effect   short circuit   inus condition   occurrence   house burning  contrasted  conditionals conditional statements   statements  causality  important distinction   statements  causality require  antecedent  precede  coincide   consequent  time whereas conditional statements   require  temporal order confusion commonly arises since many different statements  english may  presented using   form  arguably   form  far  commonly used  make  statement  causality  two types  statements  distinct however  example    following statements  true  interpreting     material conditional  first  true since   antecedent   consequent  true  second  true  sentential logic  indeterminate  natural language regardless   consequent statement  follows   antecedent  false  ordinary indicative conditional  somewhat  structure   material conditional  instance although  first   closest neither   preceding two statements seems true   ordinary indicative reading   sentence intuitively seems   true even though    straightforward causal relation   hypothetical situation  shakespeares  writing macbeth  someone elses actually writing  another sort  conditional  counterfactual conditional   stronger connection  causality yet even counterfactual statements    examples  causality consider  following two statements   first case     correct  say     triangle caused    three sides since  relationship  triangularity  threesidedness    definition  property   three sides actually determines  state   triangle nonetheless even  interpreted counterfactually  first statement  true  early version  aristotles four cause theory  described  recognizing essential cause   version   theory   closed polygon  three sides  said    essential cause     triangle  use   word cause   course now far obsolete nevertheless   within  scope  ordinary language  say    essential   triangle    three sides  full grasp   concept  conditionals  important  understanding  literature  causality  everyday language loose conditional statements  often enough made  need   interpreted carefully questionable cause fallacies  questionable cause also known  causal fallacies noncausa pro causa latin  noncause  cause  false cause  informal fallacies   cause  incorrectly identified theories counterfactual theories subjunctive conditionals  familiar  ordinary language     form     case  b    case       case  b     case counterfactual conditionals  specifically subjunctive conditionals whose antecedents   fact false hence  name however  term used technically may apply  conditionals  true antecedents  well psychological research shows  peoples thoughts   causal relationships  events influences  judgments   plausibility  counterfactual alternatives  conversely  counterfactual thinking    situation   turned  differently changes  judgments   causal role  events  agents nonetheless  identification   cause   event   counterfactual thought    event   turned  differently   always coincide people distinguish  various sorts  causes eg strong  weak causes research   psychology  reasoning shows  people make different sorts  inferences  different sorts  causes   philosophical literature  suggestion  causation    defined  terms   counterfactual relation  made   th century scottish philosopher david hume hume remarks   may define  relation  cause  effect      first object     second never  existed  fullfledged analysis  causation  terms  counterfactual conditionals  came   th century  development   possible world semantics   evaluation  counterfactual conditionals   paper causation david lewis proposed   following definition   notion  causal dependence causation   defined   chain  causal dependence   c causes e      exists  sequence  events c d d d e    event   sequence depends   previous note   analysis   purport  explain   make causal judgements    reason  causation  rather  give  metaphysical account          causal relation   pair  events  correct  analysis   power  explain certain features  causation knowing  causation   matter  counterfactual dependence  may reflect   nature  counterfactual dependence  account   nature  causation  example   paper counterfactual dependence  times arrow lewis sought  account   timedirectedness  counterfactual dependence  terms   semantics   counterfactual conditional  correct  theory can serve  explain  fundamental part   experience     can  causally affect  future    past probabilistic causation interpreting causation   deterministic relation means    causes b   must always  followed  b   sense war   cause deaths   smoking cause cancer  emphysema   result many turn   notion  probabilistic causation informally   person   smoker probabilistically causes b  person  now  will  cancer   time   future   information   occurred increases  likelihood  bs occurrence formally pba pb  pba   conditional probability  b will occur given  information   occurred  pbis  probability  b will occur   knowledge whether      occur  intuitive condition   adequate   definition  probabilistic causation      general  thus  meeting  intuitive notion  cause  effect  example   denotes  event  person   smoker b denotes  event  person now   will  cancer   time   future  c denotes  event  person now   will  emphysema  time   future   following three relationships hold pba pb pca pc  pbc pb  last relationship states  knowing   person  emphysema increases  likelihood   will  cancer  reason      information   person  emphysema increases  likelihood   person   smoker thus indirectly increases  likelihood   person will  cancer however    want  conclude   emphysema causes cancer thus  need additional conditions   temporal relationship    b   rational explanation    mechanism  action   hard  quantify  last requirement  thus different authors prefer somewhat different definitions  publications  anderson  vastag laur a  duchessi gupta  kim lee et al cardenas voordijk  dewulf  shown  number  examples  tests  probabilistic causation assertions  applications  different fields causal calculus  experimental interventions  infeasible  illegal  derivation  cause effect relationship  observational studies must rest   qualitative theoretical assumptions  example  symptoms   cause diseases usually expressed   form  missing arrows  causal graphs   bayesian networks  path diagrams  theory underlying  derivations relies   distinction  conditional probabilities   formula  interventional probabilities   formula  former reads  probability  finding cancer   person known  smoke  started unforced   experimenter      unspecified time   past   latter reads  probability  finding cancer   person forced   experimenter  smoke   specified time   past  former   statistical notion  can  estimated  observation  negligible intervention   experimenter   latter   causal notion   estimated   experiment   important controlled randomized intervention   specifically characteristic  quantal phenomena  observations defined  incompatible variables always involve important intervention   experimenter  described quantitatively   heisenberg uncertainty principle  classical thermodynamics processes  initiated  interventions called thermodynamic operations   branches  science  example astronomy  experimenter can often observe  negligible intervention  theory  causal calculus permits one  infer interventional probabilities  conditional probabilities  causal bayesian networks  unmeasured variables one  practical result   theory   characterization  confounding variables namely  sufficient set  variables   adjusted   yield  correct causal effect  variables  interest  can  shown   sufficient set  estimating  causal effect  formula  formula   set  nondescendants  formula  formulaseparate formula  formula  removing  arrows emanating  formula  criterion called backdoor provides  mathematical definition  confounding  helps researchers identify accessible sets  variables worthy  measurement structure learning  derivations  causal calculus rely   structure   causal graph parts   causal structure can  certain assumptions  learned  statistical data  basic idea goes back  sewall wrights work  path analysis  recovery algorithm  developed  rebane  pearl  rests  wrights distinction   three possible types  causal substructures allowed   directed acyclic graph dag type  type represent   statistical dependencies ie formula  formula  independent given formula   therefore indistinguishable within purely crosssectional data type however can  uniquely identified since formula  formula  marginally independent    pairs  dependent thus   skeletons  graphs stripped  arrows   three triplets  identical  directionality   arrows  partially identifiable   distinction applies  formula  formula  common ancestors except  one must first condition   ancestors algorithms   developed  systematically determine  skeleton   underlying graph   orient  arrows whose directionality  dictated   conditional independencies observed alternative methods  structure learning search   many possible causal structures among  variables  remove ones   strongly incompatible   observed correlations  general  leaves  set  possible causal relations     tested  analyzing time series data  preferably designing appropriately controlled experiments  contrast  bayesian networks path analysis   generalization structural equation modeling serve better  estimate  known causal effect   test  causal model   generate causal hypotheses  nonexperimental data causal direction can often  inferred  information  time  available    according  many though   theories causes must precede  effects temporally  can  determined  statistical time series models  instance    statistical test based   idea  granger causality   direct experimental manipulation  use  temporal data can permit statistical tests   preexisting theory  causal direction  instance  degree  confidence   direction  nature  causality  much greater  supported  crosscorrelations arima models  crossspectral analysis using vector time series data   crosssectional data derivation theories nobel prize laureate herbert  simon  philosopher nicholas rescher claim   asymmetry   causal relation  unrelated   asymmetry   mode  implication  contraposes rather  causal relation    relation  values  variables   function  one variable  cause   another  effect  given  system  equations   set  variables appearing   equations  can introduce  asymmetric relation among individual equations  variables  corresponds perfectly   commonsense notion   causal ordering  system  equations must  certain properties  importantly   values  chosen arbitrarily  remaining values will  determined uniquely   path  serial discovery   perfectly causal  postulate  inherent serialization    system  equations may correctly capture causation   empirical fields including physics  economics manipulation theories  theorists  equated causality  manipulability   theories x causes y    case  one can change x  order  change y  coincides  commonsense notions  causations since often  ask causal questions  order  change  feature   world  instance   interested  knowing  causes  crime    might find ways  reducing   theories   criticized  two primary grounds first theorists complain   accounts  circular attempting  reduce causal claims  manipulation requires  manipulation   basic  causal interaction  describing manipulations  noncausal terms  provided  substantial difficulty  second criticism centers around concerns  anthropocentrism  seems  many people  causality   existing relationship   world   can harness   desires  causality  identified   manipulation   intuition  lost   sense  makes humans overly central  interactions   world  attempts  defend manipulability theories  recent accounts  dont claim  reduce causality  manipulation  accounts use manipulation   sign  feature  causation without claiming  manipulation   fundamental  causation process theories  theorists  interested  distinguishing  causal processes  noncausal processes russell salmon  theorists often want  distinguish   process   pseudoprocess   example  ball moving   air  process  contrasted   motion   shadow  pseudoprocess  former  causal  nature   latter   salmon claims  causal processes can  identified   ability  transmit  alteration  space  time  alteration   ball  mark   pen perhaps  carried     ball goes   air    hand  alteration   shadow insofar    possible will   transmitted   shadow   moves along  theorists claim   important concept  understanding causality   causal relationships  causal interactions  rather identifying causal processes  former notions can   defined  terms  causal processes fields science   scientific investigation  efficient causality  cause  effect   best conceived   temporally transient processes within  conceptual frame   scientific method  investigator sets  several distinct  contrasting temporally transient material processes    structure  experiments  records candidate material responses normally intending  determine causality   physical world  instance one may want  know whether  high intake  carrots causes humans  develop  bubonic plague  quantity  carrot intake   process   varied  occasion  occasion  occurrence  nonoccurrence  subsequent bubonic plague  recorded  establish causality  experiment must fulfill certain criteria  one example    mentioned     criteria  mentioned   example instances   hypothesized cause must  set   occur   time   hypothesized effect  relatively unlikely   absence   hypothesized cause  unlikelihood    established  empirical evidence  mere observation   correlation   nearly adequate  establish causality  nearly  cases establishment  causality relies  repetition  experiments  probabilistic reasoning hardly ever  causality established  firmly     less probable   often  convenient  establishment  causality   contrasting material states  affairs  fully comparable  differ   one variable factor perhaps measured   real number otherwise experiments  usually difficult  impossible  interpret   sciences    difficult  nearly impossible  set  material states  affairs  closely test hypotheses  causality  sciences can   sense  regarded  softer physics   useful   careful   use   word cause  physics properly speaking  hypothesized cause   hypothesized effect   temporally transient processes  example force   useful concept   explanation  acceleration  force      cause   needed  example  temporally transient process might  characterized   definite change  force   definite time   process can  regarded   cause causality   inherently implied  equations  motion  postulated   additional constraint  needs   satisfied ie  cause always precedes  effect  constraint  mathematical implications    kramerskronig relations causality  one    fundamental  essential notions  physics causal efficacy  propagate faster  light otherwise reference coordinate systems   constructed using  lorentz transform  special relativity    observer  see  effect precede  cause ie  postulate  causality   violated causal notions appear   context   flow  massenergy  example   commonplace  argue  causal efficacy can  propagated  waves   electromagnetic waves    propagate  faster  light wave packets  group velocity  phase velocity  waves  propagate causal efficacy    must travel  faster  light thus light waves often propagate causal efficacy  de broglie waves often  phase velocity faster  light  consequently   propagating causal efficacy causal notions  important  general relativity   extent   existence   arrow  time demands   universes semiriemannian manifold  orientable   future  past  globally definable quantities engineering  causal system   system  output  internal states  depends    current  previous input values  system    dependence  input values   future  addition  possible past  current input values  termed  acausal system   system  depends solely  future input values   anticausal system acausal filters  example can  exist  postprocessing filters   filters can extract future values   memory buffer   file biology medicine  epidemiology austin bradford hill built upon  work  hume  popper  suggested   paper  environment  disease association  causation  aspects   association   strength consistency specificity  temporality  considered  attempting  distinguish causal  noncausal associations   epidemiological situation see bradfordhill criteria    note however  temporality    necessary criterion among  aspects directed acyclic graphs dags  increasingly used  epidemiology  help enlighten causal thinking psychology psychologists take  empirical approach  causality investigating  people  nonhuman animals detect  infer causation  sensory information prior experience  innate knowledge attribution theory   theory concerning  people explain individual occurrences  causation attribution can  external assigning causality   outside agent  force claiming   outside thing motivated  event  internal assigning causality  factors within  person taking personal responsibility  accountability  ones actions  claiming   person  directly responsible   event taking causation one step   type  attribution  person provides influences  future behavior  intention behind  cause   effect can  covered   subject  action see also accident blame intent  responsibility whereas david hume argued  causes  inferred  noncausal observations immanuel kant claimed  people  innate assumptions  causes within psychology patricia cheng attempted  reconcile  humean  kantian views according   power pc theory people filter observations  events   basic belief  causes   power  generate  prevent  effects thereby inferring specific causeeffect relations  view  causation depends    consider    relevant events another way  view  statement lightning causes thunder   see  lightning  thunder  two perceptions    event viz  electric discharge   perceive first visually   aurally david sobel  alison gopnik   psychology department  uc berkeley designed  device known   blicket detector   turn    object  placed    research suggests  even young children will easily  swiftly learn   new causal power   object  spontaneously use  information  classifying  naming  object  researchers   anjan chatterjee   university  pennsylvania  jonathan fugelsang   university  waterloo  using neuroscience techniques  investigate  neural  psychological underpinnings  causal launching events   one object causes another object  move  temporal  spatial factors can  manipulated see causal reasoning psychology   information statistics  economics statistics  economics usually employ preexisting data  experimental data  infer causality  regression methods  body  statistical techniques involves substantial use  regression analysis typically  linear relationship    postulated   formula   ith observation   dependent variable hypothesized    caused variable formula  jk   ith observation   jth independent variable hypothesized    causative variable  formula   error term   ith observation containing  combined effects    causative variables  must  uncorrelated   included independent variables    reason  believe  none   formulas  caused  y  estimates   coefficients formula  obtained    hypothesis  formula  rejected   alternative hypothesis  formula  equivalently  formula causes y   rejected    hand    hypothesis  formula   rejected  equivalently  hypothesis   causal effect  formula  y   rejected   notion  causality  one  contributory causality  discussed    true value formula   change  formula will result   change  y unless   causative variables either included   regression  implicit   error term change    way   exactly offset  effect thus  change  formula   sufficient  change y likewise  change  formula   necessary  change y   change  y   caused  something implicit   error term     causative explanatory variable included   model   way  testing  causality requires belief     reverse causation   y  cause formula  belief can  established  one  several ways first  variable formula may   noneconomic variable  example  rainfall amount formula  hypothesized  affect  futures price y   agricultural commodity   impossible   fact  futures price affects rainfall amount provided  cloud seeding  never attempted second  instrumental variables technique may  employed  remove  reverse causation  introducing  role   variables instruments   known   unaffected   dependent variable third  principle  effects  precede causes can  invoked  including   right side   regression  variables  precede  time  dependent variable  principle  invoked  example  testing  granger causality    multivariate analog vector autoregression    control  lagged values   dependent variable  testing  causal effects  lagged independent variables regression analysis controls   relevant variables  including   regressors explanatory variables  helps  avoid false inferences  causality due   presence   third underlying variable  influences   potentially causative variable   potentially caused variable  effect   potentially caused variable  captured  directly including    regression   effect will   picked    indirect effect   potentially causative variable  interest metaphysics  deterministic worldview holds   history   universe can  exhaustively represented   progression  events following one   cause  effect  incompatibilist version   holds      thing   will compatibilism    hand holds  determinism  compatible   even necessary   will management  quality control  manufacturing   s kaoru ishikawa developed  cause  effect diagram known   ishikawa diagram  fishbone diagram  diagram categorizes causes     six main categories shown   categories   subdivided ishikawas method identifies causes  brainstorming sessions conducted among various groups involved   manufacturing process  groups can   labeled  categories   diagrams  use   diagrams  now spread beyond quality control    used   areas  management   design  engineering ishikawa diagrams   criticized  failing  make  distinction  necessary conditions  sufficient conditions  seems  ishikawa   even aware   distinction humanities history   discussion  history events  sometimes considered     way  agents  can  bring   historical events thus  combination  poor harvests  hardships   peasants high taxes lack  representation   people  kingly ineptitude  among  causes   french revolution    somewhat platonic  hegelian view  reifies causes  ontological entities  aristotelian terminology  use approximates   case   efficient cause  philosophers  history   arthur danto  claimed  explanations  history  elsewhere describe  simply  event something  happens   change like many practicing historians  treat causes  intersecting actions  sets  actions  bring  larger changes  dantos words  decide    elements  persist   change  rather simple  treating  individuals shift  attitude    considerably  complex  metaphysically challenging    interested    change  say  breakup  feudalism   emergence  nationalism much   historical debate  causes  focused   relationship  communicative   actions  singular  repeated ones   actions structures  action  group  institutional contexts  wider sets  conditions john gaddis  distinguished  exceptional  general causes following marc bloch   routine  distinctive links  causal relationships  accounting   happened  hiroshima  august  attach greater importance   fact  president truman ordered  dropping   atomic bomb    decision   army air force  carry   orders   also pointed   difference  immediate intermediate  distant causes   part christopher lloyd puts forward four general concepts  causation used  history  metaphysical idealist concept  asserts   phenomena   universe  products   emanations   omnipotent    final cause  empiricist  humean regularity concept   based   idea  causation   matter  constant conjunctions  events  functionalteleologicalconsequential concept   goaldirected   goals  causes   realist structurist  dispositional approach  sees relational structures  internal dispositions   causes  phenomena law according  law  jurisprudence legal cause must  demonstrated  hold  defendant liable   crime   tort ie  civil wrong   negligence  trespass  must  proven  causality   sufficient causal link relates  defendants actions   criminal event  damage  question causation  also  essential legal element  must  proven  qualify  remedy measures  international trade law theology note  concept  omnicausality  abrahamic theology    belief  god  set  motion  events   dawn  time    determiner   cause   things   therefore  attempt  rectify  apparent incompatibility  determinism   existence   omnipotent god history western philosophy aristotelian aristotle identified four kinds  answer  explanatory mode  various  questions  thought    given topic  four kinds  explanatory mode  important     right   result  traditional specialized philosophical peculiarities  language  translations  ancient greek latin  english  word cause  nowadays  specialized philosophical writings used  label aristotles four kinds  ordinary language   various meanings   word cause  commonest referring  efficient cause  topic   present article  aristotles four kinds  explanatory modes  one  efficient cause   cause  defined   leading paragraph   present article   three explanatory modes might  rendered material composition structure  dynamics   criterion  completion  word  aristotle used    present purpose  greek word   better translated  explanation   cause   words   often used  current english another translation  aristotle    meant  four becauses  four kinds  answer   questions aristotle assumed efficient causality  referring   basic fact  experience  explicable   reducible  anything  fundamental  basic   works  aristotle  four causes  listed   essential cause  logical ground  moving cause   final cause   listing  statement  essential cause   demonstration   indicated object conforms   definition   word  refers    statement  logical ground   argument     object statement  true    examples   idea   cause  general   context  aristotles usage   explanation  word efficient used  can also  translated  aristotle  moving  initiating efficient causation  connected  aristotelian physics  recognized  four elements earth air fire water  added  fifth element aether water  earth   intrinsic property gravitas  heaviness intrinsically fall toward whereas air  fire   intrinsic property levitas  lightness intrinsically rise away  earths centerthe motionless center   universein  straight line  accelerating   substances approach   natural place  air remained  earth however    escape earth  eventually achieving infinite speedan absurdityaristotle inferred   universe  finite  size  contains  invisible substance  held planet earth   atmosphere  sublunary sphere centered   universe  since celestial bodies exhibit perpetual unaccelerated motion orbiting planet earth  unchanging relations aristotle inferred   fifth element aither  fills space  composes celestial bodies intrinsically moves  perpetual circles   constant motion  two points  object traveling  straight line  point   b  back must stop  either point  returning    left    thing exhibits natural motion  canaccording  aristotelian metaphysicsexhibit enforced motion imparted   efficient cause  form  plants endows plants   processes nutrition  reproduction  form  animals adds locomotion   form  humankind adds reason atop   rock normally exhibits natural motionexplained   rocks material cause   composed   element earthbut  living thing can lift  rock  enforced motion diverting  rock   natural place  natural motion    kind  explanation aristotle identified  final cause specifying  purpose  criterion  completion  light   something   understood aristotle  explained aristotle  discerned two modes  causation proper prior causation  accidental chance causation  causes proper  accidental can  spoken  potential   actual particular  generic   language refers   effects  causes   generic effects  assigned  generic causes particular effects  particular causes  actual effects  operating causes averting infinite regress aristotle inferred  first moveran unmoved mover  first movers motion  must   caused    unmoved mover must  moved  toward  particular goal  desire middle ages  line  aristotelian cosmology thomas aquinas posed  hierarchy prioritizing aristotles four causes final efficient material formal aquinas sought  identify  first efficient causenow simply first causeas everyone  agree said aquinas  call  god later   middle ages many scholars conceded   first cause  god  explained  many earthly events occur within gods design  plan  thereby scholars sought dom  investigate  numerous secondary causes   middle ages  aristotelian philosophy  aquinas  word cause   broad meaning  meant answer    question  explanation  aristotelian scholars recognized four kinds   answers   end   middle ages  many philosophical usages  meaning   word cause narrowed  often lost  broad meaning   restricted  just one   four kinds  authors   niccol  machiavelli   field  political thinking  francis bacon concerning science  generally aristotles moving cause   focus   interest  widely used modern definition  causality   newly narrowed sense  assumed  david hume  undertook  epistemological  metaphysical investigation   notion  moving cause  denied   can ever perceive cause  effect except  developing  habit  custom  mind   come  associate two types  object  event always contiguous  occurring one     part iii section xv   book  treatise  human nature hume expanded    list  eight ways  judging whether two things might  cause  effect  first three   additionally   three connected criteria  come   experience     source     philosophical reasonings   two   physicist max born distinguished determination  causality   determination meant  actual events   linked  laws  nature  certainly reliable predictions  retrodictions can  made  sufficient present data       two kinds  causation   may  call nomic  generic causation  singular causation nomic causality means  cause  effect  linked    less certain  probabilistic general laws covering many possible  potential instances  may recognize    probabilized version  criterion  hume mentioned just   occasion  singular causation   particular occurrence   definite complex  events   physically linked  antecedence  contiguity   may  recognize  criteria   hume mentioned just  hindu philosophy vedic period ca bce literature  karmas eastern origins karma   belief held  sanathana dharma  major religions   persons actions cause certain effects   current life andor  future life positively  negatively  various philosophical schools darsanas provide different accounts   subject  doctrine  satkaryavada affirms   effect inheres   cause   way  effect  thus either  real  apparent modification   cause  doctrine  asatkaryavada affirms   effect   inhere   cause    new arising see nyaya   details   theory  causation   nyaya school  brahma samhita brahma describes krishna   prime cause   causes bhagavadgt identifies five causes   action knowing   can  perfected  body  individual soul  senses  efforts   supersoul according  monierwilliams   nyya causation theory  sutra ii   vaisheshika philosophy  casual nonexistence  effectual nonexistence   effectual nonexistence  casual nonexistence  cause precedes  effect   threads  cloth metaphors three causes  monierwilliams also proposed  aristotles   nyayas causality  considered conditional aggregates necessary  mans productive work buddhist philosophy  general  universal definition  pratityasamutpada  dependent origination  dependent arising  interdependent coarising   everything arises  dependence upon multiple causes  conditions nothing exists   singular independent entity  traditional example  buddhist texts   three sticks standing upright  leaning     supporting    one stick  taken away   two will fall   ground causality   chittamatrin buddhist school approach asangas c ce mindonly buddhist school asserts  objects cause consciousness   minds image  causes precede effects  must  different entities  subject  object  different   school    objects   entities external   perceiving consciousness  chittamatrin   yogachara svatantrika schools accept     objects external   observers causality  largely follows  nikayas approach  abhidharmakoakrik approach  vasubandhus abhidharma commentary text   sarvstivda school c ce   four intricate causal conditioning constructions   root cause immediate antecedent object support  predominance   six causes  instrumentality kraahetu deemed  primary factor  result production simultaneity  coexistence  connects phenomena  arise simultaneously homogeneity explaining  homogenous flow  evokes phenomena continuity association  operates   mental factors  explains  consciousness appears  assemblages  mental factors dominance  forms ones habitual cognitive  behaviorist dispositions  fruition referring  whatever   actively wholesome  unwholesome result  four conditions  six causes interact     explaining phenomenal experience  instance  conscious moment acts    homogenous cause  well   immediate antecedent consciousness condition rise   concomitants   subsequent moment  vaibhashika c ce   early buddhist school  favors direct object contact  accepts simultaneous cause  effects   based   consciousness example  says intentions  feelings  mutually accompanying mental factors  support   like poles  tripod  contrast simultaneous cause  effect rejectors say    effect already exists    effect   way   past present  future  accepted   basis  various buddhist schools causality view points\r\n"}
{"index":{"_id":59}}
{"conceptLabelTag":"factor analysis","conceptLabel":"factor analysis","conceptDescription":"factor analysis factor analysis   statistical method used  describe variability among observed correlated variables  terms   potentially lower number  unobserved variables called factors  example   possible  variations  six observed variables mainly reflect  variations  two unobserved underlying variables factor analysis searches   joint variations  response  unobserved latent variables  observed variables  modelled  linear combinations   potential factors plus error terms factor analysis aims  find independent latent variables followers  factor analytic methods believe   information gained   interdependencies  observed variables can  used later  reduce  set  variables   dataset factor analysis   used   significant degree  physics biology  chemistry   used  heavily  psychometrics personality theories marketing product management operations research users  factor analysis believe   helps  deal  data sets    large numbers  observed variables   thought  reflect  smaller number  underlyinglatent variables factor analysis  related  principal component analysis pca   two   identical    significant controversy   field  differences   two techniques see section  exploratory factor analysis versus principal components analysis  pca    basic version  exploratory factor analysis efa   developed   early days prior   advent  highspeed computers   point  view  exploratory analysis  eigenvalues  pca  inflated component loadings ie contaminated  error variance statistical model definition suppose    set  formula observable random variables formula  means formula suppose   unknown constants formula  formula unobserved random variables formulacalled common factors   influence   observed random variables  formula  formula  formula   formula  unobserved stochastic error terms  zero mean  finite variance  may       formula  matrix terms      formula observations   will   dimensions formula formula  formula  column  formula  formula denote values  one particular observation  matrix formula   vary across observations also  will impose  following assumptions  formula  solution    set  equations following  constraints  formula  defined   factors  formula   loading matrix suppose formula  note    conditions just imposed  formula     note    orthogonal matrix formula   set formula  formula  criteria   factors  factor loadings still hold hence  set  factors  factor loadings  unique    orthogonal transformation example suppose  psychologist   hypothesis    two kinds  intelligence verbal intelligence  mathematical intelligence neither    directly observed evidence   hypothesis  sought   examination scores    different academic fields  students   student  chosen randomly   large population   students scores  random variables  psychologists hypothesis may say      academic fields  score averaged   group   students  share  common pair  values  verbal  mathematical intelligences   constant times  level  verbal intelligence plus another constant times  level  mathematical intelligence ie    combination   two factors  numbers   particular subject    two kinds  intelligence  multiplied  obtain  expected score  posited   hypothesis       intelligence level pairs   called factor loading   subject  example  hypothesis may hold   average students aptitude   field  astronomy   numbers    factor loadings associated  astronomy  academic subjects may  different factor loadings two students  identical degrees  verbal intelligence  identical degrees  mathematical intelligence may  different aptitudes  astronomy  individual aptitudes differ  average aptitudes  difference  called  error  statistical term  means  amount    individual differs    average     levels  intelligence see errors  residuals  statistics  observable data  go  factor analysis   scores     students  total  numbers  factor loadings  levels   two kinds  intelligence   student must  inferred   data mathematical model    example   following matrices will  indicated  indexed variables subject indices will  indicated using letters ab  c  values running   formula   equal     example factor indices will  indicated using letters p q  r  values running   formula   equal     example instance  sample indices will  indicated using letters ij  k  values running   formula   example    sample  formula students responded   formula questions  ith students score   ath question  given  formula  purpose  factor analysis   characterize  correlations   variables formula    formula   particular instance  set  observations  order   variables    equal footing   normalized   sample mean    sample variance  given   factor analysis model   particular sample     succinctly   matrix notation   observe   doubling  scale   verbal intelligencethe first component   column  fis measured  simultaneously halving  factor loadings  verbal intelligence makes  difference   model thus  generality  lost  assuming   standard deviation  verbal intelligence  likewise  mathematical intelligence moreover  similar reasons  generality  lost  assuming  two factors  uncorrelated      words  formula   kronecker delta  formula   formulathe errors  assumed   independent   factors note  since  rotation   solution  also  solution  makes interpreting  factors difficult see disadvantages    particular example     know beforehand   two types  intelligence  uncorrelated    interpret  two factors   two different types  intelligence even    uncorrelated   tell  factor corresponds  verbal intelligence   corresponds  mathematical intelligence without  outside argument  values   loadings l  averages   variances   errors must  estimated given  observed data x  f  assumption   levels   factors  fixed   given f  fundamental theorem may  derived    conditions  term   left   ab term   correlation matrix  formula matrix   observed data   formula diagonal elements will  s  last term   right will   diagonal matrix  terms less  unity  first term   right   reduced correlation matrix  will  equal   correlation matrix except   diagonal values  will  less  unity  diagonal elements   reduced correlation matrix  called communalities  represent  fraction   variance   observed variable   accounted    factors  sample data formula will   course exactly obey  fundamental equation given  due  sampling errors inadequacy   model etc  goal   analysis    model   find  factors formula  loadings formula    sense give  best fit   data  factor analysis  best fit  defined   minimum   mean square error   offdiagonal residuals   correlation matrix   equivalent  minimizing  offdiagonal components   error covariance    model equations  expected values  zero     contrasted  principal component analysis  seeks  minimize  mean square error   residuals   advent  high speed computers considerable effort  devoted  finding approximate solutions   problem particularly  estimating  communalities   means   simplifies  problem considerably  yielding  known reduced correlation matrix    used  estimate  factors   loadings   advent  highspeed computers  minimization problem can  solved quickly  directly   communalities  calculated   process rather   needed beforehand  minres algorithm  particularly suited   problem   hardly   means  finding  exact solution geometric interpretation  parameters  variables  factor analysis can  given  geometrical interpretation  data formula  factors formula   errors formula can  viewed  vectors   formuladimensional euclidean space sample space represented  formula formula  formula respectively since  data  standardized  data vectors   unit length formula  factor vectors define  formuladimensional linear subspace ie  hyperplane   space upon   data vectors  projected orthogonally  follows   model equation   independence   factors   errors formula    example  hyperplane  just  dimensional plane defined   two factor vectors  projection   data vectors onto  hyperplane  given    errors  vectors   projected point   data point   perpendicular   hyperplane  goal  factor analysis   find  hyperplane    best fit   data   sense   doesnt matter   factor vectors  define  hyperplane  chosen  long    independent  lie   hyperplane     specify    orthogonal  normal formula   loss  generality   suitable set  factors  found  may also  arbitrarily rotated within  hyperplane    rotation   factor vectors will define   hyperplane  also   solution   result    example    fitting hyperplane  two dimensional     know beforehand   two types  intelligence  uncorrelated    interpret  two factors   two different types  intelligence even    uncorrelated   tell  factor corresponds  verbal intelligence   corresponds  mathematical intelligence  whether  factors  linear combinations   without  outside argument  data vectors formula  unit length  correlation matrix   data  given  formula  correlation matrix can  geometrically interpreted   cosine   angle   two data vectors formula  formula  diagonal elements will clearly  s    diagonal elements will  absolute values less   equal  unity  reduced correlation matrix  defined   goal  factor analysis   choose  fitting hyperplane    reduced correlation matrix reproduces  correlation matrix  nearly  possible except   diagonal elements   correlation matrix   known   unit value   words  goal   reproduce  accurately  possible  crosscorrelations   data specifically   fitting hyperplane  mean square error   offdiagonal components    minimized    accomplished  minimizing   respect   set  orthonormal factor vectors  can  seen   term   right  just  covariance   errors   model  error covariance  stated    diagonal matrix     minimization problem will  fact yield  best fit   model  will yield  sample estimate   error covariance    offdiagonal components minimized   mean square sense  can  seen  since  formula  orthogonal projections   data vectors  length will  less   equal   length   projected data vector   unity  square   lengths  just  diagonal elements   reduced correlation matrix  diagonal elements   reduced correlation matrix  known  communalities large values   communalities will indicate   fitting hyperplane  rather accurately reproducing  correlation matrix    noted   mean values   factors must also  constrained   zero    follows   mean values   errors will also  zero practical implementation type  factor analysis exploratory factor analysis efa  used  identify complex interrelationships among items  group items   part  unified concepts  researcher makes   priori assumptions  relationships among factors confirmatory factor analysis cfa    complex approach  tests  hypothesis   items  associated  specific factors cfa uses structural equation modeling  test  measurement model whereby loading   factors allows  evaluation  relationships  observed variables  unobserved variables structural equation modeling approaches can accommodate measurement error   less restrictive  leastsquares estimation hypothesized models  tested  actual data   analysis  demonstrate loadings  observed variables   latent variables factors  well   correlation   latent variables types  factoring principal component analysis pca   widely used method  factor extraction    first phase  efa factor weights  computed  extract  maximum possible variance  successive factoring continuing      meaningful variance left  factor model must   rotated  analysis canonical factor analysis also called raos canonical factoring   different method  computing   model  pca  uses  principal axis method canonical factor analysis seeks factors    highest canonical correlation   observed variables canonical factor analysis  unaffected  arbitrary rescaling   data common factor analysis also called principal factor analysis pfa  principal axis factoring paf seeks  least number  factors  can account   common variance correlation   set  variables image factoring  based   correlation matrix  predicted variables rather  actual variables   variable  predicted   others using multiple regression alpha factoring  based  maximizing  reliability  factors assuming variables  randomly sampled   universe  variables   methods assume cases   sampled  variables fixed factor regression model   combinatorial model  factor model  regression model  alternatively  can  viewed   hybrid factor model whose factors  partially known terminology factor loadings commonality   square  standardized outer loading   item analogous  pearsons r  squared factor loading   percent  variance   indicator variable explained   factor  get  percent  variance    variables accounted    factor add  sum   squared factor loadings   factor column  divide   number  variables note  number  variables equals  sum   variances   variance   standardized variable       dividing  factors eigenvalue   number  variables interpreting factor loadings  one rule  thumb  confirmatory factor analysis loadings    higher  confirm  independent variables identified  priori  represented   particular factor   rationale   level corresponds   half   variance   indicator  explained   factor however  standard   high one  reallife data may well  meet  criterion     researchers particularly  exploratory purposes will use  lower level     central factor    factors   event factor loadings must  interpreted   light  theory   arbitrary cutoff levels  oblique rotation one gets   pattern matrix   structure matrix  structure matrix  simply  factor loading matrix   orthogonal rotation representing  variance   measured variable explained   factor    unique  common contributions basis  pattern matrix  contrast contains coefficients  just represent unique contributions   factors  lower  pattern coefficients   rule since  will   common contributions  variance explained  oblique rotation  researcher looks    structure  pattern coefficients  attributing  label   factor principles  oblique rotation can  derived   cross entropy   dual entropy communality  sum   squared factor loadings   factors   given variable row   variance   variable accounted     factors    called  communality  communality measures  percent  variance   given variable explained    factors jointly  may  interpreted   reliability   indicator spurious solutions   communality exceeds    spurious solution  may reflect  small  sample   researcher   many    factors uniqueness   variable   uniqueness   variability   variable minus  communality eigenvaluescharacteristic roots  eigenvalue   given factor measures  variance    variables   accounted    factor  ratio  eigenvalues   ratio  explanatory importance   factors  respect   variables   factor   low eigenvalue    contributing little   explanation  variances   variables  may  ignored  redundant   important factors eigenvalues measure  amount  variation   total sample accounted    factor extraction sums  squared loadings initial eigenvalues  eigenvalues  extraction listed  spss  extraction sums  squared loadings     pca extraction    extraction methods eigenvalues  extraction will  lower   initial counterparts spss also prints rotation sums  squared loadings  even  pca  eigenvalues will differ  initial  extraction eigenvalues though  total will    factor scores also called component scores  pca   scores   case row   factor column  compute  factor score   given case   given factor one takes  cases standardized score   variable multiplies   corresponding loadings   variable   given factor  sums  products computing factor scores allows one  look  factor outliers also factor scores may  used  variables  subsequent modeling explained  pca   factor analysis perspective criteria  determining  number  factors researchers wish  avoid  subjective  arbitrary criteria  factor retention   made sense    number  objective methods   developed  solve  problem allowing users  determine  appropriate range  solutions  investigate methods may  agree  instance  parallel analysis may suggest factors  velicers map suggests   researcher may request   factor solutions  discuss   terms   relation  external data  theory modern criteria horns parallel analysis pa  montecarlo based simulation method  compares  observed eigenvalues   obtained  uncorrelated normal variables  factor  component  retained   associated eigenvalue  bigger   th   distribution  eigenvalues derived   random data pa  one    recommended rules  determining  number  components  retain  many programs fail  include  option  notable exception  r velicers map test involves  complete principal components analysis followed   examination   series  matrices  partial correlations p  squared correlation  step see figure   average squared offdiagonal correlation   unpartialed correlation matrix  step  first principal component   associated items  partialed  thereafter  average squared offdiagonal correlation   subsequent correlation matrix   computed  step  step  first two principal components  partialed    resultant average squared offdiagonal correlation   computed  computations  carried   k minus one step k representing  total number  variables   matrix thereafter    average squared correlations   step  lined    step number   analyses  resulted   lowest average squared partial correlation determines  number  components  factors  retain velicer   method components  maintained  long   variance   correlation matrix represents systematic variance  opposed  residual  error variance although methodologically akin  principal components analysis  map technique   shown  perform quite well  determining  number  factors  retain  multiple simulation studies  procedure  made available  spsss user interface see courtney  guidance older methods kaiser criterion  kaiser rule   drop  components  eigenvalues     eigenvalue equal   information accounted    average single item  kaiser criterion   default  spss   statistical software    recommended  used   sole cutoff criterion  estimating  number  factors   tends  overextract factors  variation   method   created   researcher calculates confidence intervals   eigenvalue  retains  factors    entire confidence interval greater  scree plot  cattell scree test plots  components   x axis   corresponding eigenvalues   yaxis  one moves   right toward later components  eigenvalues drop   drop ceases   curve makes  elbow toward less steep decline cattells scree test says  drop   components   one starting  elbow  rule  sometimes criticised   amenable  researchercontrolled    picking  elbow can  subjective   curve  multiple elbows    smooth curve  researcher may  tempted  set  cutoff   number  factors desired   research agenda variance explained criteria  researchers simply use  rule  keeping enough factors  account  sometimes   variation   researchers goal emphasizes parsimony explaining variance    factors  possible  criterion    low  rotation methods  unrotated output maximises variance accounted    first  subsequent factors  forces  factors   orthogonal  datacompression comes   cost    items load   early factors  usually   many items load substantially    one factor rotation serves  make  output  understandable  seeking socalled simple structure  pattern  loadings   item loads strongly   one   factors  much  weakly    factors rotations can  orthogonal  oblique allowing  factors  correlate varimax rotation   orthogonal rotation   factor axes  maximize  variance   squared loadings   factor column    variables rows   factor matrix    effect  differentiating  original variables  extracted factor  factor will tend   either large  small loadings   particular variable  varimax solution yields results  make   easy  possible  identify  variable   single factor     common rotation option however  orthogonality ie independence  factors  often  unrealistic assumption oblique rotations  inclusive  orthogonal rotation    reason oblique rotations   preferred method quartimax rotation   orthogonal alternative  minimizes  number  factors needed  explain  variable  type  rotation often generates  general factor    variables  loaded   high  medium degree   factor structure  usually  helpful   research purpose equimax rotation   compromise  varimax  quartimax criteria direct oblimin rotation   standard method  one wishes  nonorthogonal oblique solution   one    factors  allowed   correlated  will result  higher eigenvalues  diminished interpretability   factors see  promax rotation   alternative nonorthogonal oblique rotation method   computationally faster   direct oblimin method  therefore  sometimes used   large datasets  psychometrics history charles spearman pioneered  use  factor analysis   field  psychology   sometimes credited   invention  factor analysis  discovered  school childrens scores   wide variety  seemingly unrelated subjects  positively correlated  led   postulate   general mental ability  g underlies  shapes human cognitive performance  postulate now enjoys broad support   field  intelligence research    known   g theory raymond cattell expanded  spearmans idea   twofactor theory  intelligence  performing   tests  factor analysis  used  multifactor theory  explain intelligence cattells theory addressed alternate factors  intellectual development including motivation  psychology cattell also developed several mathematical methods  adjusting psychometric graphs    scree test  similarity coefficients  research led   development   theory  fluid  crystallized intelligence  well   personality factors theory  personality cattell   strong advocate  factor analysis  psychometrics  believed   theory   derived  research  supports  continued use  empirical observation  objective testing  study human intelligence applications  psychology factor analysis  used  identify factors  explain  variety  results  different tests  example intelligence research found  people  get  high score   test  verbal ability  also good   tests  require verbal abilities researchers explained   using factor analysis  isolate one factor often called crystallized intelligence  verbal intelligence  represents  degree   someone  able  solve problems involving verbal skills factor analysis  psychology   often associated  intelligence research however  also   used  find factors   broad range  domains   personality attitudes beliefs etc   linked  psychometrics   can assess  validity   instrument  finding   instrument indeed measures  postulated factors exploratory factor analysis versus principal components analysis  exploratory factor analysis  principal component analysis  treated  synonymous techniques   fields  statistics    criticised eg fabrigar et al suhr  factor analysis  researcher makes  assumption   underlying causal model exists whereas pca  simply  variable reduction technique researchers  argued   distinctions   two techniques may mean    objective benefits  preferring one    based   analytic goal   factor model  incorrectly formulated   assumptions   met  factor analysis will give erroneous results factor analysis   used successfully  adequate understanding   system permits good initial model formulations principal component analysis employs  mathematical transformation   original data   assumptions   form   covariance matrix  aim  pca   determine   linear combinations   original variables  can  used  summarize  data set without losing much information arguments contrasting pca  efa fabrigar et al address  number  reasons used  suggest  principal components analysis   equivalent  factor analysis variance versus covariance factor analysis takes  account  random error   inherent  measurement whereas pca fails     point  exemplified  brown  indicated   respect   correlation matrices involved   calculations   reason brown recommends using factor analysis  theoretical ideas  relationships  variables exist whereas pca   used   goal   researcher   explore patterns   data differences  procedure  results  differences  principal components analysis  factor analysis   illustrated  suhr  marketing  basic steps  information collection  data collection stage  usually done  marketing research professionals survey questions ask  respondent  rate  product sample  descriptions  product concepts   range  attributes anywhere  five  twenty attributes  chosen   include things like ease  use weight accuracy durability colourfulness price  size  attributes chosen will vary depending   product  studied   question  asked    products   study  data  multiple products  coded  input   statistical program   r spss sas stata statistica jmp  systat analysis  analysis will isolate  underlying factors  explain  data using  matrix  associations factor analysis   interdependence technique  complete set  interdependent relationships  examined    specification  dependent variables independent variables  causality factor analysis assumes    rating data  different attributes can  reduced     important dimensions  reduction  possible   attributes may  related     rating given   one attribute  partially  result   influence   attributes  statistical algorithm deconstructs  rating called  raw score   various components  reconstructs  partial scores  underlying factor scores  degree  correlation   initial raw score   final factor score  called  factor loading  physical  biological sciences factor analysis  also  widely used  physical sciences   geochemistry hydrochemistry astrophysics  cosmology  well  biological sciences   ecology molecular biology  biochemistry  groundwater quality management   important  relate  spatial distribution  different chemical parameters  different possible sources   different chemical signatures  example  sulfide mine  likely   associated  high levels  acidity dissolved sulfates  transition metals  signatures can  identified  factors  rmode factor analysis   location  possible sources can  suggested  contouring  factor scores  geochemistry different factors can correspond  different mineral associations  thus  mineralisation  microarray analysis factor analysis can  used  summarizing highdensity oligonucleotide dna microarrays data  probe level  affymetrix genechips   case  latent variable corresponds   rna concentration   sample implementation factor analysis   implemented  several statistical analysis programs since  s\r\n"}
{"index":{"_id":60}}
{"conceptLabelTag":"kernel trick","conceptLabel":"kernel trick","conceptDescription":"kernel method  machine learning kernel methods   class  algorithms  pattern analysis whose best known member   support vector machine svm  general task  pattern analysis   find  study general types  relations  example clusters rankings principal components correlations classifications  datasets  many algorithms  solve  tasks  data  raw representation    explicitly transformed  feature vector representations via  userspecified feature map  contrast kernel methods require   userspecified kernel ie  similarity function  pairs  data points  raw representation kernel methods owe  name   use  kernel functions  enable   operate   highdimensional implicit feature space without ever computing  coordinates   data   space  rather  simply computing  inner products   images   pairs  data   feature space  operation  often computationally cheaper   explicit computation   coordinates  approach  called  kernel trick kernel functions   introduced  sequence data graphs text images  well  vectors algorithms capable  operating  kernels include  kernel perceptron support vector machines svm gaussian processes principal components analysis pca canonical correlation analysis ridge regression spectral clustering linear adaptive filters  many others  linear model can  turned   nonlinear model  applying  kernel trick   model replacing  features predictors   kernel function  kernel algorithms  based  convex optimization  eigenproblems   statistically wellfounded typically  statistical properties  analyzed using statistical learning theory  example using rademacher complexity motivation  informal explanation kernel methods can  thought   instancebased learners rather  learning  fixed set  parameters corresponding   features   inputs  instead remember  formulath training example formula  learn    corresponding weight formula prediction  unlabeled inputs ie     training set  treated   application   similarity function formula called  kernel   unlabeled input formula     training inputs formula  instance  kernelized binary classifier typically computes  weighted sum  similarities  kernel classifiers  described  early   s   invention   kernel perceptron  rose  great prominence   popularity   support vector machine svm   s   svm  found   competitive  neural networks  tasks   handwriting recognition mathematics  kernel trick  kernel trick avoids  explicit mapping   needed  get linear learning algorithms  learn  nonlinear function  decision boundary   formula  formula   input space formula certain functions formula can  expressed   inner product  another space formula  function formula  often referred    kernel   kernel function  word kernel  used  mathematics  denote  weighting function   weighted sum  integral certain problems  machine learning  additional structure   arbitrary weighting function formula  computation  made much simpler   kernel can  written   form   feature map formula  satisfies  key restriction   formula must   proper inner product    hand  explicit representation  formula   necessary  long  formula   inner product space  alternative follows  mercers theorem  implicitly defined function formula exists whenever  space formula can  equipped   suitable measure ensuring  function formula satisfies mercers condition mercers theorem  akin   generalization   result  linear algebra  associates  inner product   positivedefinite matrix  fact mercers condition can  reduced   simpler case   choose   measure  counting measure formula   formula  counts  number  points inside  set formula   integral  mercers theorem reduces   summation   summation holds   finite sequences  points formula  formula   choices  formula realvalued coefficients formula cf positive definite kernel   function formula satisfies mercers condition  algorithms  depend  arbitrary relationships   native space formula   fact   linear interpretation   different setting  range space  formula  linear interpretation gives us insight   algorithm furthermore   often  need  compute formula directly  computation    case  support vector machines  cite  running time shortcut   primary benefit researchers also use   justify  meanings  properties  existing algorithms theoretically  gram matrix formula  respect  formula sometimes also called  kernel matrix  formula must  positive semidefinite psd empirically  machine learning heuristics choices   function formula    satisfy mercers condition may still perform reasonably  formula  least approximates  intuitive idea  similarity regardless  whether formula   mercer kernel formula may still  referred    kernel   kernel function formula  also  covariance function  used  gaussian processes   gram matrix formula can also  called  covariance matrix finally suppose  formula   square matrix  formula   positivesemidefinite matrix applications application areas  kernel methods  diverse  include geostatistics kriging inverse distance weighting d reconstruction bioinformatics chemoinformatics information extraction  handwriting recognition\r\n"}
{"index":{"_id":61}}
{"conceptLabelTag":"support vector machine","conceptLabel":"support vector machine","conceptDescription":"support vector machine  machine learning support vector machines svms also support vector networks  supervised learning models  associated learning algorithms  analyze data used  classification  regression analysis given  set  training examples  marked  belonging  one     two categories  svm training algorithm builds  model  assigns new examples  one category    making   nonprobabilistic binary linear classifier  svm model   representation   examples  points  space mapped    examples   separate categories  divided   clear gap    wide  possible new examples   mapped    space  predicted  belong   category based   side   gap  fall  addition  performing linear classification svms can efficiently perform  nonlinear classification using   called  kernel trick implicitly mapping  inputs  highdimensional feature spaces  data   labeled supervised learning   possible   unsupervised learning approach  required  attempts  find natural clustering   data  groups   map new data   formed groups  clustering algorithm  provides  improvement   support vector machines  called support vector clustering   often used  industrial applications either  data   labeled     data  labeled   preprocessing   classification pass motivation classifying data   common task  machine learning suppose  given data points  belong  one  two classes   goal   decide  class  new data point will     case  support vector machines  data point  viewed   formuladimensional vector  list  formula numbers   want  know whether  can separate  points   formuladimensional hyperplane   called  linear classifier   many hyperplanes  might classify  data one reasonable choice   best hyperplane   one  represents  largest separation  margin   two classes   choose  hyperplane    distance     nearest data point   side  maximized    hyperplane exists   known   maximummargin hyperplane   linear classifier  defines  known   maximum margin classifier  equivalently  perceptron  optimal stability definition  formally  support vector machine constructs  hyperplane  set  hyperplanes   high  infinitedimensional space  can  used  classification regression   tasks intuitively  good separation  achieved   hyperplane    largest distance   nearest trainingdata point   class socalled functional margin since  general  larger  margin  lower  generalization error   classifier whereas  original problem may  stated   finite dimensional space  often happens   sets  discriminate   linearly separable   space   reason   proposed   original finitedimensional space  mapped   much higherdimensional space presumably making  separation easier   space  keep  computational load reasonable  mappings used  svm schemes  designed  ensure  dot products may  computed easily  terms   variables   original space  defining   terms   kernel function formula selected  suit  problem  hyperplanes   higherdimensional space  defined   set  points whose dot product   vector   space  constant  vectors defining  hyperplanes can  chosen   linear combinations  parameters formula  images  feature vectors formula  occur   data base   choice   hyperplane  points formula   feature space   mapped   hyperplane  defined   relation formula note   formula becomes small  formula grows  away  formula  term   sum measures  degree  closeness   test point formula   corresponding data base point formula   way  sum  kernels  can  used  measure  relative nearness   test point   data points originating  one      sets   discriminated note  fact   set  points formula mapped   hyperplane can  quite convoluted   result allowing much  complex discrimination  sets    convex     original space applications svms can  used  solve various real world problems history  original svm algorithm  invented  vladimir n vapnik  alexey ya chervonenkis   bernhard e boser isabelle m guyon  vladimir n vapnik suggested  way  create nonlinear classifiers  applying  kernel trick  maximummargin hyperplanes  current standard incarnation soft margin  proposed  corinna cortes  vapnik   published  linear svm   given  training dataset  formula points   form   formula  either   indicating  class    point formula belongs  formula   formuladimensional real vector  want  find  maximummargin hyperplane  divides  group  points formula   formula   group  points   formula   defined    distance   hyperplane   nearest point formula  either group  maximized  hyperplane can  written   set  points formula satisfying  formula    necessarily normalized normal vector   hyperplane  parameter formula determines  offset   hyperplane   origin along  normal vector formula hardmargin   training data  linearly separable  can select two parallel hyperplanes  separate  two classes  data    distance     large  possible  region bounded   two hyperplanes  called  margin   maximummargin hyperplane   hyperplane  lies halfway    hyperplanes can  described   equations  geometrically  distance   two hyperplanes  formula   maximize  distance   planes  want  minimize formula   also   prevent data points  falling   margin  add  following constraint   formula either   constraints state   data point must lie   correct side   margin  can  rewritten   can put  together  get  optimization problem  formula  formula  solve  problem determine  classifier formula  easytosee  important consequence   geometric description   maxmargin hyperplane  completely determined   formula  lie nearest    formula  called support vectors softmargin  extend svm  cases    data   linearly separable  introduce  hinge loss function  function  zero   constraint   satisfied   words  formula lies   correct side   margin  data   wrong side   margin  functions value  proportional   distance   margin   wish  minimize   parameter formula determines  tradeoff  increasing  marginsize  ensuring   formula lie   correct side   margin thus  sufficiently small values  formula  softmargin svm will behave identically   hardmargin svm   input data  linearly classifiable  will still learn  viable classification rule   nonlinear classification  original maximummargin hyperplane algorithm proposed  vapnik  constructed  linear classifier however  bernhard e boser isabelle m guyon  vladimir n vapnik suggested  way  create nonlinear classifiers  applying  kernel trick originally proposed  aizerman et al  maximummargin hyperplanes  resulting algorithm  formally similar except  every dot product  replaced   nonlinear kernel function  allows  algorithm  fit  maximummargin hyperplane   transformed feature space  transformation may  nonlinear   transformed space high dimensional although  classifier   hyperplane   transformed feature space  may  nonlinear   original input space   noteworthy  working   higherdimensional feature space increases  generalization error  support vector machines although given enough samples  algorithm still performs well  common kernels include  kernel  related   transform formula   equation formula  value w  also   transformed space  formula dot products  w  classification can   computed   kernel trick ie formula computing  svm classifier computing  softmargin svm classifier amounts  minimizing  expression   form  focus   softmargin classifier since  noted  choosing  sufficiently small value  formula yields  hardmargin classifier  linearly classifiable input data  classical approach  involves reducing   quadratic programming problem  detailed    recent approaches   subgradient descent  coordinate descent will  discussed primal minimizing can  rewritten   constrained optimization problem   differentiable objective function   following way   formula  introduce  variable formula note  formula   smallest nonnegative number satisfying formula thus  can rewrite  optimization problem  follows   called  primal problem dual  solving   lagrangian dual    problem one obtains  simplified problem   called  dual problem since  dual maximization problem   quadratic function   formula subject  linear constraints   efficiently solvable  quadratic programming algorithms   variables formula  defined   moreover formula exactly  formula lies   correct side   margin  formula  formula lies   margins boundary  follows  formula can  written   linear combination   support vectors  offset formula can  recovered  finding  formula   margins boundary  solving kernel trick suppose now    like  learn  nonlinear classification rule  corresponds   linear classification rule   transformed data points formula moreover   given  kernel function formula  satisfies formula  know  classification vector formula   transformed space satisfies   formula  obtained  solving  optimization problem  coefficients formula can  solved  using quadratic programming     can find  index formula   formula   formula lies   boundary   margin   transformed space   solve finally new points can  classified  computing modern methods recent algorithms  finding  svm classifier include subgradient descent  coordinate descent  techniques  proven  offer significant advantages   traditional approach  dealing  large sparse datasetssubgradient methods  especially efficient    many training examples  coordinate descent   dimension   feature space  high subgradient descent subgradient descent algorithms   svm work directly   expression note  formula   convex function  formula  formula   traditional gradient descent  sgd methods can  adapted  instead  taking  step   direction   functions gradient  step  taken   direction   vector selected   functions subgradient  approach   advantage   certain implementations  number  iterations   scale  formula  number  data points coordinate descent coordinate descent algorithms   svm work   dual problem   formula iteratively  coefficient formula  adjusted   direction  formula   resulting vector  coefficients formula  projected onto  nearest vector  coefficients  satisfies  given constraints typically euclidean distances  used  process   repeated   nearoptimal vector  coefficients  obtained  resulting algorithm  extremely fast  practice although  performance guarantees   proven empirical risk minimization  softmargin support vector machine described    example   empirical risk minimization erm algorithm   hinge loss seen  way support vector machines belong   natural class  algorithms  statistical inference  many   unique features  due   behavior   hinge loss  perspective can provide  insight     svms work  allow us  better analyze  statistical properties risk minimization  supervised learning one  given  set  training examples formula  labels formula  wishes  predict formula given formula    one forms  hypothesis formula   formula   good approximation  formula  good approximation  usually defined   help   loss function formula  characterizes  bad formula    prediction  formula    like  choose  hypothesis  minimizes  expected risk   cases  dont know  joint distribution  formula outright   cases  common strategy   choose  hypothesis  minimizes  empirical risk  certain assumptions   sequence  random variables formula  example    generated   finite markov process   set  hypotheses  considered  small enough  minimizer   empirical risk will closely approximate  minimizer   expected risk  formula grows large  approach  called empirical risk minimization  erm regularization  stability  order   minimization problem    welldefined solution    place constraints   set formula  hypotheses  considered  formula   normed space    case  svm  particularly effective technique   consider   hypotheses formula   formula   equivalent  imposing  regularization penalty formula  solving  new optimization problem formula  approach  called tikhonov regularization  generally formula can   measure   complexity   hypothesis formula   simpler hypotheses  preferred svm   hinge loss recall   softmargin svm classifier formula  chosen  minimize  following expression formula  light    discussion  see   svm technique  equivalent  empirical risk minimization  tikhonov regularization    case  loss function   hinge loss   perspective svm  closely related   fundamental classification algorithms   regularized leastsquares  logistic regression  difference   three lies   choice  loss function regularized leastsquares amounts  empirical risk minimization   squareloss formula logistic regression employs  logloss formula target functions  difference   hinge loss    loss functions  best stated  terms  target functions  function  minimizes expected risk   given pair  random variables formula  particular let formula denote formula conditional   event  formula   classification setting    optimal classifier  therefore formula   squareloss  target function   conditional expectation function formula   logistic loss   logit function formula     target functions yield  correct classifier  formula  give us  information   need  fact  give us enough information  completely describe  distribution  formula    hand one can check   target function   hinge loss  exactly formula thus   sufficiently rich hypothesis spaceor equivalently   appropriately chosen kernelthe svm classifier will converge   simplest function  terms  formula  correctly classifies  data  extends  geometric interpretation  svmfor linear classification  empirical risk  minimized   function whose margins lie   support vectors   simplest     maxmargin classifier properties svms belong   family  generalized linear classifiers  can  interpreted   extension   perceptron  can also  considered  special case  tikhonov regularization  special property    simultaneously minimize  empirical classification error  maximize  geometric margin hence   also known  maximum margin classifiers  comparison   svm   classifiers   made  meyer leisch  hornik parameter selection  effectiveness  svm depends   selection  kernel  kernels parameters  soft margin parameter c  common choice   gaussian kernel    single parameter formula  best combination  c  formula  often selected   grid search  exponentially growing sequences  c  formula  example formula formula typically  combination  parameter choices  checked using cross validation   parameters  best crossvalidation accuracy  picked alternatively recent work  bayesian optimization can  used  select c  formula often requiring  evaluation  far fewer parameter combinations  grid search  final model   used  testing   classifying new data   trained   whole training set using  selected parameters issues potential drawbacks   svm include  following aspects extensions support vector clustering svc svc   similar method  also builds  kernel functions   appropriate  unsupervised learning  datamining   considered  fundamental method  data science multiclass svm multiclass svm aims  assign labels  instances  using support vector machines   labels  drawn   finite set  several elements  dominant approach      reduce  single multiclass problem  multiple binary classification problems common methods   reduction include crammer  singer proposed  multiclass svm method  casts  multiclass classification problem   single optimization problem rather  decomposing   multiple binary classification problems see also lee lin  wahba transductive support vector machines transductive support vector machines extend svms     also treat partially labeled data  semisupervised learning  following  principles  transduction   addition   training set formula  learner  also given  set  test examples   classified formally  transductive support vector machine  defined   following primal optimization problem minimize  formula subject    formula   formula  transductive support vector machines  introduced  vladimir n vapnik  structured svm svms   generalized  structured svms   label space  structured   possibly infinite size regression  version  svm  regression  proposed   vladimir n vapnik harris drucker christopher j c burges linda kaufman  alexander j smola  method  called support vector regression svr  model produced  support vector classification  described  depends    subset   training data   cost function  building  model   care  training points  lie beyond  margin analogously  model produced  svr depends    subset   training data   cost function  building  model ignores  training data close   model prediction another svm version known  least squares support vector machine lssvm   proposed  suykens  vandewalle training  original svr means solving  formula   training sample  target value formula  inner product plus intercept formula   prediction   sample  formula    parameter  serves   threshold  predictions    within  formula range   true predictions slack variables  usually added     allow  errors   allow approximation   case   problem  infeasible implementation  parameters   maximummargin hyperplane  derived  solving  optimization  exist several specialized algorithms  quickly solving  qp problem  arises  svms mostly relying  heuristics  breaking  problem   smaller moremanageable chunks another approach   use  interior point method  uses newtonlike iterations  find  solution   karushkuhntucker conditions   primal  dual problems instead  solving  sequence  broken  problems  approach directly solves  problem altogether  avoid solving  linear system involving  large kernel matrix  low rank approximation   matrix  often used   kernel trick another common method  platts sequential minimal optimization smo algorithm  breaks  problem   dimensional subproblems   solved analytically eliminating  need   numerical optimization algorithm  matrix storage  algorithm  conceptually simple easy  implement generally faster   better scaling properties  difficult svm problems  special case  linear support vector machines can  solved  efficiently    kind  algorithms used  optimize  close cousin logistic regression  class  algorithms includes subgradient descent eg pegasos  coordinate descent eg liblinear liblinear   attractive training time properties  convergence iteration takes time linear   time taken  read  train data   iterations also   qlinear convergence property making  algorithm extremely fast  general kernel svms can also  solved  efficiently using subgradient descent eg ppacksvm especially  parallelization  allowed kernel svms  available  many machine learning toolkits including libsvm matlab sas svmlight kernlab scikitlearn shogun weka shark jkernelmachines opencv  others\r\n"}
{"index":{"_id":62}}
{"conceptLabelTag":"baum welch algorithm","conceptLabel":"baum welch algorithm","conceptDescription":"baumwelch algorithm  electrical engineering computer science statistical computing  bioinformatics  baumwelch algorithm  used  find  unknown parameters   hidden markov model hmm  makes use   forwardbackward algorithm   named  leonard e baum  lloyd r welch history hidden markov models   baumwelch algorithm  first described   series  articles  leonard e baum   peers   institute  defense analysis   late s one   first major applications  hmms    field  speech processing   s hmms  emerging   useful tool   analysis  biological systems  information   particular genetic information   since become  important tool   probabilistic modeling  genomic sequences description  hidden markov model describes  joint probability   collection  hidden  observed discrete random variables  relies   assumption   ith hidden variable given   th hidden variable  independent  previous hidden variables   current observation variables depend    current hidden state  baumwelch algorithm uses  well known em algorithm  find  maximum likelihood estimate   parameters   hidden markov model given  set  observed feature vectors let formula   discrete hidden random variable  formula possible values  assume  formula  independent  time formula  leads   definition   timeindependent stochastic transition matrix  initial state distribution ie  formula  given   observation variables formula can take one  formula possible values  probability   certain observation  time formula  state formula  given  taking  account   possible values  formula  formula  obtain  formula matrix formula  observation sequence  given  formula thus  can describe  hidden markov chain  formula  baumwelch algorithm finds  local maximum  formula ie  hmm parameters formula  maximise  probability   observation algorithm set formula  random initial conditions  can also  set using prior information   parameters    available forward procedure let formula  probability  seeing  formula    state formula  time formula   found recursively backward procedure let formula    probability   ending partial sequence formula given starting state formula  time formula  calculate formula  update  can now calculate  temporary variables according  bayes theorem    probability    state formula  time formula given  observed sequence formula   parameters formula    probability    state formula  formula  times formula  formula respectively given  observed sequence formula  parameters formula    expected frequency spent  state formula  time formula    expected number  transitions  state   state j compared   expected total number  transitions away  state   clarify  number  transitions away  state    mean transitions   different state j    state including    equivalent   number  times state   observed   sequence  t  t t    indicator function  formula   expected number  times  output observations   equal  formula   state formula   expected total number  times  state formula  steps  now repeated iteratively   desired level  convergence note   possible  overfit  particular data set   formula  algorithm also   guarantee  global maximum suppose    chicken    collect eggs  noon everyday now whether    chicken  laid eggs  collection depends   unknown factors   hidden  can however  simplicity assume     two states  determine whether  chicken lays eggs now  dont know  state   initial starting point  dont know  transition probabilities   two states   dont know  probability   chicken lays  egg given  particular state  start  first guess  transition  emission matrices example   take set  observations e eggs n  eggs nn nn nn nn ne ee en nn nn  next step   estimate  new transition matrix thus  new estimate    transition  now formula referred   pseudo probabilities   following tables   calculate      transition probabilities  normalize   add   gives us  updated transition matrix next  want  estimate  new emission matrix  new estimate   e coming  emission  now formula  allows us  calculate  emission matrix  described    algorithm  adding   probabilities   respective observed sequences   repeat   n came     n  e came   normalize  estimate  initial probabilities  assume  sequences start   hidden state  calculate  highest probability   repeat     normalize  give  updated initial vector finally  repeat  steps   resulting probabilities converge satisfactorily applications speech recognition hidden markov models  first applied  speech recognition  james k baker  continuous speech recognition occurs   following steps modeled   hmm feature analysis  first undertaken  temporal andor spectral features   speech signal  produces  observation vector  feature   compared   sequences   speech recognition units  units   phonemes syllables  wholeword units  lexicon decoding system  applied  constrain  paths investigated   words   systems lexicon word dictionary  investigated similar   lexicon decoding  system path   constrained   rules  grammar  syntax finally semantic analysis  applied   system outputs  recognized utterance  limitation  many hmm applications  speech recognition    current state  depends   state   previous timestep   unrealistic  speech  dependencies  often several timesteps  duration  baumwelch algorithm also  extensive applications  solving hmms used   field  speech synthesis cryptanalysis  baumwelch algorithm  often used  estimate  parameters  hmms  deciphering hidden  noisy information  consequently  often used  cryptanalysis  data security  observer  like  extract information   data stream without knowing   parameters   transmission  can involve reverse engineering  channel encoder hmms    consequence  baumwelch algorithm  also  used  identify spoken phrases  encrypted voip calls  addition hmm cryptanalysis   important tool  automated investigations  cachetiming data  allows   automatic discovery  critical algorithm state  example key values applications  bioinformatics finding genes prokaryotic  glimmer gene locator  interpolated markov modeler software   early genefinding program used   identification  coding regions  prokaryotic dna glimmer uses interpolated markov models imms  identify  coding regions  distinguish    noncoding dna  latest release glimmer   shown   increased specificity  accuracy compared   predecessors  regard  predicting translation initiation sites demonstrating  average accuracy  locating locations compared  confirmed genes  prokaryotes eukaryotic  genscan webserver   gene locator capable  analyzing eukaryotic sequences   one million basepairs mbp long genscan utilizes  general inhomogeneous three periodic fifth order markov model  dna coding regions additionally  model accounts  differences  gene density  structure   intron lengths  occur  different isochores   integrated genefinding software   time  genscans release assumed input sequences contained exactly one gene genscan solves  general case  partial complete  multiple genes  even  gene    present genscan  shown  exactly predict exon location  accuracy  specificity compared   annotated database copynumber variation detection copynumber variations cnvs   abundant form  genome structure variation  humans  discretevalued bivariate hmm dbhmm  used assigning chromosomal regions  seven distinct states unaffected regions deletions duplications  four transition states solving  model using baumwelch demonstrated  ability  predict  location  cnv breakpoint  approximately bp  microarray experiments  magnitude  resolution enables  precise correlations  different cnvs  across populations  previously possible allowing  study  cnv population frequencies  also demonstrated  direct inheritance pattern   particular cnv\r\n"}
{"index":{"_id":63}}
{"conceptLabelTag":"complete linkage clustering","conceptLabel":"complete linkage clustering","conceptDescription":"completelinkage clustering completelinkage clustering  one  several methods  agglomerative hierarchical clustering   beginning   process  element    cluster     clusters   sequentially combined  larger clusters   elements end      cluster   step  two clusters separated   shortest distance  combined  definition  shortest distance   differentiates   different agglomerative clustering methods  completelinkage clustering  link  two clusters contains  element pairs   distance  clusters equals  distance   two elements one   cluster   farthest away     shortest   links  remains   step causes  fusion   two clusters whose elements  involved  method  also known  farthest neighbour clustering  result   clustering can  visualized   dendrogram  shows  sequence  cluster fusion   distance    fusion took place mathematically  complete linkage function  distance formula  clusters formula  formula  described   following expression formula  complete linkage clustering avoids  drawback   alternative single linkage method  socalled chaining phenomenon  clusters formed via single linkage clustering may  forced together due  single elements  close    even though many   elements   cluster may   distant    complete linkage tends  find compact clusters  approximately equal diameters naive algorithm  following algorithm   agglomerative scheme  erases rows  columns   proximity matrix  old clusters  merged  new ones  formula proximity matrix d contains  distances dij  clusterings  assigned sequence numbers n  lk   level   kth clustering  cluster  sequence number m  denoted m   proximity  clusters r  s  denoted drs  algorithm  composed   following steps optimally efficient algorithm  algorithm explained   easy  understand   complexity formula  may d defays proposed  optimally efficient algorithm   complexity formula known  clink published inspired   similar algorithm slink  singlelinkage clustering  linkages alternative linkage schemes include single linkage  average linkage clustering implementing  different linkage   naive algorithm  simply  matter  using  different formula  calculate intercluster distances   initial computation   proximity matrix   step    algorithm  optimally efficient algorithm  however  available  arbitrary linkages  formula    adjusted   highlighted using bold text\r\n"}
{"index":{"_id":64}}
{"conceptLabelTag":"kernel methods","conceptLabel":"kernel methods","conceptDescription":"kernel method  machine learning kernel methods   class  algorithms  pattern analysis whose best known member   support vector machine svm  general task  pattern analysis   find  study general types  relations  example clusters rankings principal components correlations classifications  datasets  many algorithms  solve  tasks  data  raw representation    explicitly transformed  feature vector representations via  userspecified feature map  contrast kernel methods require   userspecified kernel ie  similarity function  pairs  data points  raw representation kernel methods owe  name   use  kernel functions  enable   operate   highdimensional implicit feature space without ever computing  coordinates   data   space  rather  simply computing  inner products   images   pairs  data   feature space  operation  often computationally cheaper   explicit computation   coordinates  approach  called  kernel trick kernel functions   introduced  sequence data graphs text images  well  vectors algorithms capable  operating  kernels include  kernel perceptron support vector machines svm gaussian processes principal components analysis pca canonical correlation analysis ridge regression spectral clustering linear adaptive filters  many others  linear model can  turned   nonlinear model  applying  kernel trick   model replacing  features predictors   kernel function  kernel algorithms  based  convex optimization  eigenproblems   statistically wellfounded typically  statistical properties  analyzed using statistical learning theory  example using rademacher complexity motivation  informal explanation kernel methods can  thought   instancebased learners rather  learning  fixed set  parameters corresponding   features   inputs  instead remember  formulath training example formula  learn    corresponding weight formula prediction  unlabeled inputs ie     training set  treated   application   similarity function formula called  kernel   unlabeled input formula     training inputs formula  instance  kernelized binary classifier typically computes  weighted sum  similarities  kernel classifiers  described  early   s   invention   kernel perceptron  rose  great prominence   popularity   support vector machine svm   s   svm  found   competitive  neural networks  tasks   handwriting recognition mathematics  kernel trick  kernel trick avoids  explicit mapping   needed  get linear learning algorithms  learn  nonlinear function  decision boundary   formula  formula   input space formula certain functions formula can  expressed   inner product  another space formula  function formula  often referred    kernel   kernel function  word kernel  used  mathematics  denote  weighting function   weighted sum  integral certain problems  machine learning  additional structure   arbitrary weighting function formula  computation  made much simpler   kernel can  written   form   feature map formula  satisfies  key restriction   formula must   proper inner product    hand  explicit representation  formula   necessary  long  formula   inner product space  alternative follows  mercers theorem  implicitly defined function formula exists whenever  space formula can  equipped   suitable measure ensuring  function formula satisfies mercers condition mercers theorem  akin   generalization   result  linear algebra  associates  inner product   positivedefinite matrix  fact mercers condition can  reduced   simpler case   choose   measure  counting measure formula   formula  counts  number  points inside  set formula   integral  mercers theorem reduces   summation   summation holds   finite sequences  points formula  formula   choices  formula realvalued coefficients formula cf positive definite kernel   function formula satisfies mercers condition  algorithms  depend  arbitrary relationships   native space formula   fact   linear interpretation   different setting  range space  formula  linear interpretation gives us insight   algorithm furthermore   often  need  compute formula directly  computation    case  support vector machines  cite  running time shortcut   primary benefit researchers also use   justify  meanings  properties  existing algorithms theoretically  gram matrix formula  respect  formula sometimes also called  kernel matrix  formula must  positive semidefinite psd empirically  machine learning heuristics choices   function formula    satisfy mercers condition may still perform reasonably  formula  least approximates  intuitive idea  similarity regardless  whether formula   mercer kernel formula may still  referred    kernel   kernel function formula  also  covariance function  used  gaussian processes   gram matrix formula can also  called  covariance matrix finally suppose  formula   square matrix  formula   positivesemidefinite matrix applications application areas  kernel methods  diverse  include geostatistics kriging inverse distance weighting d reconstruction bioinformatics chemoinformatics information extraction  handwriting recognition\r\n"}
{"index":{"_id":65}}
{"conceptLabelTag":"randomized weighted majority algorithm","conceptLabel":"randomized weighted majority algorithm","conceptDescription":"randomized weighted majority algorithm  randomized weighted majority algorithm   algorithm  machine learning theory  improves  mistake bound   weighted majority algorithm imagine  every morning   stock market opens  get  prediction     experts  whether  stock market will go     goal   somehow combine  set  predictions   single prediction    use  make  buy  sell decision   day  rwma gives us  way    combination    prediction record will  nearly  good     single best expert  hindsight motivation  machine learning  weighted majority algorithm wma   metalearning algorithm  predicts  expert advice     randomized algorithm suppose   formula experts   best expert makes formula mistakes  weighted majority algorithm wma makes   formula mistakes      good bound  can  better  introducing randomization randomized weighted majority algorithm rwma  nonrandomized weighted majority algorithm wma  guarantees  upper bound  formula   problematic  highly errorprone experts eg  best expert still makes  mistake   time suppose   formula rounds using formula experts   best expert makes formula mistakes  can  guarantee  upper bound  formula   number  mistakes     known limitation  wma attempts  improve  shortcoming   explored  order  improve  dependence  formula instead  predicting based  majority vote  weights  used  probabilities hence  name randomized weighted majority  formula   weight  expert formula let formula  will follow expert formula  probability formula  goal   bound  worstcase expected number  mistakes assuming   adversary  world   select one   answers  correct   make  coin toss    better   worst case idea  worst case   deterministic algorithm weighted majority algorithm    weights split  now     bad since  also   chance  getting  right also  tradeoff  dependence  formula  formula  will generalize  multiply  formula instead  necessarily  formula analysis   formulath round define formula    fraction  weight   wrong answers  formula   probability  make  mistake   formulath round let formula denote  total number  mistakes  made  far furthermore  define formula using  fact  expectation  additive   formulath round formula becomes formula reason  formula fraction   multiplying  formula  formula lets say  formula   number  mistakes   best expert  far  can use  inequality formula now  solve first take  natural log   sides  get formula simplify formula  formula formula now use formula   result  formula lets see   made  progress  formula  get formula  formula  get formula   can see  made progress roughly   form formula uses  randomized weighted majorityrwmn can use  combine multiple algorithms   nearly  well  best  hindsight can apply randomized weighted majority algorithm  situations  experts  making choices    combined  cant  combined easilyfor instance repeated gameplaying  online shortest path problemin  online shortest path problem  expert  telling   different way  drive  work  pick one using randomized weighted majority algorithm later  find   well    done  penalize appropriately    right  want  generalize  just loss    losses  goal   expected loss    much worse  loss  best expertwe generalize  penalize formula meaning  two examples  loss formula gives  weight  one example  loss  one example  loss analysis still oes  extensions bandit problem efficient algorithm   cases  many experts sleeping expertsspecialists setting\r\n"}
{"index":{"_id":66}}
{"conceptLabelTag":"online learning","conceptLabel":"online learning","conceptDescription":"online machine learning  computer science online machine learning   method  machine learning   data becomes available   sequential order   used  update  best predictor  future data   step  opposed  batch learning techniques  generate  best predictor  learning   entire training data set   online learning   common technique used  areas  machine learning    computationally infeasible  train   entire dataset requiring  need  outofcore algorithms   also used  situations    necessary   algorithm  dynamically adapt  new patterns   data    data   generated   function  time eg stock price prediction two general modelling strategies exist  online learning models statistical learning models  adversarial models  statistical learning models eg stochastic gradient descent perceptrons  data samples  assumed   independent  identically distributed random variables ie    adapting  time   algorithm just   limited access   data  adversarial models looking   learning problem   game  two players  learner vs  data generator  goal   minimize losses regardless   move played    player   model  opponent  allowed  dynamically adapt  data generated based   output   learning algorithm spam filtering falls   category   adversary will dynamically generate new spam based   current behavior   spam detector examples  algorithms   model include follow  leader follow  regularized leader etc introduction   setting  supervised learning  function  formula    learned  formula  thought    space  inputs  formula   space  outputs  predicts well  instances   drawn   joint probability distribution formula  formula  reality  learner never knows  true distribution formula  instances instead  learner usually  access   training set  examples formula   setting  loss function  given  formula   formula measures  difference   predicted value formula   true value formula  ideal goal   select  function formula  formula   space  functions called  hypothesis space    notion  total loss  minimised depending   type  model statistical  adversarial one can devise different notions  loss  lead  different learning algorithms statistical learning models  statistical learning models  training sample formula  assumed    drawn iid   true distribution formula   objective   minimize  expected risk  common paradigm   situation   estimate  function formula  empirical risk minimization  regularized empirical risk minimization usually tikhonov regularization  choice  loss function  gives rise  several wellknown learning algorithms   regularized least squares  support vector machines   case  online learning  data  still assumed   iid without access    data  purely online model   category  learn based  just  new input formula  current best predictor formula   extra stored information   usually expected   storage requirements independent  training data size  many formulations  example nonlinear kernel methods true online learning   possible though  form  hybrid online learning  recursive algorithms can  used  formula  permitted  depend  formula   previous data points formula   case  space requirements   longer guaranteed   constant since  requires storing  previous data points   solution may take less time  compute   addition   new data point  compared  batch learning techniques  important generalization   techniques  minibatch techniques  process  small batch  formula data points   time  can  considered  online algorithms  formula much smaller   total number  training points minibatch techniques  used  repeated passing   training data called incremental methods  obtain optimized outofcore versions  machine learning algorithms  eg stochastic gradient descent  combined  backpropogation   currently  de facto training method  training artificial neural networks example linear least squares  simple example  linear least squares  used  explain  variety  ideas  online learning  ideas  general enough   applied   settings  eg   convex loss functions batch learning   setting  supervised learning   square loss function  intent   minimize  empirical loss let formula   formula data matrix  formula   formula matrix  target values   arrival   first formula data points assuming   covariance matrix formula  invertible otherwise   preferential  proceed   similar fashion  tikhonov regularization  best solution formula   linear least squares problem  given  now calculating  covariance matrix formula takes time formula inverting  formula matrix takes time formula   rest   multiplication takes time formula giving  total time  formula  formula total points   dataset    recompute  solution   arrival  every datapoint formula  naive approach will   total complexity formula note   storing  matrix formula  updating    step needs  adding formula  takes formula time reducing  total time  formula    additional storage space  formula  store formula online learning recursive least squares  recursive least squares algorithm considers  online approach   least squares problem  can  shown   initialising formula  formula  solution   linear least squares problem given   previous section can  computed   following iteration   iteration algorithm can  proved using induction  formula  proof also shows  formula one can look  rls also   context  adaptive filters see rls  complexity  formula steps   algorithm  formula    order  magnitude faster   corresponding batch learning complexity  storage requirements  every step formula    store  matrix formula   constant  formula   case  formula   invertible consider  regularised version   problem loss function formula   easy  show    algorithm works  formula   iterations proceed  give formula stochastic gradient descent    replaced   formula  formula  becomes  stochastic gradient descent algorithm   case  complexity  formula steps   algorithm reduces  formula  storage requirements  every step formula  constant  formula however  stepsize formula needs   chosen carefully  solve  expected risk minimization problem  detailed   choosing  decaying step size formula one can prove  convergence   average iterate formula  setting   special case  stochastic optimization  well known problem  optimization incremental stochastic gradient descent  practice one can perform multiple stochastic gradient passes also called cycles  epochs   data  algorithm thus obtained  called incremental gradient method  corresponds   iteration  main difference   stochastic gradient method     sequence formula  chosen  decide  training point  visited   formulath step   sequence can  stochastic  deterministic  number  iterations   decoupled   number  points  point can  considered     incremental gradient method can  shown  provide  minimizer   empirical risk incremental techniques can  advantageous  considering objective functions made    sum  many terms eg  empirical error corresponding    large dataset kernel methods kernels can  used  extend   algorithms  nonparametric models  models   parameters form  infinite dimensional space  corresponding procedure will  longer  truly online  instead involve storing   data points   still faster   brute force method  discussion  restricted   case   square loss though  can  extended   convex loss  can  shown   easy induction   formula   data matrix  formula   output  formula steps   sgd algorithm   formula   sequence formula satisfies  recursion notice   formula  just  standard kernel  formula   predictor    form now   general kernel formula  introduced instead  let  predictor     proof will also show  predictor minimising  least squares loss  obtained  changing   recursion    expression requires storing   data  updating formula  total time complexity   recursion  evaluating   formulath datapoint  formula  formula   cost  evaluating  kernel   single pair  points thus  use   kernel  allowed  movement   finite dimensional parameter space formula   possibly infinite dimensional feature represented   kernel formula  instead performing  recursion   space  parameters formula whose dimension      size   training dataset  general    consequence   representer theorem progressive learning progressive learning   effective learning model   demonstrated   human learning process    process  learning continuously  direct experience progressive learning technique plt  machine learning can learn new classeslabels dynamically   run though online learning can learn new samples  data  arrive sequentially   learn new classes  data  introduced   model  learning paradigm  progressive learning  independent   number  class constraints   can learn new classes  still retaining  knowledge  previous classes whenever  new class nonnative   knowledge learnt thus far  encountered  classifier gets remodeled automatically   parameters  calculated    way   retains  knowledge learnt thus far  technique  suitable  realworld applications   number  classes  often unknown  online learning  realtime data  required adversarial models sequential learning  sequential learning  learning problem can  thought    game  two players  learner vs nature   goal   minimize losses regardless   move played    player  game proceeds  follows  formula since  distributional assumptions  made   data  goal    perform  well    entire sequence  examples   viewed ahead  time let formula   hypothesis  achieves  least loss   sequence ie  minimizes formula  can  thought    benchmark  beat  thus    preferable   sequence  functions formula    low loss relative     customary  call   regret   hypothesis set formula thus  sequential learning  learner  trying  minimize  regret   defined   learner  thus required   competitive   best fixed predictor  formula  adversarial models  members   hypothesis set  also called experts   additional constraints  imposed  one can prove covers impossibility result  states     hypothesis set formula     online learning algorithm  regret   least linear  formula however  learning   feasible   like  obtain  sublinear bound   regret    average regret goes  formula  formula one way      add  realisability constraint  states   exists  fixed hypothesis  formula generating  target values   case one can show   regret formula  bounded  formula however realisability  usually  strong   assumption another way  bound  regret   move   setup  online convex optimisation online convex optimisation  online convex optimisation oco  hypothesis set   loss functions  forced   convex  obtain stronger learning bounds  modified sequential game  now  follows  formula thus  regret  minimised   now competing   best weight vector formula   example consider  case  online least squares linear regression   weight vectors come   convex set formula  nature sends back  convex loss function formula note   formula  implicitly sent  formula  online prediction problems however  fit   framework  oco  example  online classification  prediction domain   loss functions   convex   scenarios two simple techniques  convexification  used randomisation  surrogate loss functions  simple online convex optimisation algorithms  follow  leader ftl  simplest learning rule  try   select   current step  hypothesis    least loss   past rounds  algorithm  called follow  leader   simply given round formula   method can thus  looked   greedy algorithm   case  online quadratic optimization   loss function  formula one can show  regret bound  grows  formula however similar bounds   obtained   ftl algorithm   important families  models like online linear optimization    one modifies ftl  adding regularisation follow  regularised leader ftrl    natural modification  ftl   used  stabilise  ftl solutions  obtain better regret bounds  choose  regularisation function formula   perform  learning  round  follows   special example consider  case  online linear optimisation ie  nature sends back loss functions   form formula also let formula suppose  regularisation function formula  chosen   positive number formula  one can show   regret minimising iteration becomes note   can  rewritten  formula  looks exactly like online gradient descent   instead  convex subspace  formula  need   projected onto leading   modified update rule  algorithm  known  lazy projection   vector formula accumulates  gradients   also known  nesterovs dual averaging algorithm   scenario  linear loss functions  quadratic regularisation  regret  bounded  formula  thus  average regret goes   desired online subgradient descent osd   proved  regret bound  linear loss functions formula  generalise  algorithm   convex loss function  subgradient formula  formula  used   linear approximation  formula near formula leading   online subgradient descent algorithm initialise parameter formula  formula one can use  osd algorithm  derive formula regret bounds   online version  svms  classification  use  hinge lossformula  sequential algorithms quadratically regularised ftrl algorithms lead  lazily projected gradient algorithms  described   use    arbitrary convex functions  regularisers one uses online mirror descent another algorithm  called prediction  expert advice   case  hypothesis set consists  formula functions  maintain  distribution formula   formula experts  predict  sampling  expert   distribution   euclidean regularisation one can show  regret bound  formula  can  improved    formula bound  using  better regulariser   reading   algorithms refer  comparison   models  paradigm  online learning interestingly  three distinct interpretations depending   choice   learning model     distinct implications   predictive quality   sequence  functions formula  prototypical stochastic gradient descent algorithm  used   discussion  noted   recursion  given  statistical learning model  first interpretation consider  stochastic gradient descent method  applied   problem  minimizing  expected risk formula defined  indeed   case   infinite stream  data since  examples formula  assumed   drawn iid   distribution formula  sequence  gradients  formula    iteration   iid sample  stochastic estimates   gradient   expected risk formula  therefore one can apply complexity results   stochastic gradient descent method  bound  deviation formula  formula   minimizer  formula  interpretation  also valid   case   finite training set although  multiple passes   data  gradients   longer independent still complexity results can  obtained  special cases  second interpretation applies   case   finite training set  considers  sgd algorithm   instance  incremental gradient descent method   case one instead looks   empirical risk since  gradients  formula   incremental gradient descent iterations  also stochastic estimates   gradient  formula  interpretation  also related   stochastic gradient descent method  applied  minimize  empirical risk  opposed   expected risk since  interpretation concerns  empirical risk    expected risk multiple passes   data  readily allowed  actually lead  tighter bounds   deviations formula  formula   minimizer  formula adversarial model  third interpretation    recursion  distinctly different   first two  concerns  case  sequential trials   data  potentially  iid  can perhaps  selected   adversarial manner since  distributional assumptions  made   data  goal    perform  well    entire sequence  examples   viewed ahead  time   regret    minimised   hypothesis set formula   setting   recursion can  considered   instance   online subgradient descent method     complexity bounds  guarantee formula regret    noted  although  three interpretations   algorithm yield complexity bounds  three distinct settings  bound depends   choice  stepsize sequence formula   different way  thus  consequences   three interpretations   simultaneously applied  stepsize sequence   selected   way   tailored   interpretation    relevant instead furthermore   algorithm   interpretations can  extended   case   nonlinear kernel  simply considering formula    feature space associated   kernel although   case  memory requirements   iteration   longer formula   rather   order   number  data points considered  far\r\n"}
{"index":{"_id":67}}
{"conceptLabelTag":"statistics","conceptLabel":"statistics","conceptDescription":"computational statistics computational statistics  statistical computing   interface  statistics  computer science    area  computational science  scientific computing specific   mathematical science  statistics  area  also developing rapidly leading  calls   broader concept  computing   taught  part  general statistical education  terms computational statistics  statistical computing  often used interchangeably although carlo lauro  former president   international association  statistical computing proposed making  distinction defining statistical computing   application  computer science  statistics  computational statistics  aiming   design  algorithm  implementing statistical methods  computers including  ones unthinkable   computer age eg bootstrap simulation  well   cope  analytically intractable problems sic  term computational statistics may also  used  refer  computationally intensive statistical methods including resampling methods markov chain monte carlo methods local regression kernel density estimation artificial neural networks  generalized additive models\r\n"}
{"index":{"_id":68}}
{"conceptLabelTag":"learning automata","conceptLabel":"learning automata","conceptDescription":"learning automata learning automata  one type  machine learning algorithm studied since s compared   learning scheme  branch   theory  adaptive control  devoted  learning automata surveyed  narendra  thathachar   originally described explicitly  finite state automata learning automata select  current action based  past experiences   environment  will fall   range  reinforcement learning   environment  stochastic  markov decision process mdp  used history research  learning automata can  traced back   work  tsetlin   early s   soviet union together   colleagues  published  collection  papers    use matrices  describe automata functions additionally tsetlin worked  reasonable  collective automata behaviour   automata games learning automata  also investigated  researches   united states   s however  term learning automaton   used  narendra  thathachar introduced    survey paper  definition  learning automaton   adaptive decisionmaking unit situated   random environment  learns  optimal action  repeated interactions   environment  actions  chosen according   specific probability distribution   updated based   environment response  automaton obtains  performing  particular action  respect   field  reinforcement learning learning automata  characterized  policy iterators  contrast   reinforcement learners policy iterators directly manipulate  policy another example  policy iterators  evolutionary algorithms formally narendra  thathachar define  stochastic automaton  consist    paper  investigate  stochastic automata  rs  g  bijective allowing   confuse actions  states  states    automaton correspond   states   discretestate discreteparameter markov process   time step t  automaton reads  input   environment updates pt  pt   randomly chooses  successor state according   probabilities pt  outputs  corresponding action  automatons environment  turn reads  action  sends  next input   automaton frequently  input set x  used   corresponding   nonpenalty   penalty response   environment respectively   case  automaton  learn  minimize  number  penalty responses   feedback loop  automaton  environment  called  pmodel  generally  qmodel allows  arbitrary finite input set x   smodel uses  interval  real numbers  x finite actionset learning automata finite actionset learning automata fala   class  learning automata    number  possible actions  finite    mathematical terms    size   actionset  finite\r\n"}
{"index":{"_id":69}}
{"conceptLabelTag":"nonlinear regression","conceptLabel":"nonlinear regression","conceptDescription":"nonlinear regression  statistics nonlinear regression   form  regression analysis   observational data  modeled   function    nonlinear combination   model parameters  depends  one   independent variables  data  fitted   method  successive approximations general  data consist  error independent variables explanatory variables x   associated observed dependent variables response variables y  y  modeled   random variable   mean given   nonlinear function fx systematic error may  present   treatment  outside  scope  regression analysis   independent variables   error    errorsinvariables model also outside  scope  example  michaelismenten model  enzyme kinetics can  written   formula   parameter formula formula   parameter formula  s   independent variable x  function  nonlinear     expressed   linear combination   two formulas  examples  nonlinear functions include exponential functions logarithmic functions trigonometric functions power functions gaussian function  lorenz curves  functions    exponential  logarithmic functions can  transformed     linear   transformed standard linear regression can  performed  must  applied  caution see linearization    details  general    closedform expression   bestfitting parameters     linear regression usually numerical optimization algorithms  applied  determine  bestfitting parameters   contrast  linear regression  may  many local minima   function   optimized  even  global minimum may produce  biased estimate  practice estimated values   parameters  used  conjunction   optimization algorithm  attempt  find  global minimum   sum  squares  details concerning nonlinear data modeling see least squares  nonlinear least squares regression statistics  assumption underlying  procedure    model can  approximated   linear function  formula  follows     least squares estimators  given   nonlinear regression statistics  computed  used   linear regression statistics  using j  place  x   formulas  linear approximation introduces bias   statistics therefore  caution  usual  required  interpreting statistics derived   nonlinear model ordinary  weighted least squares  bestfit curve  often assumed     minimizes  sum  squared residuals    ordinary least squares ols approach however  cases   dependent variable    constant variance  sum  weighted squared residuals may  minimized see weighted least squares  weight  ideally  equal   reciprocal   variance   observation  weights may  recomputed   iteration   iteratively weighted least squares algorithm linearization transformation  nonlinear regression problems can  moved   linear domain   suitable transformation   model formulation  example consider  nonlinear regression problem  parameters   b   multiplicative error term u   take  logarithm   sides  becomes  u lnu suggesting estimation   unknown parameters   linear regression  lny  x  computation    require iterative optimization however use   nonlinear transformation requires caution  influences   data values will change  will  error structure   model   interpretation   inferential results  may   desired effects    hand depending    largest source  error   nonlinear transformation may distribute  errors   gaussian fashion   choice  perform  nonlinear transformation must  informed  modeling considerations  michaelismenten kinetics  linear lineweaverburk plot  v  s   much used however since    sensitive  data error   strongly biased toward fitting  data   particular range   independent variable s  use  strongly discouraged  error distributions  belong   exponential family  link function may  used  transform  parameters   generalized linear model framework segmentation  independent  explanatory variable say x can  split   classes  segments  linear regression can  performed per segment segmented regression  confidence analysis may yield  result   dependent  response variable say y behaves differently   various segments  figure shows   soil salinity x initially exerts  influence   crop yield y  mustard colza   critical  threshold value breakpoint    yield  affected negatively\r\n"}
{"index":{"_id":70}}
{"conceptLabelTag":"multi task learning","conceptLabel":"multi task learning","conceptDescription":"multitask learning multitask learning mtl   subfield  machine learning   multiple learning tasks  solved    time  exploiting commonalities  differences across tasks  can result  improved learning efficiency  prediction accuracy   taskspecific models  compared  training  models separately   widely cited paper rich caruana gave  following characterizationmultitask learning   approach  inductive transfer  improves generalization  using  domain information contained   training signals  related tasks   inductive bias     learning tasks  parallel  using  shared representation   learned   task can help  tasks  learned better   classification context mtl aims  improve  performance  multiple classification tasks  learning  jointly one example   spamfilter  can  treated  distinct  related classification tasks across different users  make   concrete consider  different people  different distributions  features  distinguish spam emails  legitimate ones  example  english speaker may find   emails  russian  spam    russian speakers yet    definite commonality   classification task across users  example one common feature might  text related  money transfer solving  users spam classification problem jointly via mtl can let  solutions inform    improve performance  examples  settings  mtl include multiclass classification  multilabel classification multitask learning works  regularization induced  requiring  algorithm  perform well   related task can  superior  regularization  prevents overfitting  penalizing  complexity uniformly one situation  mtl may  particularly helpful    tasks share significant commonalities   generally slightly  sampled however  discussed  mtl  also  shown   beneficial  learning unrelated tasks methods task grouping  overlap within  mtl paradigm information can  shared across      tasks depending   structure  task relatedness one may want  share information selectively across  tasks  example tasks may  grouped  exist   hierarchy   related according   general metric suppose  developed  formally    parameter vector modeling  task   linear combination   underlying basis similarity  terms   basis can indicate  relatedness   tasks  example  sparsity overlap  nonzero coefficients across tasks indicates commonality  task grouping  corresponds   tasks lying   subspace generated   subset  basis elements  tasks  different groups may  disjoint  overlap arbitrarily  terms   bases task relatedness can  imposed  priori  learned   data exploiting unrelated tasks one can attempt learning  group  principal tasks using  group  auxiliary tasks unrelated   principal ones  many applications joint learning  unrelated tasks  use   input data can  benecial  reason   prior knowledge  task relatedness can lead  sparser   informative representations   task grouping essentially  screening  idiosyncrasies   data distribution novel methods  builds   prior multitask methodology  favoring  shared lowdimensional representation within  task grouping   proposed  programmer can impose  penalty  tasks  different groups  encourages  two representations   orthogonal experiments  synthetic  real data  indicated  incorporating unrelated tasks can result  significant improvements  standard multitask learning methods transfer  knowledge related  multitask learning   concept  knowledge transfer whereas traditional multitask learning implies   shared representation  developed concurrently across tasks transfer  knowledge implies  sequentially shared representation large scale machine learning projects    deep convolutional neural network googlenet  imagebased object classifier can develop robust representations  may  useful   algorithms learning related tasks  example  pretrained model can  used   feature extractor  perform preprocessing  another learning algorithm   pretrained model can  used  initialize  model  similar architecture    finetuned  learn  different classification task mathematics reproducing hilbert space  vector valued functions rkhsvv  mtl problem can  cast within  context  rkhsvv  complete inner product space  vectorvalued functions equipped   reproducing kernel  particular recent focus    cases  task structure can  identified via  separable kernel described   presentation  derives  ciliberto et al rkhsvv concepts suppose  training data set  formula  formula formula  formula indexes task  formula let formula   setting    consistent input  output space    loss function formula   task  results   regularized machine learning problem  formula   vector valued reproducing kernel hilbert space  functions formula  components formula  reproducing kernel   space formula  functions formula   symmetric matrixvalued function formula   formula   following reproducing property holds separable kernels  form   kernel formula induces   representation   feature space  structures  output across tasks  natural simplification   choose  separable kernel  factors  separate kernels   input space formula    tasks formula   case  kernel relating scalar components formula  formula  given  formula  vector valued functions formula  can write formula  formula   scalar reproducing kernel  formula   symmetric positive semidefinite formula matrix henceforth denote formula  factorization property separability implies  input feature space representation   vary  task      interaction   input kernel   task kernel  structure  tasks  represented solely  formula methods  nonseparable kernels formula   current field  research   separable case  representation theorem  reduced  formula  model output   training data   formula  formula   formula empirical kernel matrix  entries formula  formula   formula matrix  rows formula   separable kernel equation can  rewritten   formula   weighted average  formula applied entrywise  y  kca  weight  zero  formula   missing observation note  second term  can  derived  follows formula formula bilinearity formula reproducing property formula known task structure task structure representations   three largely equivalent ways  represent task structure   regularizer   output metric    output mapping regularizer   separable kernel  can  shown   formula  formula   formula element   pseudoinverse  formula  formula   rkhs based   scalar kernel formula  formula  formulation shows  formula controls  weight   penalty associated  formula note  formula arises  formula proof formula formula formula formula formula formula formula formula formula output metric  alternative output metric  formula can  induced   inner product formula   squared loss    equivalence   separable kernels formula   alternative metric  formula   canonical metric output mapping outputs can  mapped  formula   higher dimensional space  encode complex structures   trees graphs  strings  linear maps formula  appropriate choice  separable kernel  can  shown  formula task structure examples via  regularizer formulation one can represent  variety  task structures easily learning tasks together   structure learning problem can  generalized  admit learning task matrix   follows choice  formula must  designed  learn matrices    given type see special cases  optimization  restricting   case  convex losses  coercive penalties ciliberto et al  shown  although   convex jointly  c    related problem  jointly convex specifically   convex set formula  equivalent problem  convex    minimum value   formula   minimizer   formula   minimizer   perturbation via  barrier formula forces  objective functions   equal  formula   boundary  formula special cases spectral penalties dinnuzo et al suggested setting f   frobenius norm formula  optimized directly using block coordinate descent  accounting  difficulties   boundary  formula clustered tasks learning jacob et al suggested  learn    setting  t tasks  organized  r disjoint clusters   case let formula   matrix  formula setting formula  formula  task matrix formula can  parameterized   function  formula formula  terms  penalize  average  clusters variance  within clusters variance respectively   task predictions m   convex     convex relaxation formula   formulation formula generalizations nonconvex penalties penalties can  constructed     constrained    graph laplacian     low rank factorization however  penalties   convex   analysis   barrier method proposed  ciliberto et al   go    cases nonseparable kernels separable kernels  limited  particular    account  structures   interaction space   input  output domains jointly future work  needed  develop models   kernels applications spam filtering using  principles  mtl techniques  collaborative spam filtering  facilitates personalization   proposed  large scale open membership email systems  users   label enough messages   individual local classifier   effective   data   noisy   used   global filter across  users  hybrid globalindividual classifier can  effective  absorbing  influence  users  label emails  diligently   general public  can  accomplished  still providing sufficient quality  users   labeled instances web search using boosted decision trees one can enable implicit data sharing  regularization  learning method can  used  websearch ranking data sets one example   use ranking data sets  several countries  multitask learning  particularly helpful  data sets  different countries vary largely  size    cost  editorial judgments    demonstrated  learning various tasks jointly can lead  signicant improvements  performance  surprising reliability roboearth  order  facilitate transfer  knowledge  infrastructure   developed one  project roboearth aims  set   open source internet database  can  accessed  continually updated  around  world  goal   facilitate  cloudbased interactive knowledge base accessible  technology companies  academic institutions  can enhance  sensing acting  learning capabilities  robots   artificial intelligence agents software package  multitask learning via structural regularization malsar matlab package implements  following multitask learning algorithms\r\n"}
{"index":{"_id":71}}
{"conceptLabelTag":"case based reasoning","conceptLabel":"case based reasoning","conceptDescription":"casebased reasoning casebased reasoning cbr broadly construed   process  solving new problems based   solutions  similar past problems  auto mechanic  fixes  engine  recalling another car  exhibited similar symptoms  using casebased reasoning  lawyer  advocates  particular outcome   trial based  legal precedents   judge  creates case law  using casebased reasoning    engineer copying working elements  nature practicing biomimicry  treating nature   database  solutions  problems casebased reasoning   prominent kind  analogy making    argued  casebased reasoning     powerful method  computer reasoning  also  pervasive behavior  everyday human problem solving   radically   reasoning  based  past cases personally experienced  view  related  prototype theory    deeply explored  cognitive science process casebased reasoning   formalized  purposes  computer reasoning   fourstep process comparison   methods  first glance cbr may seem similar   rule induction algorithms  machine learning like  ruleinduction algorithm cbr starts   set  cases  training examples  forms generalizations   examples albeit implicit ones  identifying commonalities   retrieved case   target problem   instance  procedure  plain pancakes  mapped  blueberry pancakes  decision  made  use   basic batter  frying method thus implicitly generalizing  set  situations    batter  frying method can  used  key difference however   implicit generalization  cbr   generalization  rule induction lies    generalization  made  ruleinduction algorithm draws  generalizations   set  training examples   target problem  even known    performs eager generalization  instance   ruleinduction algorithm  given recipes  plain pancakes dutch apple pancakes  banana pancakes   training examples     derive  training time  set  general rules  making  types  pancakes      testing time     given say  task  cooking blueberry pancakes  difficulty   ruleinduction algorithm   anticipating  different directions     attempt  generalize  training examples    contrast  cbr  delays implicit generalization   cases  testing time  strategy  lazy generalization   pancake example cbr  already  given  target problem  cooking blueberry pancakes thus  can generalize  cases exactly  needed  cover  situation cbr therefore tends    good approach  rich complex domains     myriad ways  generalize  case criticism critics  cbr argue     approach  accepts anecdotal evidence   main operating principle without statistically relevant data  backing  implicit generalization    guarantee   generalization  correct however  inductive reasoning  data   scarce  statistical relevance  inherently based  anecdotal evidence   recent work  develops cbr within  statistical framework  formalizes casebased inference   specific type  probabilistic inference thus  becomes possible  produce casebased predictions equipped   certain level  confidence history cbr traces  roots   work  roger schank   students  yale university   early s schanks model  dynamic memory   basis   earliest cbr systems janet kolodners cyrus  michael lebowitzs ipp  schools  cbr  closely allied fields emerged   s  directed  topics   legal reasoning memorybased reasoning  way  reasoning  examples  massively parallel machines  combinations  cbr   reasoning methods   s interest  cbr grew internationally  evidenced   establishment   international conference  casebased reasoning   well  european german british italian   cbr workshops cbr technology  resulted   deployment   number  successful systems  earliest  lockheeds clavier  system  laying  composite parts   baked   industrial convection oven cbr   used extensively  help desk applications    compaq smart system   found  major application area   health sciences  earlier version    article  posted  nupedia\r\n"}
{"index":{"_id":72}}
{"conceptLabelTag":"concept learning","conceptLabel":"concept learning","conceptDescription":"concept learning concept learning also known  category learning concept attainment  concept formation  largely based   works   cognitive psychologist jerome bruner bruner goodnow austin defined concept attainment  concept learning   search   listing  attributes  can  used  distinguish exemplars  non exemplars  various categories  simply put concepts   mental categories  help us classify objects events  ideas building   understanding   object event  idea   set  common relevant features thus concept learning   strategy  requires  learner  compare  contrast groups  categories  contain conceptrelevant features  groups  categories    contain conceptrelevant features concept learning also refers   learning task    human  machine learner  trained  classify objects   shown  set  example objects along   class labels  learner simplifies    observed  condensing    form   example  simplified version     learned   applied  future examples concept learning may  simple  complex  learning takes place  many areas   concept  difficult   less likely   learner will  able  simplify  therefore will  less likely  learn colloquially  task  known  learning  examples  theories  concept learning  based   storage  exemplars  avoid summarization  overt abstraction   kind concept learning inferring  booleanvalued function  training examples   input  output  concept   idea  something formed  combining   features  attributes  construct  given concept every concept  two components attributes features  one must look   decide whether  data instance   positive one   concept  rule denotes  conjunction  constraints   attributes will qualify   positive instance   concept types  concepts concept learning must  distinguished  learning  reciting something  memory recall  discriminating  two things  differ discrimination however  issues  closely related since memory recall  facts   considered  trivial conceptual process  prior exemplars representing  concept  invariant similarly  discrimination      initial concept learning discrimination processes  involved  refining concepts  means   repeated presentation  exemplars concrete  perceptual concepts vs abstract concepts defined  relational  associated concepts complex concepts constructs    schema   script  examples  complex concepts  schema   organization  smaller concepts  features   revised  situational information  assist  comprehension  script    hand   list  actions   person follows  order  complete  desired goal  example   script    process  buying  cd   several actions  must occur   actual act  purchasing  cd   script provides  sequence   necessary actions  proper order   actions  order   successful  purchasing  cd methods  learning  concept discovery every baby discovers concepts     discovering     fingers can  individually controlled   care givers  individuals although   perception driven formation   concept    memorizing perceptions examples supervised  unsupervised generalizing  examples may lead  learning  new concept  concept formation    generalizing  examples words hearing  reading new words leads  learning new concepts  forming  new concept    learning  dictionary definition  person may  previously formed  new concept  encountering  word  phrase   exemplars comparison  contrast  efficient way  learn new categories   induce new categorization rules   comparing   example objects   informed   categorical relation comparing two exemplars   informed   two     category allows identifying  attributes shared   category members   exemplifies variability within  category    hand contrasting two exemplars   informed   two   different categories may allow identifying attributes  diagnostic value within category comparison   categories contrast   similarly useful  category learning hammer et al   capacity  use  two forms  comparisonbased learning changes  childhood hammer et al invention  prehistoric people  lacked tools used  fingernails  scrape food  killed animals  smashed melons  noticed   broken stone sometimes   sharp edge like  fingernail   therefore suitable  scraping food inventing  stone tool  avoid broken fingernails   new concept theoretical issues  general  theoretical issues underlying concept learning   underlying induction  issues  addressed  many diverse publications including literature  subjects like version spaces statistical learning theory pac learning information theory  algorithmic information theory    broad theoretical ideas  also discussed  watanabe solomonoff ab  rendell see  reference list  modern psychological theories   difficult  make  general statements  human  animal concept learning without already assuming  particular psychological theory  concept learning although  classical views  concepts  concept learning  philosophy speak   process  abstraction data compression simplification  summarization currently popular psychological theories  concept learning diverge    basic points  history  psychology  seen  rise  fall  many theories  concept learning classical conditioning  defined  pavlov created  earliest experimental technique reinforcement learning  described  watson  elaborated  clark hull created  lasting paradigm  behavioral psychology cognitive psychology emphasized  computer  information flow metaphor  concept formation neural network models  concept formation   structure  knowledge  opened powerful hierarchical models  knowledge organization   george millers wordnet neural networks  based  computational models  learning using factor analysis  convolution neural networks also  open  neuroscience  psychophysiological models  learning following karl lashley  donald hebb rulebased rulebased theories  concept learning began  cognitive psychology  early computer models  learning  might  implemented   high level computer language  computational statements   ifthen production rules  take classification data   rulebased theory  input    result   rulebased learner   hopes  producing   accurate model   data hekenaho  majority  rulebased models    developed  heuristic meaning  rational analyses    provided   models   related  statistical approaches  induction  rational analysis  rulebased models  presume  concepts  represented  rules    ask   degree  belief  rational agent    agreement   rule   observed examples provided goodman griffiths feldman  tenenbaum rulebased theories  concept learning  focused    perceptual learning  less  definition learning rules can  used  learning   stimuli  confusable  opposed  simple  rules  used  learning decisions  made based  properties alone  rely  simple criteria    require  lot  memory rouder  ratcliff example  rulebased theory  radiologist using rulebased categorization  observe whether specific properties   xray image meet certain criteria  example    extreme difference  brightness   suspicious region relative   regions  decision   based   property alone see rouder  ratcliff prototype  prototype view  concept learning holds  people abstract   central tendency  prototype   examples experienced  use    basis   categorization decisions  prototype view  concept learning holds  people categorize based  one   central examples   given category followed   penumbra  decreasingly typical examples  implies  people   categorize based   list  things   correspond   definition  rather   hierarchical inventory based  semantic similarity   central examples  illustrate imagine  following mental representations   category sports  first illustration demonstrates  mental representation     categorize  definition definition  sports  athletic activity requiring skill  physical prowess  often   competitive nature  second illustration demonstrates  mental representation  prototype theory  predict baseball football basketball soccer hockey tennis golf bikeracing weightlifting skateboarding snowboarding boxing wrestling fishing hunting hiking skydiving bungeeing cooking walking gatorade water protein diet   evident  prototype theory hypothesizes   continuous less discrete way  categorization    list  things  match  categorys definition   limited exemplar exemplar theory   storage  specific instances exemplars  new objects evaluated   respect   closely  resemble specific known members  nonmembers   category  theory hypothesizes  learners store examples verbatim  theory views concept learning  highly simplistic  individual properties  represented  individual properties   abstract     create rules  example   exemplar theory might look like  water  wet   simply known    one   stored examples  water   property wet exemplar based theories  become  empirically popular   years   evidence suggesting  human learners use exemplar based strategies   early learning forming prototypes  generalizations later  life  important result  exemplar models  psychology literature    deemphasis  complexity  concept learning one   best known exemplar theories  concept learning   generalized context model gcm  problem  exemplar theory   exemplar models critically depend  two measures similarity  exemplars    rule  determine group membership sometimes   difficult  attain  distinguish  measures multipleprototype  recently cognitive psychologists  begun  explore  idea   prototype  exemplar models form two extremes    suggested  people  able  form  multiple prototype representation besides  two extreme representations  example consider  category spoon   two distinct subgroups  conceptual clusters spoons tend   either large  wooden  small  made  metal  prototypical spoon     mediumsize object made   mixture  metal  wood   clearly  unrealistic proposal   natural representation   category spoon  instead consist  multiple  least two prototypes one   cluster  number  different proposals   made   regard anderson griffiths canini sanborn navarro love medin gureckis vanpaemel storms  models can  regarded  providing  compromise  exemplar  prototype models explanationbased  basic idea  explanationbased learning suggests   new concept  acquired  experiencing examples    forming  basic outline put simply  observing  receiving  qualities   thing  mind forms  concept  possesses   identified   qualities  original theory proposed  mitchell keller  kedarcabelli   called explanationbased generalization   learning occurs  progressive generalizing  theory  first developed  program machines  learn  applied  human cognition  translates  follows  mind actively separates information  applies    one thing  enters    broader description   category  things   done  identifying sufficient conditions  something  fit   category similar  schematizing  revised model revolves around  integration  four mental processes generalization chunking operationalization  analogy  particular theory  concept learning  relatively new   research   conducted  test  bayesian bayes theorem  important   provides  powerful tool  understanding manipulating  controlling data  takes  larger view    limited  data analysis alone  approach  subjective   requires  assessment  prior probabilities making  also  complex however  bayesians show   accumulated evidence   application  bayes law  sufficient  work will overcome  subjectivity   inputs involved bayesian inference can  used   honestly collected data    major advantage    scientific focus one model  incorporates  bayesian theory  concept learning   actr model developed  john r anderson  actr model   programming language  defines  basic cognitive  perceptual operations  enable  human mind  producing  stepbystep simulation  human behavior  theory exploits  idea   task humans perform consists   series  discrete operations  model   applied  learning  memory higher level cognition natural language perception  attention humancomputer interaction education  computer generated forces  addition  john r anderson joshua tenenbaum    contributor   field  concept learning  studied  computational basis  human learning  inference using behavioral testing  adults children  machines  bayesian statistics  probability theory  also  geometry graph theory  linear algebra tenenbaum  working  achieve  better understanding  human learning  computational terms  trying  build computational systems  come closer   capacities  human learners component display theory m d merrills component display theory cdt   cognitive matrix  focuses   interaction  two dimensions  level  performance expected   learner   types  content   material   learned merrill classifies  learners level  performance  find use remember  material content  facts concepts procedures  principles  theory also calls upon four primary presentation forms  several  secondary presentation forms  primary presentation forms include rules examples recall  practice secondary presentation forms include prerequisites objectives helps mnemonics  feedback  complete lesson includes  combination  primary  secondary presentation forms    effective combination varies  learner  learner  also  concept  concept another significant aspect   cdt model    allows   learner  control  instructional strategies used  adapt   meet     learning style  preference  major goal   model   reduce three common errors  concept formation overgeneralization undergeneralization  misconception\r\n"}
{"index":{"_id":73}}
{"conceptLabelTag":"web mining","conceptLabel":"web mining","conceptDescription":"web mining web mining   application  data mining techniques  discover patterns   world wide web web mining can  divided  three different types web usage mining web content mining  web structure mining web usage mining web usage mining   application  data mining techniques  discover interesting usage patterns  web data  order  understand  better serve  needs  webbased applications usage data captures  identity  origin  web users along   browsing behavior   web site web usage mining  can  classified  depending   kind  usage data considered studies related  work weichbroth et al  concerned  two areas constraintbased data mining algorithms applied  web usage mining  developed software tools systems costa  seco demonstrated  web log mining can  used  extract semantic information hyponymy relationships  particular   user   given community pros web usage mining essentially  many advantages  makes  technology attractive  corporations including  government agencies  technology  enabled ecommerce   personalized marketing  eventually results  higher trade volumes government agencies  using  technology  classify threats  fight  terrorism  predicting capability  mining applications can benefit society  identifying criminal activities companies can establish better customer relationship  understanding  needs   customer better  reacting  customer needs faster companies can find attract  retain customers  can save  production costs  utilizing  acquired insight  customer requirements  can increase profitability  target pricing based   profiles created  can even find customers  might default   competitor  company will try  retain  customer  providing promotional offers   specific customer thus reducing  risk  losing  customer  customers cons web usage mining     create issues   technology  used  data  personal nature might cause concerns   criticized ethical issue involving web usage mining   invasion  privacy privacy  considered lost  information concerning  individual  obtained used  disseminated especially   occurs without  knowledge  consent  obtained data will  analyzed  clustered  form profiles  data will  made anonymous  clustering      personal profiles thus  applications deindividualize  users  judging    mouse clicks deindividualization can  defined   tendency  judging  treating people   basis  group characteristics instead     individual characteristics  merits another important concern    companies collecting  data   specific purpose might use  data  totally different purposes   essentially violates  users interests  growing trend  selling personal data   commodity encourages website owners  trade personal data obtained   site  trend  increased  amount  data  captured  traded increasing  likeliness  ones privacy  invaded  companies  buy  data  obliged make  anonymous   companies  considered authors   specific release  mining patterns   legally responsible   contents   release  inaccuracies   release will result  serious lawsuits     law preventing   trading  data  mining algorithms might use controversial attributes like sex race religion  sexual orientation  categorize individuals  practices might    antidiscrimination legislation  applications make  hard  identify  use   controversial attributes     strong rule   usage   algorithms   attributes  process  result  denial  service   privilege   individual based   race religion  sexual orientation right now  situation can  avoided   high ethical standards maintained   data mining company  collected data   made anonymous    obtained data   obtained patterns   traced back   individual  might look    poses  threat  ones privacy however additional information can  inferred   application  combining two separate unscrupulous data   user web structure mining web structure mining  uses graph theory  analyze  node  connection structure   web site according   type  web structural data web structure mining can  divided  two kinds web structure mining terminology techniques  web structure mining web content mining web content mining   mining extraction  integration  useful data information  knowledge  web page content  heterogeneity   lack  structure  permits much   everexpanding information sources   world wide web   hypertext documents makes automated discovery organization  search  indexing tools   internet   world wide web   lycos alta vista webcrawler aliweb metacrawler  others provide  comfort  users     generally provide structural information  categorize filter  interpret documents  factors  prompted researchers  develop  intelligent tools  information retrieval   intelligent web agents  well   extend database  data mining techniques  provide  higher level  organization  semistructured data available   web  agentbased approach  web mining involves  development  sophisticated ai systems  can act autonomously  semiautonomously  behalf   particular user  discover  organize webbased information web content mining  differentiated  two different points  view information retrieval view  database view summarized  research works done  unstructured data  semistructured data  information retrieval view  shows     researches use bag  words   based   statistics  single words  isolation  represent unstructured text  take single word found   training corpus  features   semistructured data   works utilize  html structures inside  documents   utilized  hyperlink structure   documents  document representation    database view  order    better information management  querying   web  mining always tries  infer  structure   web site  transform  web site  become  database   several ways  represent documents vector space model  typically used  documents constitute  whole vector space  representation   realize  importance  words   document  resolve  tfidf term frequency times inverse document frequency  introduced  multiscanning  document  can implement feature selection   condition   category result  rarely affected  extraction  feature subset  needed  general algorithm   construct  evaluating function  evaluate  features  feature set information gain cross entropy mutual information  odds ratio  usually used  classifier  pattern analysis methods  text data mining   similar  traditional data mining techniques  usual evaluative merits  classification accuracy precision  recall  information score web mining   important component  content pipeline  web portals   used  data confirmation  validity verification data integrity  building taxonomies content management content generation  opinion mining web mining  foreign languages chinese    noted   language code  chinese words   complicated compared    english  gb big  hz code  common chinese word codes  web documents  text mining one needs  identify  code standard   html documents  transform   inner code  use  data mining techniques  find useful knowledge  useful patterns\r\n"}
{"index":{"_id":74}}
{"conceptLabelTag":"fitness function","conceptLabel":"fitness function","conceptDescription":"fitness function  fitness function   particular type  objective function   used  summarise   single figure  merit  close  given design solution   achieving  set aims  particular   fields  genetic programming  genetic algorithms  design solution  commonly represented   string  numbers referred    chromosome   round  testing  simulation  idea   delete  n worst design solutions   breed n new ones   best design solutions  design solution therefore needs   awarded  figure  merit  indicate  close  came  meeting  overall specification    generated  applying  fitness function   test  simulation results obtained   solution  reason  genetic algorithms   considered    lazy way  performing design work  precisely    effort involved  designing  workable fitness function even though    longer  human designer   computer  comes    final design    human designer    design  fitness function    designed badly  algorithm will either converge   inappropriate solution  will  difficulty converging   moreover  fitness function must   correlate closely   designers goal  must also  computed quickly speed  execution   important   typical genetic algorithm must  iterated many times  order  produce  usable result   nontrivial problem fitness approximation may  appropriate especially   following cases two main classes  fitness functions exist one   fitness function   change   optimizing  fixed function  testing   fixed set  test cases  one   fitness function  mutable   niche differentiation  coevolving  set  test cases another way  looking  fitness functions   terms   fitness landscape  shows  fitness   possible chromosome definition   fitness function   straightforward  many cases  often  performed iteratively   fittest solutions produced  ga     desired   cases    hard  impossible  come  even   guess   fitness function definition might  interactive genetic algorithms address  difficulty  outsourcing evaluation  external agents normally humans\r\n"}
{"index":{"_id":75}}
{"conceptLabelTag":"ica","conceptLabel":"ica","conceptDescription":"independent component analysis  signal processing independent component analysis ica   computational method  separating  multivariate signal  additive subcomponents   done  assuming   subcomponents  nongaussian signals     statistically independent    ica   special case  blind source separation  common example application   cocktail party problem  listening   one persons speech   noisy room introduction independent component analysis attempts  decompose  multivariate signal  independent nongaussian signals   example sound  usually  signal   composed   numerical addition   time t  signals  several sources  question   whether   possible  separate  contributing sources   observed total signal   statistical independence assumption  correct blind ica separation   mixed signal gives  good results   also used  signals    supposed   generated   mixing  analysis purposes  simple application  ica   cocktail party problem   underlying speech signals  separated   sample data consisting  people talking simultaneously   room usually  problem  simplified  assuming  time delays  echoes  important note  consider    n sources  present  least n observations eg microphones  needed  recover  original signals  constitutes  square case j d  d   input dimension   data  j   dimension   model  cases  underdetermined j d  overdetermined j d   investigated   ica separation  mixed signals gives  good results  based  two assumptions  three effects  mixing source signals two assumptions three effects  mixing source signals  principles contribute   basic establishment  ica   signals  happen  extract   set  mixtures  independent like sources signals   nongaussian histograms like source signals   low complexity like source signals   must  source signals defining component independence ica finds  independent components also called factors latent variables  sources  maximizing  statistical independence   estimated components  may choose one  many ways  define  proxy  independence   choice governs  form   ica algorithm  two broadest definitions  independence  ica   minimizationofmutual information mmi family  ica algorithms uses measures like kullbackleibler divergence  maximum entropy  nongaussianity family  ica algorithms motivated   central limit theorem uses kurtosis  negentropy typical algorithms  ica use centering subtract  mean  create  zero mean signal whitening usually   eigenvalue decomposition  dimensionality reduction  preprocessing steps  order  simplify  reduce  complexity   problem   actual iterative algorithm whitening  dimension reduction can  achieved  principal component analysis  singular value decomposition whitening ensures   dimensions  treated equally  priori   algorithm  run wellknown algorithms  ica include infomax fastica jade  kernelindependent component analysis among others  general ica  identify  actual number  source signals  uniquely correct ordering   source signals   proper scaling including sign   source signals ica  important  blind signal separation   many practical applications   closely related   even  special case   search   factorial code   data ie  new vectorvalued representation   data vector    gets uniquely encoded   resulting code vector loss coding   code components  statistically independent mathematical definitions linear independent component analysis can  divided  noiseless  noisy cases  noiseless ica   special case  noisy ica nonlinear ica   considered   separate case general definition  data  represented   random vector formula   components   random vector formula  task   transform  observed data formula using  linear static transformation w  formula  maximally independent components formula measured   function formula  independence generative model linear noiseless ica  components formula   observed random vector formula  generated   sum   independent components formula formula formula weighted   mixing weights formula   generative model can  written  vectorial form  formula   observed random vector formula  represented   basis vectors formula  basis vectors formula form  columns   mixing matrix formula   generative formula can  written  formula  formula given  model  realizations samples formula   random vector formula  task   estimate   mixing matrix formula   sources formula   done  adaptively calculating  formula vectors  setting   cost function  either maximizes  nongaussianity   calculated formula  minimizes  mutual information   cases  priori knowledge   probability distributions   sources can  used   cost function  original sources formula can  recovered  multiplying  observed signals formula   inverse   mixing matrix formula also known   unmixing matrix    assumed   mixing matrix  square formula   number  basis vectors  greater   dimensionality   observed vectors formula  task  overcomplete   still solvable   pseudo inverse linear noisy ica   added assumption  zeromean  uncorrelated gaussian noise formula  ica model takes  form formula nonlinear ica  mixing   sources   need   linear using  nonlinear mixing function formula  parameters formula  nonlinear ica model  formula identifiability  independent components  identifiable    permutation  scaling   sources  identifiability requires  binary independent component analysis  special variant  ica  binary ica    signal sources  monitors   binary form  observations  monitors  disjunctive mixtures  binary independent sources  problem  shown   applications  many domains including medical diagnosis multicluster assignment network tomography  internet resource management let formula   set  binary variables  formula monitors  formula   set  binary variables  formula sources sourcemonitor connections  represented   unknown mixing matrix formula  formula indicates  signal   ith source can  observed   jth monitor  system works  follows   time   source formula  active formula    connected   monitor formula formula   monitor formula will observe  activity formula formally    formula  boolean   formula  boolean  note  noise   explicitly modeled rather can  treated  independent sources   problem can  heuristically solved  assuming variables  continuous  running fastica  binary observation data  get  mixing matrix formula real values  apply round number techniques  formula  obtain  binary values  approach   shown  produce  highly inaccurate result another method   use dynamic programming recursively breaking  observation matrix formula   submatrices  run  inference algorithm   submatrices  key observation  leads   algorithm   submatrix formula  formula  formula corresponds   unbiased observation matrix  hidden components     connection   formulath monitor experimental results  show   approach  accurate  moderate noise levels  generalized binary ica framework introduces  broader problem formulation    necessitate  knowledge   generative model   words  method attempts  decompose  source   independent components  much  possible  without losing  information   prior assumption   way   generated although  problem appears quite complex  can  accurately solved   branch  bound search tree algorithm  tightly upper bounded   single multiplication   matrix   vector methods  blind source separation projection pursuit signal mixtures tend   gaussian probability density functions  source signals tend   nongaussian probability density functions  source signal can  extracted   set  signal mixtures  taking  inner product   weight vector   signal mixtures   inner product provides  orthogonal projection   signal mixtures  remaining challenge  finding   weight vector one type  method     projection pursuit projection pursuit seeks one projection   time    extracted signal   nongaussian  possible  contrasts  ica  typically extracts m signals simultaneously  m signal mixtures  requires estimating  m m unmixing matrix one practical advantage  projection pursuit  ica   fewer  m signals can  extracted  required   source signal  extracted  m signal mixtures using  melement weight vector  can use kurtosis  recover  multiple source signal  finding  correct weight vectors   use  projection pursuit  kurtosis   probability density function   signal   finite sample  computed   formula   sample mean  formula  extracted signals  constant ensures  gaussian signals  zero kurtosis supergaussian signals  positive kurtosis  subgaussian signals  negative kurtosis  denominator   variance  formula  ensures   measured kurtosis takes account  signal variance  goal  projection pursuit   maximize  kurtosis  make  extracted signal  nonnormal  possible using kurtosis   measure  nonnormality  can now examine   kurtosis   signal formula extracted   set  m mixtures formula varies   weight vector formula  rotated around  origin given  assumption   source signal formula  supergaussian   expect  multiple source mixture signals  can use kurtosis  gramschmidt orthogonalization gso  recover  signals given m signal mixtures   mdimensional space gso project  data points onto  mdimensional space  using  weight vector  can guarantee  independence   extracted signals   use  gso  order  find  correct value  formula  can use gradient descent method  first   whiten  data  transform formula   new mixture formula   unit variance  formula  process can  achieved  applying singular value decomposition  formula rescaling  vector formula  let formula  signal extracted   weighted vector formula  formula   weight vector w  unit length   formula   kurtosis can  written   updating process  formula   formula   small constant  guarantee  formula converge   optimal solution   update  normalized formula  set formula  repeat  updating process till  converges  can also use another algorithm  update  weight vector formula another approach  using negentropy instead  kurtosis negentropy   robust method  kurtosis  kurtosis   sensitive  outliers  negentropy method  based   important property  gaussian distribution  gaussian variable   largest entropy among  random variables  equal variance   also  reason   want  find   nongaussian variables  simple proof can  found  differential entropy y   gaussian random variable    covariance matrix  x  approximation  negentropy   proof can  found  page   book independent component analysis written  aapo hyv rinen juha karhunen  erkki oja  contribute great works  ica  approximation also suffers   problem  kurtosis sensitive  outliers  approaches  developed  choice  formula  formula  based  infomax ica  essentially  multivariate parallel version  projection pursuit whereas projection pursuit extracts  series  signals one   time   set  m signal mixtures ica extracts m signals  parallel  tends  make ica  robust  projection pursuit  projection pursuit method uses gramschmidt orthogonalizaton  ensure  independence   extracted signal  ica use infomax  maximum likelihood estimate  ensure  independence   extracted signal  nonnormality   extracted signal  achieved  assigning  appropriate model  prior   signal  process  ica based  infomax  short  given  set  signal mixtures formula   set  identical independent model cumulative distribution functionscdfs formula  seek  unmixing matrix formula  maximizes  joint entropy   signals formula  formula   signals extracted  formula given  optimal formula  signals formula  maximum entropy   therefore independent  ensures   extracted signals formula  also independent formula   invertible function    signal model note    source signal model probability density function formula matches  probability density function   extracted signal formula  maximizing  joint entropy  formula also maximizes  amount  mutual information  formula  formula   reason using entropy  extract independent signals  known  infomax consider  entropy   vector variable formula  formula   set  signals extracted   unmixing matrix formula   finite set  values sampled   distribution  pdf formula  entropy  formula can  estimated   joint pdf formula can  shown   related   joint pdf formula   extracted signals   multivariate form  formula   jacobian matrix   formula  formula   pdf assumed  source signals formula therefore therefore  know   formula formula   uniform distribution  formula  maximized since  formula   absolute value   determinant   unmixing matix formula therefore  since formula  maximizing formula   affect formula   can maximize  function  achieve  independence  extracted signal    m marginal pdfs   model joint pdf formula  independent  use  commonly supergaussian model pdf   source signals formula      sum given  observed signal mixture formula  corresponding set  extracted signals formula  source signal model formula  can find  optimal unmixing matrix formula  make  extracted signals independent  nongaussian like  projection pursuit situation  can use gradient descent method  find  optimal solution   unmixing matrix based  maximum likelihood estimation maximum likelihood estimation mle   standard statistical tool  finding parameter values eg  unmixing matrix formula  provide  best fit   data eg  extracted signals formula   given  model eg  assumed joint probability density function pdf formula  source signals  ml model includes  specification   pdf    case   pdf formula   unknown source signals formula using ml ica  objective   find  unmixing matrix  yields extracted signals formula   joint pdf  similar  possible   joint pdf formula   unknown source signals formula mle  thus based   assumption    model pdf formula   model parameters formula  correct   high probability   obtained   data formula   actually observed conversely  formula  far   correct parameter values   low probability   observed data   expected using mle  call  probability   observed data   given set  model parameter values eg  pdf formula   matrix formula  likelihood   model parameter values given  observed data  define  likelihood function formula  formula formula  equals   probability density  formula since formula thus   wish  find  formula    likely   generated  observed mixtures formula   unknown source signals formula  pdf formula   need  find  formula  maximizes  likelihood formula  unmixing matrix  maximizes equation  known   mle   optimal unmixing matrix   common practice  use  log likelihood    easier  evaluate   logarithm   monotonic function  formula  maximizes  function formula also maximizes  logarithm formula  allows us  take  logarithm  equation   yields  log likelihood function formula   substitute  commonly used highkurtosis model pdf   source signals formula    formula  matrix formula  maximizes  function   maximum likelihood estimation history  background  early general framework  independent component analysis  introduced  jeanny h rault  bernard ans   rejoined  christian jutten     clearly stated  pierre comon   tony bell  terry sejnowski introduced  fast  efficient ica algorithm based  infomax  principle introduced  ralph linsker    many algorithms available   literature   ica  largely used one including  industrial applications   fastica algorithm developed  aapo hyv rinen  erkki oja  uses  kurtosis  cost function  examples  rather related  blind source separation    general approach  used  example one can drop  independence assumption  separate mutually correlated signals thus statistically dependent signals sepp hochreiter  j rgen schmidhuber showed   obtain nonlinear ica  source separation   byproduct  regularization  method   require  priori knowledge   number  independent sources applications ica can  extended  analyze nonphysical signals  instance ica   applied  discover discussion topics   bag  news list archives  ica applications  listed \r\n"}
{"index":{"_id":76}}
{"conceptLabelTag":"mixture model","conceptLabel":"mixture model","conceptDescription":"mixture model  statistics  mixture model   probabilistic model  representing  presence  subpopulations within  overall population without requiring   observed data set  identify  subpopulation    individual observation belongs formally  mixture model corresponds   mixture distribution  represents  probability distribution  observations   overall population however  problems associated  mixture distributions relate  deriving  properties   overall population     subpopulations mixture models  used  make statistical inferences   properties   subpopulations given  observations   pooled population without subpopulation identity information  ways  implementing mixture models involve steps  attribute postulated subpopulationidentities  individual observations  weights towards  subpopulations   case  can  regarded  types  unsupervised learning  clustering procedures however   inference procedures involve  steps mixture models    confused  models  compositional data ie data whose components  constrained  sum   constant value etc however compositional models can  thought   mixture models  members   population  sampled  random conversely mixture models can  thought   compositional models   total size   population   normalized  structure   mixture model general mixture model  typical finitedimensional mixture model   hierarchical model consisting   following components  addition   bayesian setting  mixture weights  parameters will   random variables  prior distributions will  placed   variables    case  weights  typically viewed   kdimensional random vector drawn   dirichlet distribution  conjugate prior   categorical distribution   parameters will  distributed according   respective conjugate priors mathematically  basic parametric mixture model can  described  follows   bayesian setting  parameters  associated  random variables  follows  characterization uses f  h  describe arbitrary distributions  observations  parameters respectively typically h will   conjugate prior  f  two  common choices  f  gaussian aka normal  realvalued observations  categorical  discrete observations  common possibilities   distribution   mixture components  specific examples gaussian mixture model  typical nonbayesian gaussian mixture model looks like   bayesian version   gaussian mixture model   follows multivariate gaussian mixture model  bayesian gaussian mixture model  commonly extended  fit  vector  unknown parameters denoted  bold  multivariate normal distributions   multivariate distribution ie one modelling  vector formula  n random variables one may model  vector  parameters   several observations   signal  patches within  image using  gaussian mixture model prior distribution   vector  estimates given     vector component  characterized  normal distributions  weights formula means formula  covariance matrices formula  incorporate  prior   bayesian estimation  prior  multiplied   known distribution formula   data formula conditioned   parameters formula   estimated   formulation  posterior distribution formula  also  gaussian mixture model   form  new parameters formula  formula   updated using  em algorithm  distributions  useful  assuming patchwise shapes  images  clusters  example   case  image representation  gaussian may  tilted expanded  warped according   covariance matrices formula one gaussian distribution   set  fit   patch usually  size x pixels   image notably  distribution  points around  cluster see kmeans may  accurately given enough gaussian components  scarcely  k components  needed  accurately model  given image distribution  cluster  data categorical mixture model  typical nonbayesian mixture model  categorical observations looks like   random variables  typical bayesian mixture model  categorical observations looks like   random variables examples  financial model financial returns often behave differently  normal situations   crisis times  mixture model  return data seems reasonable sometimes  model used   diffusion model    mixture  two normal distributions see financial economicschallenges  criticism   context house prices assume   observe  prices  n different houses different types  houses  different neighborhoods will  vastly different prices   price   particular type  house   particular neighborhood eg threebedroom house  moderately upscale neighborhood will tend  cluster fairly closely around  mean one possible model   prices    assume   prices  accurately described   mixture model  k different components  distributed   normal distribution  unknown mean  variance   component specifying  particular combination  house typeneighborhood fitting  model  observed prices eg using  expectationmaximization algorithm  tend  cluster  prices according  house typeneighborhood  reveal  spread  prices   typeneighborhood note   values   prices  incomes   guaranteed   positive   tend  grow exponentially  lognormal distribution might actually   better model   normal distribution topics   document assume   document  composed  n different words   total vocabulary  size v   word corresponds  one  k possible topics  distribution   words   modelled   mixture  k different vdimensional categorical distributions  model   sort  commonly termed  topic model note  expectation maximization applied    model will typically fail  produce realistic results due among  things   excessive number  parameters  sorts  additional assumptions  typically necessary  get good results typically two sorts  additional components  added   model handwriting recognition  following example  based   example  christopher m bishop pattern recognition  machine learning imagine    given  nn blackandwhite image   known    scan   handwritten digit     dont know  digit  written  can create  mixture model  formula different components   component   vector  size formula  bernoulli distributions one per pixel   model can  trained   expectationmaximization algorithm   unlabeled set  handwritten digits  will effectively cluster  images according   digit  written   model    used  recognize  digit  another image simply  holding  parameters constant computing  probability   new image   possible digit  trivial calculation  returning  digit  generated  highest probability assessing projectile accuracy aka circular error probable cep mixture models apply   problem  directing multiple projectiles   target   air land  sea defense applications   physical andor statistical characteristics   projectiles differ within  multiple projectiles  example might  shots  multiple munitions types  shots  multiple locations directed  one target  combination  projectile types may  characterized   gaussian mixture model   wellknown measure  accuracy   group  projectiles   circular error probable cep    number r    average half   group  projectiles falls within  circle  radius r   target point  mixture model can  used  determine  estimate  value r  mixture model properly captures  different types  projectiles direct  indirect applications  financial example   one direct application   mixture model  situation    assume  underlying mechanism    observation belongs  one   number  different sources  categories  underlying mechanism may  may  however  observable   form  mixture    sources  described   component probability density function   mixture weight   probability   observation comes   component   indirect application   mixture model    assume   mechanism  mixture model  simply used   mathematical flexibilities  example  mixture  two normal distributions  different means may result   density  two modes    modeled  standard parametric distributions another example  given   possibility  mixture distributions  model fatter tails   basic gaussian ones      candidate  modeling  extreme events  combined  dynamical consistency  approach   applied  financial derivatives valuation  presence   volatility smile   context  local volatility models  defines  application fuzzy image segmentation  image processing  computer vision traditional image segmentation models often assign  one pixel  one exclusive pattern  fuzzy  soft segmentation  pattern can  certain ownership   single pixel   patterns  gaussian fuzzy segmentation naturally results  gaussian mixtures combined   analytic  geometric tools eg phase transitions  diffusive boundaries  spatially regularized mixture models  lead   realistic  computationally efficient segmentation methods identifiability identifiability refers   existence   unique characterization   one   models   class family  considered estimation procedures may   welldefined  asymptotic theory may  hold   model   identifiable example let j   class   binomial distributions    mixture  two members  j    clearly given p  p    possible  determine   mixture model uniquely    three parameters   determined definition consider  mixture  parametric distributions    class let   class   component distributions   convex hull k  j defines  class   finite mixture  distributions  j k  said   identifiable    members  unique   given two members p   k  mixtures  k distributions  distributions respectively  j       first    secondly  can reorder  summations       parameter estimation  system identification parametric mixture models  often used   know  distribution y   can sample  x    like  determine    values  situations can arise  studies    sample   population   composed  several distinct subpopulations   common  think  probability mixture modeling   missing data problem one way  understand    assume   data points  consideration  membership  one   distributions   using  model  data   start  membership  unknown  missing  job  estimation   devise appropriate parameters   model functions  choose   connection   data points  represented   membership   individual model distributions  variety  approaches   problem  mixture decomposition   proposed many   focus  maximum likelihood methods   expectation maximization em  maximum  posteriori estimation map generally  methods consider separately  question  parameter estimation  system identification    say  distinction  made   determination   number  functional form  components within  mixture   estimation   corresponding parameter values  notable departures   graphical methods  outlined  tarter  lock   recently minimum message length mml techniques   figueiredo  jain    extent  moment matching pattern analysis routines suggested  mcwilliam  loh expectation maximization em expectation maximization em  seemingly   popular technique used  determine  parameters   mixture    priori given number  components    particular way  implementing maximum likelihood estimation   problem em   particular appeal  finite normal mixtures  closedform expressions  possible     following iterative algorithm  dempster et al   posterior probabilities thus   basis   current estimate   parameters  conditional probability   given observation x  generated  state s  determined   n   sample size  parameters   updated    new component weights correspond   average conditional probability   component mean  covariance   component specific weighted average   mean  covariance   entire sample dempster also showed   successive em iteration will  decrease  likelihood  property  shared   gradient based maximization techniques moreover em naturally embeds within  constraints   probability vector   sufficiently large sample sizes positive definiteness   covariance iterates    key advantage since explicitly constrained methods incur extra computational costs  check  maintain appropriate values theoretically em   firstorder algorithm    converges slowly   fixedpoint solution redner  walker make  point arguing  favour  superlinear  second order newton  quasinewton methods  reporting slow convergence  em   basis   empirical tests   concede  convergence  likelihood  rapid even  convergence   parameter values     relative merits  em   algorithms vis vis convergence   discussed   literature  common objections   use  em      propensity  spuriously identify local maxima  well  displaying sensitivity  initial values one may address  problems  evaluating em  several initial points   parameter space    computationally costly   approaches    annealing em method  udea  nakano    initial components  essentially forced  overlap providing  less heterogeneous basis  initial guesses may  preferable figueiredo  jain note  convergence  meaningless parameter values obtained   boundary  regularity conditions breakdown eg ghosh  sen  frequently observed   number  model components exceeds  optimaltrue one   basis  suggest  unified approach  estimation  identification    initial n  chosen  greatly exceed  expected optimal value  optimization routine  constructed via  minimum message length mml criterion  effectively eliminates  candidate component    insufficient information  support    way   possible  systematize reductions  n  consider estimation  identification jointly  expectationmaximization algorithm can  used  compute  parameters   parametric mixture model distribution       iterative algorithm  two steps  expectation step   maximization step practical examples  em  mixture modeling  included   socr demonstrations  expectation step  initial guesses   parameters   mixture model partial membership   data point   constituent distribution  computed  calculating expectation values   membership variables   data point     data point x  distribution y  membership value y   maximization step  expectation values  hand  group membership plugin estimates  recomputed   distribution parameters  mixing coefficients    means   membership values   n data points  component model parameters  also calculated  expectation maximization using data points x    weighted using  membership values  example    mean  new estimates     s  expectation step  repeated  recompute new membership values  entire procedure  repeated  model parameters converge markov chain monte carlo   alternative   em algorithm  mixture model parameters can  deduced using posterior sampling  indicated  bayes theorem   still regarded   incomplete data problem whereby membership  data points   missing data  twostep iterative procedure known  gibbs sampling can  used  previous example   mixture  two gaussian distributions can demonstrate   method works   initial guesses   parameters   mixture model  made instead  computing partial memberships   elemental distribution  membership value   data point  drawn   bernoulli distribution    will  assigned  either  first   second gaussian  bernoulli parameter  determined   data point   basis  one   constituent distributions draws   distribution generate membership associations   data point plugin estimators can   used    m step  em  generate  new set  mixture model parameters   binomial draw step repeated moment matching  method  moment matching  one   oldest techniques  determining  mixture parameters dating back  karl pearsons seminal work    approach  parameters   mixture  determined    composite distribution  moments matching  given value  many instances extraction  solutions   moment equations may present nontrivial algebraic  computational problems moreover numerical analysis  day  indicated   methods may  inefficient compared  em nonetheless    renewed interest   method eg craigmile  titterington  wang mcwilliam  loh consider  characterisation   hypercuboid normal mixture copula  large dimensional systems   em   computationally prohibitive   pattern analysis routine  used  generate multivariate taildependencies consistent   set  univariate    sense bivariate moments  performance   method   evaluated using equity logreturn data  kolmogorovsmirnov test statistics suggesting  good descriptive fit spectral method  problems  mixture model estimation can  solved using spectral methods  particular  becomes useful  data points x  points  highdimensional real space   hidden distributions  known   logconcave   gaussian distribution  exponential distribution spectral methods  learning mixture models  based   use  singular value decomposition   matrix  contains data points  idea   consider  top k singular vectors  k   number  distributions   learned  projection   data point   linear subspace spanned   vectors groups points originating    distribution  close together  points  different distributions stay far apart one distinctive feature   spectral method    allows us  prove   distributions satisfy certain separation condition eg   close   estimated mixture will   close   true one  high probability graphical methods tarter  lock describe  graphical approach  mixture identification    kernel function  applied   empirical frequency plot   reduce intracomponent variance   way one may  readily identify components  differing means   method   require prior knowledge   number  functional form   components  success  rely   choice   kernel parameters    extent implicitly embeds assumptions   component structure  methods    can even probably learn mixtures  heavytailed distributions including   infinite variance see links  papers    setting em based methods   work since  expectation step  diverge due  presence  outliers  simulation  simulate  sample  size n     mixture  distributions f   n  probabilities p sum p extensions   bayesian setting additional levels can  added   graphical model defining  mixture model  example   common latent dirichlet allocation topic model  observations  sets  words drawn  d different documents   k mixture components represent topics   shared across documents  document   different set  mixture weights  specify  topics prevalent   document  sets  mixture weights share common hyperparameters   common extension   connect  latent variables defining  mixture component identities   markov chain instead  assuming    independent identically distributed random variables  resulting model  termed  hidden markov model   one    common sequential hierarchical models numerous extensions  hidden markov models   developed see  resulting article   information history mixture distributions   problem  mixture decomposition    identification   constituent components   parameters thereof   cited   literature  far back  quetelet  mclachlan although common reference  made   work  karl pearson   first author  explicitly address  decomposition problem  characterising nonnormal attributes  forehead  body length ratios  female shore crab populations  motivation   work  provided   zoologist walter frank raphael weldon   speculated   tarter  lock  asymmetry   histogram   ratios  signal evolutionary divergence pearsons approach   fit  univariate mixture  two normals   data  choosing  five parameters   mixture    empirical moments matched    model   work  successful  identifying two potentially distinct subpopulations   demonstrating  flexibility  mixtures   moment matching tool  formulation required  solution   th degree nonic polynomial    time posed  significant computational challenge subsequent works focused  addressing  problems       advent   modern computer   popularisation  maximum likelihood mle parameterisation techniques  research really took  since  time     vast body  research   subject spanning areas   fisheries research agriculture botany economics medicine genetics psychology palaeontology electrophoresis finance sedimentologygeology  zoology\r\n"}
{"index":{"_id":77}}
{"conceptLabelTag":"outlier detection","conceptLabel":"outlier detection","conceptDescription":"anomaly detection  data mining anomaly detection also outlier detection   identification  items events  observations    conform   expected pattern   items   dataset typically  anomalous items will translate   kind  problem   bank fraud  structural defect medical problems  errors   text anomalies  also referred   outliers novelties noise deviations  exceptions  particular   context  abuse  network intrusion detection  interesting objects  often  rare objects  unexpected bursts  activity  pattern   adhere   common statistical definition   outlier   rare object  many outlier detection methods  particular unsupervised methods will fail   data unless    aggregated appropriately instead  cluster analysis algorithm may  able  detect  micro clusters formed   patterns three broad categories  anomaly detection techniques exist unsupervised anomaly detection techniques detect anomalies   unlabeled test data set   assumption   majority   instances   data set  normal  looking  instances  seem  fit least   remainder   data set supervised anomaly detection techniques require  data set    labeled  normal  abnormal  involves training  classifier  key difference  many  statistical classification problems   inherent unbalanced nature  outlier detection semisupervised anomaly detection techniques construct  model representing normal behavior   given normal training data set   testing  likelihood   test instance   generated   learnt model applications anomaly detection  applicable   variety  domains   intrusion detection fraud detection fault detection system health monitoring event detection  sensor networks  detecting ecosystem disturbances   often used  preprocessing  remove anomalous data   dataset  supervised learning removing  anomalous data   dataset often results   statistically significant increase  accuracy popular techniques several anomaly detection techniques   proposed  literature    popular techniques   performance  different methods depends  lot   data set  parameters  methods  little systematic advantages  another  compared across many data sets  parameters application  data security anomaly detection  proposed  intrusion detection systems ids  dorothy denning  anomaly detection  ids  normally accomplished  thresholds  statistics  can also  done  soft computing  inductive learning types  statistics proposed  included profiles  users workstations networks remote hosts groups  users  programs based  frequencies means variances covariances  standard deviations  counterpart  anomaly detection  intrusion detection  misuse detection\r\n"}
{"index":{"_id":78}}
{"conceptLabelTag":"decision tree learning","conceptLabel":"decision tree learning","conceptDescription":"decision tree learning decision tree learning uses  decision tree   predictive model  maps observations   item represented   branches  conclusions   items target value represented   leaves   one   predictive modelling approaches used  statistics data mining  machine learning tree models   target variable can take  finite set  values  called classification trees   tree structures leaves represent class labels  branches represent conjunctions  features  lead   class labels decision trees   target variable can take continuous values typically real numbers  called regression trees  decision analysis  decision tree can  used  visually  explicitly represent decisions  decision making  data mining  decision tree describes data   resulting classification tree can   input  decision making  page deals  decision trees  data mining general decision tree learning   method commonly used  data mining  goal   create  model  predicts  value   target variable based  several input variables  example  shown   diagram  right  interior node corresponds  one   input variables   edges  children     possible values   input variable  leaf represents  value   target variable given  values   input variables represented   path   root   leaf  decision tree   simple representation  classifying examples   section assume     input features  finite discrete domains     single target feature called  classification  element   domain   classification  called  class  decision tree   classification tree   tree    internal nonleaf node  labeled   input feature  arcs coming   node labeled   input feature  labeled     possible values   target  output feature   arc leads   subordinate decision node   different input feature  leaf   tree  labeled   class   probability distribution   classes  tree can  learned  splitting  source set  subsets based   attribute value test  process  repeated   derived subset   recursive manner called recursive partitioning see  examples illustrated   figure  spaces       partitioned using recursive partitioning  recursive binary splitting  recursion  completed   subset   node     value   target variable   splitting  longer adds value   predictions  process  topdown induction  decision trees tdidt   example   greedy algorithm     far   common strategy  learning decision trees  data  data mining decision trees can  described also   combination  mathematical  computational techniques  aid  description categorization  generalization   given set  data data comes  records   form  dependent variable y   target variable    trying  understand classify  generalize  vector x  composed   input variables x x x etc   used   task types decision trees used  data mining   two main types  term classification  regression tree cart analysis   umbrella term used  refer      procedures first introduced  breiman et al trees used  regression  trees used  classification   similarities  also  differences    procedure used  determine   split  techniques often called ensemble methods construct   one decision tree  special case   decision tree   decision list    onesided decision tree   every internal node  exactly leaf node  exactly internal node   child except   bottommost node whose  child   single leaf node  less expressive decision lists  arguably easier  understand  general decision trees due   added sparsity permit nongreedy learning methods  monotonic constraints   imposed decision tree learning   construction   decision tree  classlabeled training tuples  decision tree   flowchartlike structure   internal nonleaf node denotes  test   attribute  branch represents  outcome   test   leaf  terminal node holds  class label  topmost node   tree   root node   many specific decisiontree algorithms notable ones include id  cart  invented independently  around   time   yet follow  similar approach  learning decision tree  training tuples metrics algorithms  constructing decision trees usually work topdown  choosing  variable   step  best splits  set  items different algorithms use different metrics  measuring best  generally measure  homogeneity   target variable within  subsets  examples  given   metrics  applied   candidate subset   resulting values  combined eg averaged  provide  measure   quality   split gini impurity used   cart classification  regression tree algorithm gini impurity   measure   often  randomly chosen element   set   incorrectly labeled    randomly labeled according   distribution  labels   subset gini impurity can  computed  summing  probability formula   item  label formula  chosen times  probability formula   mistake  categorizing  item  reaches  minimum zero   cases   node fall   single target category  compute gini impurity   set  items  formula classes suppose formula  let formula   fraction  items labeled  class formula   set information gain used   id c  c treegeneration algorithms information gain  based   concept  entropy  information theory entropy  defined   information gain entropyparent weighted sum  entropychildren formula information gain  used  decide  feature  split    step  building  tree simplicity  best   want  keep  tree small      step   choose  split  results   purest daughter nodes  commonly used measure  purity  called information   measured  bits    confused   unit  computer memory   node   tree  information value represents  expected amount  information    needed  specify whether  new instance   classified yes   given   example reached  node consider  example data set  four attributes outlook sunny overcast rainy temperature hot mild cool humidity high normal  windy true false   binary yes   target variable play  data points  construct  decision tree   data  need  compare  information gain    four trees  split  one   four features  split   highest information gain will  taken   first split   process will continue   children nodes  pure    information gain   split using  feature windy results  two children nodes one   windy value  true  one   windy value  false   data set   six data points   true windy value three     play value  yes  three   play value    eight remaining data points   windy value  false contain two nos  six yess  information   windytrue node  calculated using  entropy equation  since    equal number  yess  nos   node     node  windyfalse   eight data points six yess  two nos thus    find  information   split  take  weighted average   two numbers based   many observations fell   node  find  information gain   split using windy  must first calculate  information   data   split  original data contained nine yess  five nos now  can calculate  information gain achieved  splitting   windy feature  build  tree  information gain   possible first split  need   calculated  best first split   one  provides   information gain  process  repeated   impure node   tree  complete  example  adapted   example appearing  witten et al variance reduction introduced  cart variance reduction  often employed  cases   target variable  continuous regression tree meaning  use  many  metrics  first require discretization   applied  variance reduction   node  defined   total reduction   variance   target variable due   split   node  formula formula  formula   set  presplit sample indices set  sample indices    split test  true  set  sample indices    split test  false respectively     summands  indeed variance estimates though written   form without directly referring   mean decision tree advantages amongst  data mining methods decision trees  various advantages extensions decision graphs   decision tree  paths   root node   leaf node proceed  way  conjunction     decision graph   possible  use disjunctions ors  join two  paths together using minimum message length mml decision graphs    extended  allow  previously unstated new attributes   learnt dynamically  used  different places within  graph   general coding scheme results  better predictive accuracy  logloss probabilistic scoring  general decision graphs infer models  fewer leaves  decision trees alternative search methods evolutionary algorithms   used  avoid local optimal decisions  search  decision tree space  little  priori bias   also possible   tree   sampled using mcmc  tree can  searched    bottomup fashion implementations many data mining software packages provide implementations  one   decision tree algorithms several examples include salford systems cart  licensed  proprietary code   original cart authors ibm spss modeler rapidminer sas enterprise miner matlab r  open source software environment  statistical computing  includes several cart implementations   rpart party  randomforest packages weka    opensource data mining suite contains many decision tree algorithms orange   data mining software suite  includes  tree module orngtree knime microsoft sql server  scikitlearn    opensource machine learning library   python programming language\r\n"}
{"index":{"_id":79}}
{"conceptLabelTag":"clustering high dimensional data","conceptLabel":"clustering high dimensional data","conceptDescription":"clustering highdimensional data clustering highdimensional data   cluster analysis  data  anywhere    dozen  many thousands  dimensions  highdimensional spaces  data  often encountered  areas   medicine  dna microarray technology can produce  large number  measurements     clustering  text documents    wordfrequency vector  used  number  dimensions equals  size   vocabulary problems four problems need   overcome  clustering  highdimensional data recent research indicates   discrimination problems  occur     high number  irrelevant dimensions   sharednearestneighbor approaches can improve results approaches approaches towards clustering  axisparallel  arbitrarily oriented affine subspaces differ    interpret  overall goal   finding clusters  data  high dimensionality  overall different approach   find clusters based  pattern   data matrix often referred   biclustering    technique frequently utilized  bioinformatics subspace clustering  image   right shows  mere twodimensional space   number  clusters can  identified   onedimensional subspaces  clusters formula  subspace formula  formula formula formula  subspace formula can  found formula   considered  cluster   twodimensional subspace since    sparsely distributed   formula axis  two dimensions  two clusters formula  formula can  identified  problem  subspace clustering  given   fact    formula different subspaces   space  formula dimensions   subspaces   axisparallel  infinite number  subspaces  possible hence subspace clustering algorithm utilize  kind  heuristic  remain computationally feasible   risk  producing inferior results  example  downwardclosure property cf association rules can  used  build higherdimensional subspaces   combining lowerdimensional ones   subspace t containing  cluster will result   full space s also  contain  cluster ie s t  approach taken     traditional algorithms   clique subclu   also possible  define  subspace using different degrees  relevance   dimension  approach taken  imwkmeans projected clustering projected clustering seeks  assign  point   unique cluster  clusters may exist  different subspaces  general approach   use  special distance function together   regular clustering algorithm  example  predecon algorithm checks  attributes seem  support  clustering   point  adjusts  distance function   dimensions  low variance  amplified   distance function   figure   cluster formula might  found using dbscan   distance function  places less emphasis   formulaaxis  thus exaggerates  low difference   formulaaxis sufficiently enough  group  points   cluster proclus uses  similar approach   kmedoid clustering initial medoids  guessed    medoid  subspace spanned  attributes  low variance  determined points  assigned   medoid closest considering   subspace   medoid  determining  distance  algorithm  proceeds   regular pam algorithm   distance function weights attributes differently  never   hence never drops irrelevant attributes  algorithm  called  softprojected clustering algorithm hybrid approaches   algorithms try  either find  unique cluster assignment   point   clusters   subspaces many settle   result     number  possibly overlapping   necessarily exhaustive set  clusters  found  example  fires     basic approach  subspace clustering algorithm  uses  heuristic  aggressive  credibly produce  subspace clusters correlation clustering another type  subspaces  considered  correlation clustering data mining\r\n"}
{"index":{"_id":80}}
{"conceptLabelTag":"genetic algorithm","conceptLabel":"genetic algorithm","conceptDescription":"genetic algorithm  computer science  operations research  genetic algorithm ga   metaheuristic inspired   process  natural selection  belongs   larger class  evolutionary algorithms ea genetic algorithms  commonly used  generate highquality solutions  optimization  search problems  relying  bioinspired operators   mutation crossover  selection methodology optimization problems   genetic algorithm  population  candidate solutions called individuals creatures  phenotypes   optimization problem  evolved toward better solutions  candidate solution   set  properties  chromosomes  genotype  can  mutated  altered traditionally solutions  represented  binary  strings  s  s   encodings  also possible  evolution usually starts   population  randomly generated individuals    iterative process   population   iteration called  generation   generation  fitness  every individual   population  evaluated  fitness  usually  value   objective function   optimization problem  solved   fit individuals  stochastically selected   current population   individuals genome  modified recombined  possibly randomly mutated  form  new generation  new generation  candidate solutions   used   next iteration   algorithm commonly  algorithm terminates  either  maximum number  generations   produced   satisfactory fitness level   reached   population  typical genetic algorithm requires  standard representation   candidate solution    array  bits arrays   types  structures can  used  essentially   way  main property  makes  genetic representations convenient    parts  easily aligned due   fixed size  facilitates simple crossover operations variable length representations may also  used  crossover implementation   complex   case treelike representations  explored  genetic programming  graphform representations  explored  evolutionary programming  mix   linear chromosomes  trees  explored  gene expression programming   genetic representation   fitness function  defined  ga proceeds  initialize  population  solutions    improve   repetitive application   mutation crossover inversion  selection operators initialization  population size depends   nature   problem  typically contains several hundreds  thousands  possible solutions often  initial population  generated randomly allowing  entire range  possible solutions  search space occasionally  solutions may  seeded  areas  optimal solutions  likely   found selection   successive generation  portion   existing population  selected  breed  new generation individual solutions  selected   fitnessbased process  fitter solutions  measured   fitness function  typically  likely   selected certain selection methods rate  fitness   solution  preferentially select  best solutions  methods rate   random sample   population   former process may   timeconsuming  fitness function  defined   genetic representation  measures  quality   represented solution  fitness function  always problem dependent  instance   knapsack problem one wants  maximize  total value  objects  can  put   knapsack   fixed capacity  representation   solution might   array  bits   bit represents  different object   value   bit  represents whether    object    knapsack  every  representation  valid   size  objects may exceed  capacity   knapsack  fitness   solution   sum  values   objects   knapsack   representation  valid  otherwise   problems   hard  even impossible  define  fitness expression   cases  simulation may  used  determine  fitness function value   phenotype eg computational fluid dynamics  used  determine  air resistance   vehicle whose shape  encoded   phenotype  even interactive genetic algorithms  used genetic operators  next step   generate  second generation population  solutions   selected   combination  genetic operators crossover also called recombination  mutation   new solution   produced  pair  parent solutions  selected  breeding   pool selected previously  producing  child solution using   methods  crossover  mutation  new solution  created  typically shares many   characteristics   parents new parents  selected   new child   process continues   new population  solutions  appropriate size  generated although reproduction methods   based   use  two parents   biology inspired  research suggests    two parents generate higher quality chromosomes  processes ultimately result   next generation population  chromosomes   different   initial generation generally  average fitness will  increased   procedure   population since   best organisms   first generation  selected  breeding along   small proportion  less fit solutions  less fit solutions ensure genetic diversity within  genetic pool   parents  therefore ensure  genetic diversity   subsequent generation  children opinion  divided   importance  crossover versus mutation   many references  fogel  support  importance  mutationbased search although crossover  mutation  known   main genetic operators   possible  use  operators   regrouping colonizationextinction  migration  genetic algorithms   worth tuning parameters    mutation probability crossover probability  population size  find reasonable settings   problem class  worked    small mutation rate may lead  genetic drift   nonergodic  nature  recombination rate    high may lead  premature convergence   genetic algorithm  mutation rate    high may lead  loss  good solutions unless elitist selection  employed termination  generational process  repeated   termination condition   reached common terminating conditions   building block hypothesis genetic algorithms  simple  implement   behavior  difficult  understand  particular   difficult  understand   algorithms frequently succeed  generating solutions  high fitness  applied  practical problems  building block hypothesis bbh consists  goldberg describes  heuristic  follows despite   lack  consensus regarding  validity   buildingblock hypothesis    consistently evaluated  used  reference throughout  years many estimation  distribution algorithms  example   proposed   attempt  provide  environment    hypothesis  hold although good results   reported   classes  problems skepticism concerning  generality andor practicality   buildingblock hypothesis   explanation  gas efficiency still remains indeed    reasonable amount  work  attempts  understand  limitations   perspective  estimation  distribution algorithms limitations   limitations   use   genetic algorithm compared  alternative optimization algorithms variants chromosome representation  simplest algorithm represents  chromosome   bit string typically numeric parameters can  represented  integers though   possible  use floating point representations  floating point representation  natural  evolution strategies  evolutionary programming  notion  realvalued genetic algorithms   offered   really  misnomer     really represent  building block theory   proposed  john henry holland   s  theory   without support though based  theoretical  experimental results see   basic algorithm performs crossover  mutation   bit level  variants treat  chromosome   list  numbers   indexes   instruction table nodes   linked list hashes objects    imaginable data structure crossover  mutation  performed    respect data element boundaries   data types specific variation operators can  designed different chromosomal data types seem  work better  worse  different specific problem domains  bitstring representations  integers  used gray coding  often employed   way small changes   integer can  readily affected  mutations  crossovers    found  help prevent premature convergence   called hamming walls    many simultaneous mutations  crossover events must occur  order  change  chromosome   better solution  approaches involve using arrays  realvalued numbers instead  bit strings  represent chromosomes results   theory  schemata suggest   general  smaller  alphabet  better  performance    initially surprising  researchers  good results  obtained  using realvalued chromosomes   explained   set  real values   finite population  chromosomes  forming  virtual alphabet  selection  recombination  dominant   much lower cardinality    expected   floating point representation  expansion   genetic algorithm accessible problem domain can  obtained   complex encoding   solution pools  concatenating several types  heterogenously encoded genes  one chromosome  particular approach allows  solving optimization problems  require vastly disparate definition domains   problem parameters  instance  problems  cascaded controller tuning  internal loop controller structure can belong   conventional regulator  three parameters whereas  external loop  implement  linguistic controller    fuzzy system    inherently different description  particular form  encoding requires  specialized crossover mechanism  recombines  chromosome  section     useful tool   modelling  simulation  complex adaptive systems especially evolution processes elitism  practical variant   general process  constructing  new population   allow  best organisms   current generation  carry    next unaltered  strategy  known  elitist selection  guarantees   solution quality obtained   ga will  decrease  one generation   next parallel implementations parallel implementations  genetic algorithms come  two flavors coarsegrained parallel genetic algorithms assume  population     computer nodes  migration  individuals among  nodes finegrained parallel genetic algorithms assume  individual   processor node  acts  neighboring individuals  selection  reproduction  variants like genetic algorithms  online optimization problems introduce timedependence  noise   fitness function adaptive gas genetic algorithms  adaptive parameters adaptive genetic algorithms agas  another significant  promising variant  genetic algorithms  probabilities  crossover pc  mutation pm greatly determine  degree  solution accuracy   convergence speed  genetic algorithms can obtain instead  using fixed values  pc  pm agas utilize  population information   generation  adaptively adjust  pc  pm  order  maintain  population diversity  well   sustain  convergence capacity  aga adaptive genetic algorithm  adjustment  pc  pm depends   fitness values   solutions  caga clusteringbased adaptive genetic algorithm   use  clustering analysis  judge  optimization states   population  adjustment  pc  pm depends   optimization states  can  quite effective  combine ga   optimization methods ga tends   quite good  finding generally good global solutions  quite inefficient  finding  last  mutations  find  absolute optimum  techniques   simple hill climbing  quite efficient  finding absolute optimum   limited region alternating ga  hill climbing can improve  efficiency  ga  overcoming  lack  robustness  hill climbing  means   rules  genetic variation may   different meaning   natural case  instance provided  steps  stored  consecutive order crossing  may sum  number  steps  maternal dna adding  number  steps  paternal dna      like adding vectors   probably may follow  ridge   phenotypic landscape thus  efficiency   process may  increased  many orders  magnitude moreover  inversion operator   opportunity  place steps  consecutive order    suitable order  favour  survival  efficiency see  instance  example  travelling salesman problem  particular  use   edge recombination operator  variation   population   whole  evolved rather   individual members  known  gene pool recombination  number  variations   developed  attempt  improve performance  gas  problems   high degree  fitness epistasis ie   fitness   solution consists  interacting subsets   variables  algorithms aim  learn  exploiting  beneficial phenotypic interactions     aligned   building block hypothesis  adaptively reducing disruptive recombination prominent examples   approach include  mga gemga  llga problem domains problems  appear   particularly appropriate  solution  genetic algorithms include timetabling  scheduling problems  many scheduling software packages  based  gas gas  also  applied  engineering genetic algorithms  often applied   approach  solve global optimization problems   general rule  thumb genetic algorithms might  useful  problem domains    complex fitness landscape  mixing ie mutation  combination  crossover  designed  move  population away  local optima   traditional hill climbing algorithm might get stuck  observe  commonly used crossover operators  change  uniform population mutation alone can provide ergodicity   overall genetic algorithm process seen   markov chain examples  problems solved  genetic algorithms include mirrors designed  funnel sunlight   solar collector antennae designed  pick  radio signals  space  walking methods  computer figures   algorithm design manual skiena advises  genetic algorithms   task history  alan turing proposed  learning machine   parallel  principles  evolution computer simulation  evolution started  early     work  nils aall barricelli   using  computer   institute  advanced study  princeton new jersey  publication   widely noticed starting   australian quantitative geneticist alex fraser published  series  papers  simulation  artificial selection  organisms  multiple loci controlling  measurable trait   beginnings computer simulation  evolution  biologists became  common   early s   methods  described  books  fraser  burnell  crosby frasers simulations included    essential elements  modern genetic algorithms  addition hansjoachim bremermann published  series  papers   s  also adopted  population  solution  optimization problems undergoing recombination mutation  selection bremermanns research also included  elements  modern genetic algorithms  noteworthy early pioneers include richard friedberg george friedman  michael conrad many early papers  reprinted  fogel although barricelli  work  reported   simulated  evolution  ability  play  simple game artificial evolution became  widely recognized optimization method   result   work  ingo rechenberg  hanspaul schwefel   s  early s rechenbergs group  able  solve complex engineering problems  evolution strategies another approach   evolutionary programming technique  lawrence j fogel   proposed  generating artificial intelligence evolutionary programming originally used finite state machines  predicting environments  used variation  selection  optimize  predictive logics genetic algorithms  particular became popular   work  john holland   early s  particularly  book adaptation  natural  artificial systems  work originated  studies  cellular automata conducted  holland   students   university  michigan holland introduced  formalized framework  predicting  quality   next generation known  hollands schema theorem research  gas remained largely theoretical   mids   first international conference  genetic algorithms  held  pittsburgh pennsylvania commercial products   late s general electric started selling  worlds first genetic algorithm product  mainframebased toolkit designed  industrial processes  axcelis inc released evolver  worlds first commercial ga product  desktop computers  new york times technology writer john markoff wrote  evolver    remained   interactive commercial genetic algorithm  evolver  sold  palisade  translated  several languages   currently   th version related techniques parent fields genetic algorithms   subfield  related fields evolutionary algorithms evolutionary algorithms   subfield  evolutionary computing swarm intelligence swarm intelligence   subfield  evolutionary computing  evolutionary computing algorithms evolutionary computation   subfield   metaheuristic methods  metaheuristic methods metaheuristic methods broadly fall within stochastic optimisation methods\r\n"}
{"index":{"_id":81}}
{"conceptLabelTag":"som","conceptLabel":"som","conceptDescription":"selforganizing map  selforganizing map som  selforganizing feature map sofm   type  artificial neural network ann   trained using unsupervised learning  produce  lowdimensional typically twodimensional discretized representation   input space   training samples called  map   therefore  method   dimensionality reduction selforganizing maps differ   artificial neural networks   apply competitive learning  opposed  errorcorrection learning   backpropagation  gradient descent    sense   use  neighborhood function  preserve  topological properties   input space  makes soms useful  visualizing lowdimensional views  highdimensional data akin  multidimensional scaling  artificial neural network introduced   finnish professor teuvo kohonen   s  sometimes called  kohonen map  network  kohonen net   computationally convenient abstraction building  work  biologically neural models   s  morphogenesis models dating back  alan turing   s like  artificial neural networks soms operate  two modes training  mapping training builds  map using input examples  competitive process also called vector quantization  mapping automatically classifies  new input vector  selforganizing map consists  components called nodes  neurons associated   node   weight vector    dimension   input data vectors   position   map space  usual arrangement  nodes   twodimensional regular spacing   hexagonal  rectangular grid  selforganizing map describes  mapping   higherdimensional input space   lowerdimensional map space  procedure  placing  vector  data space onto  map   find  node   closest smallest distance metric weight vector   data space vector    typical  consider  type  network structure  related  feedforward networks   nodes  visualized   attached  type  architecture  fundamentally different  arrangement  motivation useful extensions include using toroidal grids  opposite edges  connected  using large numbers  nodes    shown   selforganizing maps   small number  nodes behave   way   similar  kmeans larger selforganizing maps rearrange data   way   fundamentally topological  character   also common  use  umatrix  umatrix value   particular node   average distance   nodes weight vector     closest neighbors   square grid  instance  might consider  closest  nodes  von neumann  moore neighborhoods respectively  six nodes   hexagonal grid large soms display emergent properties  maps consisting  thousands  nodes   possible  perform cluster operations   map  learning algorithm  goal  learning   selforganizing map   cause different parts   network  respond similarly  certain input patterns   partly motivated   visual auditory   sensory information  handled  separate parts   cerebral cortex   human brain  weights   neurons  initialized either  small random values  sampled evenly   subspace spanned   two largest principal component eigenvectors   latter alternative learning  much faster   initial weights already give  good approximation  som weights  network must  fed  large number  example vectors  represent  close  possible  kinds  vectors expected  mapping  examples  usually administered several times  iterations  training utilizes competitive learning   training example  fed   network  euclidean distance   weight vectors  computed  neuron whose weight vector   similar   input  called  best matching unit bmu  weights   bmu  neurons close     som lattice  adjusted towards  input vector  magnitude   change decreases  time   distance within  lattice   bmu  update formula   neuron v  weight vector ws   s   step index t  index   training sample u   index   bmu  dt s   monotonically decreasing learning coefficient  dt   input vector u v s   neighborhood function  gives  distance   neuron u   neuron v  step s depending   implementations t can scan  training data set systematically t  t  repeat t   training samples size  randomly drawn   data set bootstrap sampling  implement   sampling method   jackknifing  neighborhood function u v s depends   lattice distance   bmu neuron u  neuron v   simplest form     neurons close enough  bmu   others   gaussian function   common choice  regardless   functional form  neighborhood function shrinks  time   beginning   neighborhood  broad  selforganizing takes place   global scale   neighborhood  shrunk  just  couple  neurons  weights  converging  local estimates   implementations  learning coefficient   neighborhood function decrease steadily  increasing s  others  particular   t scans  training data set  decrease  stepwise fashion  every t steps  process  repeated   input vector   usually large number  cycles  network winds  associating output nodes  groups  patterns   input data set   patterns can  named  names can  attached   associated nodes   trained net  mapping  will  one single winning neuron  neuron whose weight vector lies closest   input vector  can  simply determined  calculating  euclidean distance  input vector  weight vector  representing input data  vectors   emphasized   article    noted   kind  object  can  represented digitally    appropriate distance measure associated       necessary operations  training  possible can  used  construct  selforganizing map  includes matrices continuous functions  even  selforganizing maps variables    variables needed  vectors  bold  variant algorithm som initiation selection   good initial approximation   wellknown problem   iterative methods  learning neural networks kohonen used random initiation  som weights recently principal component initialization   initial map weights  chosen   space   first principal components  become popular due   exact reproducibility   results careful comparison   random initiation approach  principal component initialization  onedimensional som models  principal curves demonstrated   advantages  principal component som initialization   universal  best initialization method depends   geometry   specific dataset principal component initialization  preferable  dimension one   principal curve approximating  dataset can  univalently  linearly projected   first principal component quasilinear sets  nonlinear datasets however random initiation performs better examples rgb colour space consider  nm array  nodes    contains  weight vector   aware   location   array  weight vector     dimension   nodes input vector  weights may initially  set  random values now  need input  feed  map  generated map   given input exist  separate subspaces  will create three vectors  represent colors colors can  represented   red green  blue components consequently  input vectors will  three components  corresponding   color space  input vectors will   color training vector data sets used  som  data vectors  preferably  normalized vector length  equal  one  training  som fishers iris flower data neurons square grid  trained  iterations   learning rate  using  normalized iris flower data set   fourdimensional data vectors shown   color image formed   first three dimensions   fourdimensional som weight vectors top left  pseudocolor image   magnitude   som weight vectors top right  umatrix euclidean distance  weight vectors  neighboring cells   som bottom left   overlay  data points red  setosa green  versicolor  blue  virginica   umatrix based   minimum euclidean distance  data vectors  som weight vectors bottom right interpretation   two ways  interpret  som    training phase weights   whole neighborhood  moved    direction similar items tend  excite adjacent neurons therefore som forms  semantic map  similar samples  mapped close together  dissimilar ones apart  may  visualized   umatrix euclidean distance  weight vectors  neighboring cells   som   way   think  neuronal weights  pointers   input space  form  discrete approximation   distribution  training samples  neurons point  regions  high training sample concentration  fewer   samples  scarce som may  considered  nonlinear generalization  principal components analysis pca    shown using  artificial  real geophysical data  som  many advantages   conventional feature extraction methods   empirical orthogonal functions eof  pca originally som   formulated   solution   optimisation problem nevertheless    several attempts  modify  definition  som   formulate  optimisation problem  gives similar results  example elastic maps use  mechanical metaphor  elasticity  approximate principal manifolds  analogy   elastic membrane  plate\r\n"}
{"index":{"_id":82}}
{"conceptLabelTag":"linear model","conceptLabel":"linear model","conceptDescription":"linear model  statistics  term linear model  used  different ways according   context   common occurrence   connection  regression models   term  often taken  synonymous  linear regression model however  term  also used  time series analysis   different meaning   case  designation linear  used  identify  subclass  models   substantial reduction   complexity   related statistical theory  possible linear regression models   regression case  statistical model   follows given  random sample formula  relation   observations y   independent variables x  formulated   formula may  nonlinear functions     quantities  random variables representing errors   relationship  linear part   designation relates   appearance   regression coefficients   linear way    relationship alternatively one may say   predicted values corresponding    model namely  linear functions   given  estimation  undertaken   basis   least squares analysis estimates   unknown parameters  determined  minimising  sum  squares function    can readily  seen   linear aspect   model means  following time series models  example   linear time series model   autoregressive moving average model   model  values x   time series can  written   form    quantities  random variables representing innovations   new random effects  appear   certain time  also affect values  x  later times   instance  use   term linear model refers   structure    relationship  representing x   linear function  past values    time series   current  past values   innovations  particular aspect   structure means    relatively simple  derive relations   mean  covariance properties   time series note    linear part   term linear model   referring   coefficients        case   regression model  looks structurally similar  uses  statistics     instances  nonlinear model  used  contrast   linearly structured model although  term linear model   usually applied one example    nonlinear dimensionality reduction\r\n"}
{"index":{"_id":83}}
{"conceptLabelTag":"latent variable","conceptLabel":"latent variable","conceptDescription":"latent variable  statistics latent variables  latin present participle  lateo lie hidden  opposed  observable variables  variables    directly observed   rather inferred   mathematical model   variables   observed directly measured mathematical models  aim  explain observed variables  terms  latent variables  called latent variable models latent variable models  used  many disciplines including psychology economics engineering medicine physics machine learningartificial intelligence bioinformatics natural language processing econometrics management   social sciences sometimes latent variables correspond  aspects  physical reality    principle  measured  may    practical reasons   situation  term hidden variables  commonly used reflecting  fact   variables  really   hidden  times latent variables correspond  abstract concepts like categories behavioral  mental states  data structures  terms hypothetical variables  hypothetical constructs may  used   situations one advantage  using latent variables    reduces  dimensionality  data  large number  observable variables can  aggregated   model  represent  underlying concept making  easier  understand  data   sense  serve  function similar    scientific theories    time latent variables link observable subsymbolic data   real world  symbolic data   modeled world latent variables  created  factor analytic methods generally represent shared variance   degree   variables move together variables    correlation  result   latent construct based   common factor model examples  latent variables economics examples  latent variables   field  economics include quality  life business confidence morale happiness  conservatism    variables    measured directly  linking  latent variables   observable variables  values   latent variables can  inferred  measurements   observable variables quality  life   latent variable  can   measured directly  observable variables  used  infer quality  life observable variables  measure quality  life include wealth employment environment physical  mental health education recreation  leisure time  social belonging common methods  inferring latent variables bayesian algorithms  methods bayesian statistics  often used  inferring latent variables\r\n"}
{"index":{"_id":84}}
{"conceptLabelTag":"spectral clustering","conceptLabel":"spectral clustering","conceptDescription":"spectral clustering  multivariate statistics   clustering  data spectral clustering techniques make use   spectrum eigenvalues   similarity matrix   data  perform dimensionality reduction  clustering  fewer dimensions  similarity matrix  provided   input  consists   quantitative assessment   relative similarity   pair  points   dataset  application  image segmentation spectral clustering  known  segmentationbased object categorization algorithms given  enumerated set  data points  similarity matrix may  defined   symmetric matrix formula  formula represents  measure   similarity  data points  indexes formula  formula  general approach  spectral clustering   use  standard clustering method   many  methods kmeans  discussed   relevant eigenvectors   laplacian matrix  formula   many different ways  define  laplacian   different mathematical interpretations    clustering will also  different interpretations  eigenvectors   relevant   ones  correspond  smallest several eigenvalues   laplacian except   smallest eigenvalue  will   value   computational efficiency  eigenvectors  often computed   eigenvectors corresponding   largest several eigenvalues   function   laplacian one spectral clustering technique   normalized cuts algorithm  shimalik algorithm introduced  jianbo shi  jitendra malik commonly used  image segmentation  partitions points  two sets formula based   eigenvector formula corresponding   secondsmallest eigenvalue   symmetric normalized laplacian defined   formula   diagonal matrix  mathematically equivalent algorithm takes  eigenvector corresponding   largest eigenvalue   random walk normalized laplacian matrix formula  meilashi algorithm   examined   context  diffusion maps   found   related  computational quantum mechanics another possibility   use  laplacian matrix defined  rather   symmetric normalized laplacian matrix partitioning may  done  various ways    computing  median formula   components   second smallest eigenvector formula  placing  points whose component  formula  greater  formula  formula   rest  formula  algorithm can  used  hierarchical clustering  repeatedly partitioning  subsets   fashion   similarity matrix formula   already  explicitly constructed  efficiency  spectral clustering may  improved   solution   corresponding eigenvalue problem  performed   matrix fashion without explicitly manipulating  even computing  similarity matrix    lanczos algorithm  largesized graphs  second eigenvalue   normalized graph laplacian matrix  often illconditioned leading  slow convergence  iterative eigenvalue solvers preconditioning   key technology accelerating  convergence eg   matrix lobpcg method spectral clustering   successfully applied  large graphs  first identifying  community structure   clustering communities spectral clustering  closely related  nonlinear dimensionality reduction  dimension reduction techniques   locallylinear embedding can  used  reduce errors  noise  outliers  software  implement spectral clustering  available  large open source projects like scikitlearn mllib  pseudoeigenvector clustering using  power iteration method  r relationship  kmeans  kernel kmeans problem   extension   kmeans problem   input data points  mapped nonlinearly   higherdimensional feature space via  kernel function formula  weighted kernel kmeans problem  extends  problem  defining  weight formula   cluster   reciprocal   number  elements   cluster suppose formula   matrix   normalizing coefficients   point   cluster formula  formula  zero otherwise suppose formula   kernel matrix   points  weighted kernel kmeans problem  n points  k clusters  given      formula  addition   identity constrains  formula given   formula represents  vector  ones  problem can  recast   problem  equivalent   spectral clustering problem   identity constraints  formula  relaxed  particular  weighted kernel kmeans problem can  reformulated   spectral clustering graph partitioning problem  vice versa  output   algorithms  eigenvectors    satisfy  identity requirements  indicator variables defined  formula hence postprocessing   eigenvectors  required   equivalence   problems transforming  spectral clustering problem   weighted kernel kmeans problem greatly reduces  computational burden measures  compare clusterings ravi kannan santosh vempala  adrian vetta   following paper proposed  bicriteria measure  define  quality   given clustering  said   clustering   clustering   conductance   clusterin  clustering   least   weight   intercluster edges    fraction   total weight    edges   graph  also look  two approximation algorithms     paper\r\n"}
{"index":{"_id":85}}
{"conceptLabelTag":"association rule learning","conceptLabel":"association rule learning","conceptDescription":"association rule learning association rule learning   rulebased machine learning method  discovering interesting relations  variables  large databases   intended  identify strong rules discovered  databases using  measures  interestingness based   concept  strong rules rakesh agrawal tomasz imieliski  arun swami introduced association rules  discovering regularities  products  largescale transaction data recorded  pointofsale pos systems  supermarkets  example  rule formula found   sales data   supermarket  indicate    customer buys onions  potatoes together   likely  also buy hamburger meat  information can  used   basis  decisions  marketing activities   eg promotional pricing  product placements  addition    example  market basket analysis association rules  employed today  many application areas including web usage mining intrusion detection continuous production  bioinformatics  contrast  sequence mining association rule learning typically   consider  order  items either within  transaction  across transactions definition following  original definition  agrawal et al  problem  association rule mining  defined  let formula   set  formula binary attributes called items let formula   set  transactions called  database  transaction  formula   unique transaction id  contains  subset   items  formula  rule  defined   implication   form formula  formula  agrawal et al  rule  defined    set   single item formula  formula every rule  composed  two different sets  items also known  itemsets formula  formula  formula  called antecedent  lefthandside lhs  formula consequent  righthandside rhs  illustrate  concepts  use  small example   supermarket domain  set  items  formula    table  shown  small database containing  items    entry  value means  presence   item   corresponding transaction   value represents  absence   item   transaction  example rule   supermarket   formula meaning   butter  bread  bought customers also buy milk note  example  extremely small  practical applications  rule needs  support  several hundred transactions   can  considered statistically significant  datasets often contain thousands  millions  transactions useful concepts  order  select interesting rules   set   possible rules constraints  various measures  significance  interest  used  bestknown constraints  minimum thresholds  support  confidence let formula   itemset formula  association rule  formula  set  transactions   given database support support   indication   frequently  itemset appears   database  support  formula  respect  formula  defined   proportion  transactions formula   database  contains itemset formula formula   example database  itemset formula   support  formula since  occurs    transactions   transactions  argument  formula   set  preconditions  thus becomes  restrictive   grows instead   inclusive confidence confidence   indication   often  rule   found   true  confidence value   rule formula  respect   set  transactions formula   proportion   transactions  contains formula  also contains formula confidence  defined  formula  example  rule formula   confidence  formula   database  means     transactions containing butter  bread  rule  correct   times  customer buys butter  bread milk  bought  well note  formula means  support   union   items  x  y   somewhat confusing since  normally think  terms  probabilities  events   sets  items  can rewrite formula   probability formula  formula  formula   events   transaction contains itemset formula  formula respectively thus confidence can  interpreted   estimate   conditional probability formula  probability  finding  rhs   rule  transactions   condition   transactions also contain  lhs lift  lift   rule  defined  formula   ratio   observed support   expected  x  y  independent  example  rule formula   lift  formula   rule   lift    imply   probability  occurrence   antecedent     consequent  independent     two events  independent     rule can  drawn involving  two events   lift   lets us know  degree    two occurrences  dependent  one another  makes  rules potentially useful  predicting  consequent  future data sets  value  lift    considers   confidence   rule   overall data set conviction  conviction   rule  defined  formula  example  rule formula   conviction  formula  can  interpreted   ratio   expected frequency  x occurs without y    say  frequency   rule makes  incorrect prediction  x  y  independent divided   observed frequency  incorrect predictions   example  conviction value  shows   rule formula   incorrect  often times  often   association  x  y  purely random chance process association rules  usually required  satisfy  userspecified minimum support   userspecified minimum confidence    time association rule generation  usually split   two separate steps   second step  straightforward  first step needs  attention finding  frequent itemsets   database  difficult since  involves searching  possible itemsets item combinations  set  possible itemsets   power set  formula   size formula excluding  empty set     valid itemset although  size   powerset grows exponentially   number  items formula  formula efficient search  possible using  downwardclosure property  support also called antimonotonicity  guarantees    frequent itemset   subsets  also frequent  thus  infrequent itemset can   subset   frequent itemset exploiting  property efficient algorithms eg apriori  eclat can find  frequent itemsets history  concept  association rules  popularised particularly due   article  agrawal et al   acquired   citations according  google scholar   august   thus one    cited papers   data mining field however   possible    now called association rules  similar   appears   paper  guha  general data mining method developed  petr h jek et al  early circa use  minimum support  confidence  find  association rules   feature based modeling framework  found  rules  formula  formula greater  user defined constraints alternative measures  interestingness  addition  confidence  measures  interestingness  rules   proposed  popular measures  several  measures  presented  compared  tan et al   hahsler looking  techniques  can model   user  known  using  models  interestingness measures  currently  active research trend   name  subjective interestingness statistically sound associations one limitation   standard approach  discovering associations    searching massive numbers  possible associations  look  collections  items  appear   associated    large risk  finding many spurious associations   collections  items  cooccur  unexpected frequency   data      chance  example suppose   considering  collection  items  looking  rules containing two items   lefthandside  item   righthandside   approximately  rules   apply  statistical test  independence   significance level   means     chance  accepting  rule     association   assume    associations   nonetheless expect  find rules statistically sound association discovery controls  risk   cases reducing  risk  finding  spurious associations   userspecified significance levels algorithms many algorithms  generating association rules  presented  time  wellknown algorithms  apriori eclat  fpgrowth     half  job since   algorithms  mining frequent itemsets another step needs   done   generate rules  frequent itemsets found   database apriori algorithm apriori uses  breadthfirst search strategy  count  support  itemsets  uses  candidate generation function  exploits  downward closure property  support eclat algorithm eclat alt eclat stands  equivalence class transformation   depthfirst search algorithm using set intersection    naturally elegant algorithm suitable   sequential  well  parallel execution  localityenhancing properties   first introduced  zaki parthasarathy li  ogihara   series  papers written  mohammed javeed zaki srinivasan parthasarathy m ogihara wei li new algorithms  fast discovery  association rules kdd mohammed javeed zaki srinivasan parthasarathy mitsunori ogihara wei li parallel algorithms  discovery  association rules data min knowl discov fpgrowth algorithm fp stands  frequent pattern   first pass  algorithm counts occurrence  items attributevalue pairs   dataset  stores   header table   second pass  builds  fptree structure  inserting instances items   instance    sorted  descending order   frequency   dataset    tree can  processed quickly items   instance    meet minimum coverage threshold  discarded  many instances share  frequent items fptree provides high compression close  tree root recursive processing   compressed version  main dataset grows large item sets directly instead  generating candidate items  testing    entire database growth starts   bottom   header table  longest branches  finding  instances matching given condition new tree  created  counts projected   original tree corresponding   set  instances   conditional   attribute   node getting sum   children counts recursive growth ends   individual items conditional   attribute meet minimum support threshold  processing continues   remaining header items   original fptree   recursive process  completed  large item sets  minimum coverage   found  association rule creation begins others aprioridp aprioridp utilizes dynamic programming  frequent itemset mining  working principle   eliminate  candidate generation like fptree   stores support count  specialized data structure instead  tree context based association rule mining algorithm cbpnarm   algorithm developed   mine association rules   basis  context  uses context variable   basis    support   itemset  changed   basis    rules  finally populated   rule set nodesetbased algorithms fin prepost  ppv  three algorithms based  node sets  use nodes   coding fptree  represent itemsets  employ  depthfirst search strategy  discovery frequent itemsets using intersection  node sets guha procedure assoc guha   general method  exploratory data analysis   theoretical foundations  observational calculi  assoc procedure   guha method  mines  generalized association rules using fast bitstrings operations  association rules mined   method   general   output  apriori  example items can  connected   conjunction  disjunctions   relation  antecedent  consequent   rule   restricted  setting minimum support  confidence   apriori  arbitrary combination  supported interest measures can  used opus search opus   efficient algorithm  rule discovery   contrast   alternatives   require either monotone  antimonotone constraints   minimum support initially used  find rules   fixed consequent   subsequently  extended  find rules   item   consequent opus search   core technology   popular magnum opus association discovery system lore  famous story  association rule mining   beer  diaper story  purported survey  behavior  supermarket shoppers discovered  customers presumably young men  buy diapers tend also  buy beer  anecdote became popular   example   unexpected association rules might  found  everyday data   varying opinions    much   story  true daniel powers says  thomas blischok manager   retail consulting group  teradata   staff prepared  analysis  million market baskets   osco drug stores database queries  developed  identify affinities  analysis  discover    pm  consumers bought beer  diapers osco managers   exploit  beer  diapers relationship  moving  products closer together   shelves  types  association mining multirelation association rules multirelation association rules mrar   new class  association rules   contrast  primitive simple  even multirelational association rules   usually extracted  multirelational databases  rule item consists  one entity  several relations  relations indicate indirect relationship   entities consider  following mrar   first item consists  three relations live  nearby  humid   live   place   near   city  humid climate type  also  younger   health condition  good  association rules  extractable  rdbms data  semantic web data context based association rules   form  association rule context based association rules claims  accuracy  association rule mining  considering  hidden variable named context variable  changes  final set  association rules depending upon  value  context variables  example  baskets orientation  market basket analysis reflects  odd pattern   early days  monththis might    abnormal context ie salary  drawn   start   month contrast set learning   form  associative learning contrast set learners use rules  differ meaningfully   distribution across subsets weighted class learning  another form  associative learning   weight may  assigned  classes  give focus   particular issue  concern   consumer   data mining results highorder pattern discovery facilitate  capture  highorder polythetic patterns  event associations   intrinsic  complex realworld data koptimal pattern discovery provides  alternative   standard approach  association rule learning  requires   pattern appear frequently   data approximate frequent itemset mining   relaxed version  frequent itemset mining  allows    items     rows   generalized association rules hierarchical taxonomy concept hierarchy quantitative association rules categorical  quantitative data interval data association rules eg partition  age  yearincrement ranged maximal association rules sequential pattern mining discovers subsequences   common    minsup sequences   sequence database  minsup  set   user  sequence   ordered list  transactions sequential rules discovering relationships  items  considering  time ordering   generally applied   sequence database  example  sequential rule found  database  sequences  customer transactions can   customers  bought  computer  cdroms later bought  webcam   given confidence  support subspace clustering  specific type  clustering highdimensional data   many variants also based   downwardclosure property  specific clustering models warmr  shipped  part   ace data mining suite  allows association rule learning  first order relational rules\r\n"}
{"index":{"_id":86}}
{"conceptLabelTag":"genetic programming","conceptLabel":"genetic programming","conceptDescription":"genetic programming  artificial intelligence genetic programming gp   technique whereby computer programs  encoded   set  genes    modified evolved using  evolutionary algorithm often  genetic algorithm ga    application   example genetic algorithms   space  solutions consists  computer programs  results  computer programs able  perform well   predefined task  methods used  encode  computer program   artificial chromosome   evaluate  fitness  respect   predefined task  central   gp technique  still  subject  active research history  pioneering work    today known  artificial life  carried   nils aall barricelli using   early computers   s  early s evolutionary algorithms became widely recognized  optimization methods ingo rechenberg   group  able  solve complex engineering problems  evolution strategies  documented   phd thesis   resulting book john holland  highly influential   s  establishment  evolutionary algorithms   scientific community allowed    first concrete steps  study  gp idea  lawrence j fogel one   earliest practitioners   gp methodology applied evolutionary algorithms   problem  discovering finitestate automata later gprelated work grew    learning classifier system community  developed sets  sparse rules describing optimal policies  markov decision processes  richard forsyth evolved tree rules  classify heart disease  first statement  modern treebased genetic programming   procedural languages organized  treebased structures  operated   suitably defined gaoperators  given  nichael l cramer  work  later greatly expanded  john r koza  main proponent  gp   pioneered  application  genetic programming  various complex optimization  search problems gianna giavelli  student  kozas later pioneered  use  genetic programming   technique  model dna expression   s gp  mainly used  solve relatively simple problems     computationally intensive recently gp  produced many novel  outstanding results  areas   quantum computing electronic design game playing cyberterrorism prevention sorting  searching due  improvements  gp technology   exponential growth  cpu power  results include  replication  development  several postyear inventions gp  also  applied  evolvable hardware  well  computer programs developing  theory  gp    difficult     s gp  considered  sort  outcast among search techniques program representation gp evolves computer programs traditionally represented  memory  tree structures trees can  easily evaluated   recursive manner every tree node   operator function  every terminal node   operand making mathematical expressions easy  evolve  evaluate thus traditionally gp favors  use  programming languages  naturally embody tree structures  example lisp  functional programming languages  also suitable nontree representations   suggested  successfully implemented   linear genetic programming  suits   traditional imperative languages see  example banzhaf et al  commercial gp software discipulus uses automatic induction  binary machine code aim  achieve better performance gp uses directed multigraphs  generate programs  fully exploit  syntax   given assembly language  nontree representations  structurally noneffective code introns  noncoding genes may seem   useless     effect   performance   one individual however experiments seem  show faster convergence  using program representations   linear genetic programming  cartesian genetic programming  allow  noncoding genes compared  treebased program representations      noncoding genes  approaches  basic ideas  genetic programming   modified  extended   variety  ways metagenetic programming metagenetic programming   proposed meta learning technique  evolving  genetic programming system using genetic programming   suggests  chromosomes crossover  mutation   evolved therefore like  real life counterparts   allowed  change    rather   determined   human programmer metagp  formally proposed  j rgen schmidhuber  doug lenats eurisko   earlier effort  may    technique    recursive  terminating algorithm allowing   avoid infinite recursion critics   idea often say  approach  overly broad  scope however  might  possible  constrain  fitness criterion onto  general class  results   obtain  evolved gp    efficiently produce results  subclasses  might take  form   meta evolved gp  producing human walking algorithms    used  evolve human running ing etc  fitness criterion applied   meta gp  simply  one  efficiency  general problem classes  may   way  show  meta gp will reliably produce results  efficiently   created algorithm   exhaustion\r\n"}
{"index":{"_id":87}}
{"conceptLabelTag":"boosting","conceptLabel":"boosting","conceptDescription":"boosting machine learning boosting   machine learning ensemble metaalgorithm  primarily reducing bias  also variance  supervised learning   family  machine learning algorithms  convert weak learners  strong ones boosting  based   question posed  kearns  valiant can  set  weak learners create  single strong learner  weak learner  defined    classifier    slightly correlated   true classification  can label examples better  random guessing  contrast  strong learner   classifier   arbitrarily wellcorrelated   true classification robert schapires affirmative answer   paper   question  kearns  valiant   significant ramifications  machine learning  statistics  notably leading   development  boosting  first introduced  hypothesis boosting problem simply referred   process  turning  weak learner   strong learner informally  hypothesis boosting problem asks whether  efficient learning algorithm  outputs  hypothesis whose performance   slightly better  random guessing ie  weak learner implies  existence   efficient algorithm  outputs  hypothesis  arbitrary accuracy ie  strong learner algorithms  achieve hypothesis boosting quickly became simply known  boosting freund  schapires arcing adaptative resampling  combining   general technique    less synonymous  boosting boosting algorithms  boosting   algorithmically constrained  boosting algorithms consist  iteratively learning weak classifiers  respect   distribution  adding    final strong classifier    added   typically weighted   way   usually related   weak learners accuracy   weak learner  added  data  reweighted examples   misclassified gain weight  examples   classified correctly lose weight  boosting algorithms actually decrease  weight  repeatedly misclassified examples eg boost  majority  brownboost thus future weak learners focus    examples  previous weak learners misclassified   many boosting algorithms  original ones proposed  robert schapire  recursive majority gate formulation  yoav freund boost  majority   adaptive    take full advantage   weak learners however schapire  freund  developed adaboost  adaptive boosting algorithm  won  prestigious g del prize  algorithms   provable boosting algorithms   probably approximately correct learning formulation can accurately  called boosting algorithms  algorithms   similar  spirit  boosting algorithms  sometimes called leveraging algorithms although   also sometimes incorrectly called boosting algorithms  main variation  many boosting algorithms   method  weighting training data points  hypotheses adaboost   popular  perhaps   significant historically     first algorithm   adapt   weak learners however   many  recent algorithms   lpboost totalboost brownboost xgboost madaboost logitboost  others many boosting algorithms fit   anyboost framework  shows  boosting performs gradient descent  function space using  convex cost function object categorization given images containing various known objects   world  classifier can  learned    automatically categorize  objects  future images simple classifiers built based   image feature   object tend   weak  categorization performance using boosting methods  object categorization   way  unify  weak classifiers   special way  boost  overall ability  categorization problem  object categorization object categorization   typical task  computer vision  involves determining whether    image contains  specific category  object  idea  closely related  recognition identification  detection appearance based object categorization typically contains feature extraction learning  classifier  applying  classifier  new examples   many ways  represent  category  objects eg  shape analysis bag  words models  local descriptors   sift etc examples  supervised classifiers  naive bayes classifier svm mixtures  gaussians neural network etc however research  shown  object categories   locations  images can  discovered   unsupervised manner  well status quo  object categorization  recognition  object categories  images   challenging problem  computer vision especially   number  categories  large   due  high intra class variability   need  generalization across variations  objects within   category objects within one category may look quite different even   object may appear unalike  different viewpoint scale  illumination background clutter  partial occlusion add difficulties  recognition  well humans  able  recognize thousands  object types whereas    existing object recognition systems  trained  recognize    eg human face car simple objects etc research    active  dealing   categories  enabling incremental additions  new categories  although  general problem remains unsolved several multicategory objects detectors number  categories around  clustered scenes   developed one means   feature sharing  boosting boosting  binary categorization  use adaboost  face detection   example  binary categorization  two categories  faces versus background  general algorithm   follows  boosting  classifier constructed  features  yield  detection rate   formula false positive rate another application  boosting  binary categorization   system  detects pedestrians using patterns  motion  appearance  work   first  combine  motion information  appearance information  features  detect  walking person  takes  similar approach   face detection work  viola  jones boosting  multiclass categorization compared  binary categorization multiclass categorization looks  common features  can  shared across  categories    time  turn    generic edge like features  learning  detectors   category can  trained jointly compared  training separately  generalizes better needs less training data  requires less number  features  achieve  performance  main flow   algorithm  similar   binary case   different    measure   joint training error shall  defined  advance   iteration  algorithm chooses  classifier   single feature features  can  shared   categories shall  encouraged  can  done via converting multiclass classification   binary one  set  categories versus  rest   introducing  penalty error   categories      feature   classifier   paper sharing visual features  multiclass  multiview object detection  torralba et al used gentleboost  boosting  showed   training data  limited learning via sharing features   much better job   sharing given  boosting rounds also   given performance level  total number  features required  therefore  run time cost   classifier   feature sharing detectors  observed  scale approximately logarithmically   number  class ie slower  linear growth   nonsharing case similar results  shown   paper incremental learning  object detectors using  visual shape alphabet yet  authors used adaboost  boosting criticism  phillip long  google  rocco  servedio columbia university published  paper   th international conference  machine learning suggesting  many   algorithms  probably flawed  conclude  convex potential boosters  withstand random classification noise thus making  applicability   algorithms  real world noisy data sets questionable  paper shows    fraction   training data  mislabeled  boosting algorithm tries extremely hard  correctly classify  training examples  fails  produce  model  accuracy better   result   apply  branching program based boosters   apply  adaboost logitboost  others\r\n"}
{"index":{"_id":88}}
{"conceptLabelTag":"adaboost","conceptLabel":"adaboost","conceptDescription":"adaboost adaboost short  adaptive boosting   machine learning metaalgorithm formulated  yoav freund  robert schapire  won  g del prize    work  can  used  conjunction  many  types  learning algorithms  improve  performance  output    learning algorithms weak learners  combined   weighted sum  represents  final output   boosted classifier adaboost  adaptive   sense  subsequent weak learners  tweaked  favor   instances misclassified  previous classifiers adaboost  sensitive  noisy data  outliers   problems  can  less susceptible   overfitting problem   learning algorithms  individual learners can  weak   long   performance   one  slightly better  random guessing eg  error rate  smaller   binary classification  final model can  proven  converge   strong learner  every learning algorithm will tend  suit  problem types better  others  will typically  many different parameters  configurations   adjusted  achieving optimal performance   dataset adaboost  decision trees   weak learners  often referred    best outofthebox classifier  used  decision tree learning information gathered   stage   adaboost algorithm   relative hardness   training sample  fed   tree growing algorithm   later trees tend  focus  hardertoclassify examples overview problems  machine learning often suffer   curse  dimensionality  sample may consist   huge number  potential features  instance  can  haar features  used   violajones object detection framework   pixel image window  evaluating every feature can reduce    speed  classifier training  execution   fact reduce predictive power per  hughes effect unlike neural networks  svms  adaboost training process selects   features known  improve  predictive power   model reducing dimensionality  potentially improving execution time  irrelevant features   need   computed training adaboost refers   particular method  training  boosted classifier  boost classifier   classifier   form   formula   weak learner  takes  object formula  input  returns  value indicating  class   object  example   two class problem  sign   weak learner output identifies  predicted object class   absolute value gives  confidence   classification similarly  formulath classifier will  positive   sample  believed     positive class  negative otherwise  weak learner produces  output hypothesis formula   sample   training set   iteration formula  weak learner  selected  assigned  coefficient formula    sum training error formula   resulting formulastage boost classifier  minimized  formula   boosted classifier    built    previous stage  training formula   error function  formula   weak learner    considered  addition   final classifier weighting   iteration   training process  weight formula  assigned   sample   training set equal   current error formula   sample  weights can  used  inform  training   weak learner  instance decision trees can  grown  favor splitting sets  samples  high weights derivation  derivation follows rojas suppose    data set formula   item formula   associated class formula   set  weak classifiers formula    outputs  classification formula   item   formulath iteration  boosted classifier   linear combination   weak classifiers   form   formulath iteration  want  extend    better boosted classifier  adding  multiple  one   weak classifiers   remains  determine  weak classifier   best choice  formula    weight formula    define  total error formula  formula    sum   exponential loss   data point given  follows letting formula  formula  formula    can split  summation   data points   correctly classified  formula  formula     misclassified  formula since   part   righthand side   equation  depends  formula  formula  see   formula  minimizes formula   one  minimizes formula ie  weak classifier   lowest weighted error  weights formula  order  determine  desired weight formula  minimizes formula   formula   just determined  differentiate setting   zero  solving  formula yields  calculate  weighted error rate   weak classifier   formula   follows     negative logit function multiplied  thus   derived  adaboost algorithm   iteration choose  classifier formula  minimizes  total weighted error formula use   calculate  error rate formula use   calculate  weight formula  finally use   improve  boosted classifier formula  formula statistical understanding  boosting boosting   form  linear regression    features   sample formula   outputs   weak learner formula applied  formula specifically   case   weak learners  known  priori adaboost corresponds   single iteration   backfitting algorithm    smoothing splines   minimizers  formula   formula fits  exponential cost function   linear  respect   observation thus boosting  seen    specific type  linear regression  regression tries  fit formula  formula  precisely  possible without loss  generalization typically using least square error formula  adaboost error function formula takes  account  fact    sign   final result will  used thus formula can  far larger  without increasing error however  exponential increase   error  sample formula  formula increases results  excessive weight  assigned  outliers one feature   choice  exponential error function    error   final additive model   product   error   stage   formula thus  can  seen   weight update   adaboost algorithm  equivalent  recalculating  error  formula   stage    lot  flexibility allowed   choice  loss function  long   loss function  monotonic  continuously differentiable  classifier will always  driven toward purer solutions zhang provides  loss function based  least squares  modified huber loss function  function   wellbehaved  logitboost  formula close     penalise overconfident predictions formula unlike unmodified least squares   penalises samples misclassified  confidence greater  linearly  opposed  quadratically  exponentially   thus less susceptible   effects  outliers boosting  gradient descent boosting can  seen  minimization   convex loss function   convex set  functions specifically  loss  minimized  adaboost   exponential loss formula whereas logitboost performs logistic regression minimizing formula   gradient descent analogy  output   classifier   training point  considered    point formula  ndimensional space   axis corresponds   training sample  weak learner formula corresponds   vector  fixed orientation  length   goal   reach  target point formula   region   value  loss function formula  less   value   point   least number  steps thus adaboost algorithms perform either cauchy find formula   steepest gradient choose formula  minimize test error  newton choose  target point find formula  will bring formula closest   point optimization  training error example algorithm discrete adaboost   formula  formula choosing formula  chosen   can  analytically shown    minimizer   exponential error function  discrete adaboost minimize formula using  convexity   exponential function  assuming  formula   formula   differentiate  expression  respect  formula  set   zero  find  minimum   upper bound formula note    applies  formula though  can   good starting guess   cases     weak learner  biased formula  multiple leaves formula     function formula   cases  choice  weak learner  coefficient can  condensed   single step   formula  chosen   possible formula   minimizer  formula   numerical searching routine variants real adaboost  output  decision trees   class probability estimate formula  probability  formula    positive class friedman hastie  tibshirani derive  analytical minimizer  formula   fixed formula typically chosen using weighted least squares error thus rather  multiplying  output   entire tree   fixed value  leaf node  changed  output half  logit transform   previous value logitboost logitboost represents  application  established logistic regression techniques   adaboost method rather  minimizing error  respect  y weak learners  chosen  minimize  weighted leastsquares error  formula  respect    formula   newtonraphson approximation   minimizer   loglikelihood error  stage formula   weak learner formula  chosen   learner  best approximates formula  weighted least squares  p approaches either   value  formula becomes  small   z term  will  large  misclassified samples can become numerically unstable due  machine precision rounding errors  can  overcome  enforcing  limit   absolute value  z   minimum value  w gentle adaboost  previous boosting algorithms choose formula greedily minimizing  overall test error  much  possible   step gentleboost features  bounded step size formula  chosen  minimize formula    coefficient  applied thus   case   weak learner exhibits perfect classification performance gentleboost will choose formula exactly equal  formula  steepest descent algorithms will try  set formula empirical observations   good performance  gentleboost appear  back  schapire  singers remark  allowing excessively large values  formula can lead  poor generalization performance early termination  technique  speeding  processing  boosted classifiers early termination refers   testing  potential object   many layers   final classifier necessary  meet  confidence threshold speeding  computation  cases   class   object can easily  determined one  scheme   object detection framework introduced  viola  jones   application  significantly  negative samples  positive  cascade  separate boost classifiers  trained  output   stage biased    acceptably small fraction  positive samples  mislabeled  negative   samples marked  negative   stage  discarded   negative samples  filtered    stage    small number  objects  pass   entire classifier reducing computation effort  method  since  generalized   formula provided  choosing optimal thresholds   stage  achieve  desired false positive  false negative rate   field  statistics  adaboost   commonly applied  problems  moderate dimensionality early stopping  used   strategy  reduce overfitting  validation set  samples  separated   training set performance   classifier   samples used  training  compared  performance   validation samples  training  terminated  performance   validation sample  seen  decrease even  performance   training set continues  improve totally corrective algorithms  steepest descent versions  adaboost  formula  chosen   layer t  minimize test error  next layer added  said   maximally independent  layer t   unlikely   weak learner t will  chosen   similar  learner t however  remains  possibility  t produces similar information    earlier layer totally corrective algorithms   lpboost optimize  value  every coefficient   step   new layers added  always maximally independent  every previous layer  can  accomplished  backfitting linear programming    method pruning pruning refers   process  removing poorly performing weak classifiers  order  improve  memory  executiontime cost   boosted classifier  simplest methods  can  particularly effective  conjunction  totally corrective training  weight  margintrimming   coefficient   contribution   total test error   weak classifier falls   certain threshold  classifier  dropped margineantu dietterich suggest  alternative criterion  trimming weak classifiers   selected    diversity   ensemble  maximized  two weak learners produce  similar outputs efficiency can  improved  removing one    increasing  coefficient   remaining weak learner\r\n"}
{"index":{"_id":89}}
{"conceptLabelTag":"sentiment analysis","conceptLabel":"sentiment analysis","conceptDescription":"sentiment analysis sentiment analysis sometimes known  opinion mining  emotion ai refers   use  natural language processing text analysis computational linguistics  biometrics  systematically identify extract quantify  study affective states  subjective information sentiment analysis  widely applied  voice   customer materials   reviews  survey responses online  social media  healthcare materials  applications  range  marketing  customer service  clinical medicine generally speaking sentiment analysis aims  determine  attitude   speaker writer   subject  respect   topic   overall contextual polarity  emotional reaction   document interaction  event  attitude may   judgment  evaluation see appraisal theory affective state    say  emotional state   author  speaker   intended emotional communication    say  emotional effect intended   author  interlocutor types  basic task  sentiment analysis  classifying  polarity   given text   document sentence  featureaspect levelwhether  expressed opinion   document  sentence   entity featureaspect  positive negative  neutral advanced beyond polarity sentiment classification looks  instance  emotional states   angry sad  happy early work   area includes turney  pang  applied different methods  detecting  polarity  product reviews  movie reviews respectively  work    document level one can also classify  documents polarity   multiway scale   attempted  pang  snyder among others pang  lee expanded  basic task  classifying  movie review  either positive  negative  predict star ratings  either    star scale  snyder performed  indepth analysis  restaurant reviews predicting ratings  various aspects   given restaurant    food  atmosphere   fivestar scale even though   statistical classification methods  neutral class  ignored   assumption  neutral texts lie near  boundary   binary classifier several researchers suggest    every polarity problem three categories must  identified moreover  can  proven  specific classifiers    max entropy   svms can benefit   introduction   neutral class  improve  overall accuracy   classification    principle two ways  operating   neutral class either  algorithm proceeds  first identifying  neutral language filtering     assessing  rest  terms  positive  negative sentiments   builds  three way classification  one step  second approach often involves estimating  probability distribution   categories eg naive bayes classifiers  implemented  pythons nltk kit whether    use  neutral class depends   nature   data   data  clearly clustered  neutral negative  positive language  makes sense  filter  neutral language   focus   polarity  positive  negative sentiments   contrast  data  mostly neutral  small deviations towards positive  negative affect  strategy  make  harder  clearly distinguish   two poles  different method  determining sentiment   use   scaling system whereby words commonly associated    negative neutral  positive sentiment    given  associated number    scale  negative    positive  makes  possible  adjust  sentiment   given term relative   environment usually   level   sentence   piece  unstructured text  analyzed using natural language processing  concept   specified environment  given  score based   way sentiment words relate   concept   associated score  allows movement    sophisticated understanding  sentiment    now possible  adjust  sentiment value   concept relative  modifications  may surround  words  example  intensify relax  negate  sentiment expressed   concept can affect  score alternatively texts can  given  positive  negative sentiment strength score   goal   determine  sentiment   text rather   overall polarity  strength   text subjectivityobjectivity identification  task  commonly defined  classifying  given text usually  sentence  one  two classes objective  subjective  problem can sometimes   difficult  polarity classification  subjectivity  words  phrases may depend   context   objective document may contain subjective sentences eg  news article quoting peoples opinions moreover  mentioned  su results  largely dependent   definition  subjectivity used  annotating texts however pang showed  removing objective sentences   document  classifying  polarity helped improve performance featureaspectbased  refers  determining  opinions  sentiments expressed  different features  aspects  entities eg   cell phone  digital camera   bank  feature  aspect   attribute  component   entity eg  screen   cell phone  service   restaurant   picture quality   camera  advantage  featurebased sentiment analysis   possibility  capture nuances  objects  interest different features can generate different sentiment responses  example  hotel can   convenient location  mediocre food  problem involves several subproblems eg identifying relevant entities extracting  featuresaspects  determining whether  opinion expressed   featureaspect  positive negative  neutral  automatic identification  features can  performed  syntactic methods   topic modeling  detailed discussions   level  sentiment analysis can  found  lius work methods  features existing approaches  sentiment analysis can  grouped  three main categories knowledgebased techniques statistical methods  hybrid approaches knowledgebased techniques classify text  affect categories based   presence  unambiguous affect words   happy sad afraid  bored  knowledge bases   list obvious affect words  also assign arbitrary words  probable affinity  particular emotions statistical methods leverage  elements  machine learning   latent semantic analysis support vector machines bag  words  semantic orientation pointwise mutual information see peter turneys work   area  sophisticated methods try  detect  holder   sentiment ie  person  maintains  affective state   target ie  entity    affect  felt  mine  opinion  context  get  feature    opinionated  grammatical relationships  words  used grammatical dependency relations  obtained  deep parsing   text hybrid approaches leverage   machine learning  elements  knowledge representation   ontologies  semantic networks  order  detect semantics   expressed   subtle manner eg   analysis  concepts    explicitly convey relevant information    implicitly linked   concepts    open source software tools deploy machine learning statistics  natural language processing techniques  automate sentiment analysis  large collections  texts including web pages online news internet discussion groups online reviews web blogs  social media knowledgebased systems    hand make use  publicly available resources  extract  semantic  affective information associated  natural language concepts sentiment analysis can also  performed  visual content ie images  videos one   first approach   direction  sentibank utilizing  adjective noun pair representation  visual content  human analysis component  required  sentiment analysis  automated systems   able  analyze historical tendencies   individual commenter   platform   often classified incorrectly   expressed sentiment automation impacts approximately  comments   correctly classified  humans however also humans often disagree    argued   interhuman agreement provides  upper bound  automated sentiment classifiers can eventually reach sometimes  structure  sentiments  topics  fairly complex also  problem  sentiment analysis  nonmonotonic  respect  sentence extension  stopword substitution compare    let  dog stay   hotel vs    let  dog stay   hotel  address  issue  number  rulebased  reasoningbased approaches   applied  sentiment analysis including defeasible logic programming also    number  tree traversal rules applied  syntactic parse tree  extract  topicality  sentiment  open domain setting evaluation  accuracy   sentiment analysis system   principle  well  agrees  human judgments   usually measured  precision  recall however according  research human raters typically agree   time see interrater reliability thus  accurate program   nearly  well  humans even though  accuracy may  sound impressive   program  right   time humans  still disagree      time since  disagree  much   answer  sophisticated measures can  applied  evaluation  sentiment analysis systems remains  complex matter  sentiment analysis tasks returning  scale rather   binary judgement correlation   better measure  precision   takes  account  close  predicted value    target value web  rise  social media   blogs  social networks  fueled interest  sentiment analysis   proliferation  reviews ratings recommendations   forms  online expression online opinion  turned   kind  virtual currency  businesses looking  market  products identify new opportunities  manage  reputations  businesses look  automate  process  filtering   noise understanding  conversations identifying  relevant content  actioning  appropriately many  now looking   field  sentiment analysis  complicating  matter   rise  anonymous social media platforms   chan  reddit  web    democratizing publishing   next stage   web may well  based  democratizing data mining    content   getting published one step towards  aim  accomplished  research several research teams  universities around  world currently focus  understanding  dynamics  sentiment  ecommunities  sentiment analysis  cyberemotions project  instance recently identified  role  negative emotions  driving social networks discussions  problem    sentiment analysis algorithms use simple terms  express sentiment   product  service however cultural factors linguistic nuances  differing contexts make  extremely difficult  turn  string  written text   simple pro  con sentiment  fact  humans often disagree   sentiment  text illustrates  big  task    computers  get  right  shorter  string  text  harder  becomes even though short text strings might   problem sentiment analysis within microblogging  shown  twitter can  seen   valid online indicator  political sentiment tweets political sentiment demonstrates close correspondence  parties  politicians political positions indicating   content  twitter messages plausibly reflects  offline political landscape\r\n"}
{"index":{"_id":90}}
{"conceptLabelTag":"bagging","conceptLabel":"bagging","conceptDescription":"bootstrap aggregating bootstrap aggregating also called bagging   machine learning ensemble metaalgorithm designed  improve  stability  accuracy  machine learning algorithms used  statistical classification  regression  also reduces variance  helps  avoid overfitting although   usually applied  decision tree methods  can  used   type  method bagging   special case   model averaging approach history bagging bootstrap aggregating  proposed  leo breiman   improve  classification  combining classifications  randomly generated training sets see breiman technical report  description   technique given  standard training set d  size n bagging generates m new training sets formula   size n  sampling  d uniformly   replacement  sampling  replacement  observations may  repeated   formula  nn   large n  set formula  expected    fraction e   unique examples  d  rest  duplicates  kind  sample  known   bootstrap sample  m models  fitted using   m bootstrap samples  combined  averaging  output  regression  voting  classification bagging leads  improvements  unstable procedures breiman  include  example artificial neural networks classification  regression trees  subset selection  linear regression breiman  interesting application  bagging showing improvement  preimage learning  provided     hand  can mildly degrade  performance  stable methods   knearest neighbors breiman example ozone data  illustrate  basic principles  bagging    analysis   relationship  ozone  temperature data  rousseeuw  leroy analysis done  r  relationship  temperature  ozone   data set  apparently nonlinear based   scatter plot  mathematically describe  relationship loess smoothers  span  used instead  building  single smoother   complete data set bootstrap samples   data  drawn  sample  different   original data set yet resembles   distribution  variability   bootstrap sample  loess smoother  fit predictions   smoothers   made across  range   data  first predicted smooth fits appear  grey lines   figure   lines  clearly  wiggly   overfit  data  result   span   low  taking  average  smoothers  fitted   subset   original data set  arrive  one bagged predictor red line clearly  mean   stable    less overfit\r\n"}
{"index":{"_id":91}}
{"conceptLabelTag":"medical diagnosis","conceptLabel":"medical diagnosis","conceptDescription":"medical diagnosis medical diagnosis abbreviated dx  d   process  determining  disease  condition explains  persons symptoms  signs    often referred   diagnosis   medical context  implicit  information required  diagnosis  typically collected   history  physical examination   person seeking medical care often one   diagnostic procedures   diagnostic tests  also done   process sometimes posthumous diagnosis  considered  kind  medical diagnosis diagnosis  often challenging  many signs  symptoms  nonspecific  example redness   skin erythema     sign  many disorders  thus doesnt tell  healthcare professional   wrong thus differential diagnosis   several possible explanations  compared  contrasted must  performed  involves  correlation  various pieces  information followed   recognition  differentiation  patterns occasionally  process  made easy   sign  symptom   group  several   pathognomonic diagnosis   major component   procedure   doctors visit   point  view  statistics  diagnostic procedure involves classification tests history  first recorded examples  medical diagnosis  found   writings  imhotep bc  ancient egypt  edwin smith papyrus  babylonian medical textbook  diagnostic handbook written  esagilkinapli fl bc introduced  use  empiricism logic  rationality   diagnosis   illness  disease traditional chinese medicine  described   yellow emperors inner canon  huangdi neijing specified four diagnostic methods inspection auscultationolfaction interrogation  palpation hippocrates  known  make diagnoses  tasting  patients urine  smelling  sweat medical uses  diagnosis   sense  diagnostic procedure can  regarded   attempt  classification   individuals condition  separate  distinct categories  allow medical decisions  treatment  prognosis   made subsequently  diagnostic opinion  often described  terms   disease   condition    case   wrong diagnosis  individuals actual disease  condition       individuals diagnosis  diagnostic procedure may  performed  various health care professionals    physician physical therapist optometrist healthcare scientist chiropractor dentist podiatrist nurse practitioner  physician assistant  article uses diagnostician     person categories  diagnostic procedure  well   opinion reached thereby   necessarily involve elucidation   etiology   diseases  conditions  interest    caused  disease  condition  elucidation can  useful  optimize treatment  specify  prognosis  prevent recurrence   disease  condition   future  initial task   detect  medical indication  perform  diagnostic procedure indications include even   already ongoing diagnostic procedure  can   indication  perform another separate diagnostic procedure  another potentially concomitant disease  condition  may occur   result   incidental finding   sign unrelated   parameter  interest   can occur  comprehensive tests   radiological studies like magnetic resonance imaging  blood test panels  also include blood tests    relevant   ongoing diagnosis procedure general components   present   diagnostic procedure     various available methods include    number  methods  techniques  can  used   diagnostic procedure including performing  differential diagnosis  following medical algorithms  reality  diagnostic procedure may involve components  multiple methods differential diagnosis  method  differential diagnosis  based  finding  many candidate diseases  conditions  possible  can possibly cause  signs  symptoms followed   process  elimination   least  rendering  entries   less probable   medical tests   processing  aiming  reach  point   one candidate disease  condition remains  probable  final result may also remain  list  possible conditions ranked  order  probability  severity  resultant diagnostic opinion   method can  regarded   less   diagnosis  exclusion even   doesnt result   single probable disease  condition  can  least rule   imminently lifethreatening conditions unless  provider  certain   condition present  medical tests   medical imaging  performed  scheduled  part  confirm  disprove  diagnosis  also  document  patients status  keep  patients medical history   date  unexpected findings  made   process  initial hypothesis may  ruled    provider must  consider  hypotheses pattern recognition   pattern recognition method  provider uses experience  recognize  pattern  clinical characteristics   mainly based  certain symptoms  signs  associated  certain diseases  conditions  necessarily involving   cognitive processing involved   differential diagnosis  may   primary method used  cases  diseases  obvious   providers experience may enable     recognize  condition quickly theoretically  certain pattern  signs  symptoms can  directly associated   certain therapy even without  definite decision regarding    actual disease    compromise carries  substantial risk  missing  diagnosis  actually   different therapy   may  limited  cases   diagnosis can  made diagnostic criteria  term diagnostic criteria designates  specific combination  signs symptoms  test results   clinician uses  attempt  determine  correct diagnosis  examples  diagnostic criteria also known  clinical case definitions  clinical decision support system clinical decision support systems  interactive computer programs designed  assist health professionals  decisionmaking tasks  clinician interacts   software utilizing   clinicians knowledge   software  make  better analysis   patients data  either human  software  make    typically  system makes suggestions   clinician  look    clinician picks useful information  removes erroneous suggestions  diagnostic procedure methods  methods  can  used  performing  diagnostic procedure include adverse effects diagnosis problems   dominant cause  medical malpractice payments accounting   total payments   study  years  data  claims overdiagnosis overdiagnosis   diagnosis  disease  will never cause symptoms  death   patients lifetime    problem   turns people  patients unnecessarily    can lead  economic waste overutilization  treatments  may cause harm overdiagnosis occurs   disease  diagnosed correctly   diagnosis  irrelevant  correct diagnosis may  irrelevant  treatment   disease   available  needed   wanted errors  people will experience  least one diagnostic error   lifetime according   report   national academies  sciences engineering  medicine causes  factors  error  diagnosis  lag time  making  medical diagnosis  lag time   delay  time   step towards diagnosis   disease  condition  made types  lag times  mainly society  culture etymology  plural  diagnosis  diagnoses  verb   diagnose   person  diagnoses  called  diagnostician  word  derived  latin   greek word  meaning  discern distinguish medical diagnosis   actual process  making  diagnosis   cognitive process  clinician uses several sources  data  puts  pieces   puzzle together  make  diagnostic impression  initial diagnostic impression can   broad term describing  category  diseases instead   specific disease  condition   initial diagnostic impression  clinician obtains follow  tests  procedures  get  data  support  reject  original diagnosis  will attempt  narrow      specific level diagnostic procedures   specific tools   clinicians use  narrow  diagnostic possibilities social context diagnosis can take many forms  might   matter  naming  disease lesion dysfunction  disability  might   managementnaming  prognosisnaming exercise  may indicate either degree  abnormality   continuum  kind  abnormality   classification  influenced  nonmedical factors   power ethics  financial incentives  patient  doctor  can   brief summation   extensive formulation even taking  form   story  metaphor  might   means  communication    computer code    triggers payment prescription notification information  advice  might  pathogenic  salutogenic  generally uncertain  provisional   diagnostic opinion   reached  provider  able  propose  management plan  will include treatment  well  plans  followup   point   addition  treating  patients condition  provider can educate  patient   etiology progression prognosis  outcomes  possible treatments     ailments  well  providing advice  maintaining health  treatment plan  proposed  may include therapy  followup consultations  tests  monitor  condition   progress   treatment  needed usually according   medical guidelines provided   medical field   treatment   particular illness relevant information   added   medical record   patient  failure  respond  treatments   normally work may indicate  need  review   diagnosis concepts related  diagnosis subtypes  diagnoses include\r\n"}
{"index":{"_id":92}}
{"conceptLabelTag":"structural risk minimization","conceptLabel":"structural risk minimization","conceptDescription":"structural risk minimization structural risk minimization srm   inductive principle  use  machine learning commonly  machine learning  generalized model must  selected   finite data set   consequent problem  overfitting  model becoming  strongly tailored   particularities   training set  generalizing poorly  new data  srm principle addresses  problem  balancing  models complexity   success  fitting  training data  srm principle  first set    paper  vladimir vapnik  alexey chervonenkis  uses  vc dimension\r\n"}
{"index":{"_id":93}}
{"conceptLabelTag":"version space","conceptLabel":"version space","conceptDescription":"version space learning version space learning   logical approach  machine learning specifically binary classification version space learning algorithms search  predefined space  hypotheses viewed   set  logical sentences formally  hypothesis space   disjunction ie either hypothesis  true  hypothesis   subset   hypotheses   version space learning algorithm  presented  examples   will use  restrict  hypothesis space   example  hypotheses   inconsistent   removed   space  iterative refining   hypothesis space  called  candidate elimination algorithm  hypothesis space maintained inside  algorithm  version space  version space algorithm  settings     generalityordering  hypotheses   possible  represent  version space  two sets  hypotheses   specific consistent hypotheses    general consistent hypotheses  consistent indicates agreement  observed data   specific hypotheses ie  specific boundary sb cover  observed positive training examples   little   remaining feature space  possible  hypotheses  reduced   exclude  positive training example  hence become inconsistent  minimal hypotheses essentially constitute  pessimistic claim   true concept  defined just   positive data already observed thus   novel neverbeforeseen data point  observed    assumed   negative ie  data   previously  ruled    ruled    general hypotheses ie  general boundary gb cover  observed positive training examples  also cover  much   remaining feature space without including  negative training examples   enlarged   include  negative training example  hence become inconsistent  maximal hypotheses essentially constitute  optimistic claim   true concept  defined just   negative data already observed thus   novel neverbeforeseen data point  observed    assumed   positive ie  data   previously  ruled    ruled  thus  learning  version space     set possibly infinite containing  consistent hypotheses can  represented  just  lower  upper bounds maximally general  maximally specific hypothesis sets  learning operations can  performed just   representative sets  learning classification can  performed  unseen examples  testing  hypothesis learned   algorithm   example  consistent  multiple hypotheses  majority vote rule can  applied historical background  notion  version spaces  introduced  mitchell   early s   framework  understanding  basic problem  supervised learning within  context  solution search although  basic candidate elimination search method  accompanies  version space framework    popular learning algorithm    practical implementations    developed eg sverdlik reynolds hong tsang dubois quafafou  major drawback  version space learning   inability  deal  noise  pair  inconsistent examples can cause  version space  collapse ie become empty   classification becomes impossible\r\n"}
{"index":{"_id":94}}
{"conceptLabelTag":"occams razor","conceptLabel":"occams razor","conceptDescription":"occams razor occams razor also ockhams razor latin lex parsimoniae law  parsimony   problemsolving principle attributed  william  ockham c    english franciscan friar scholastic philosopher  theologian  principle can  interpreted  stating among competing hypotheses  one   fewest assumptions   selected  science occams razor  used   heuristic technique discovery tool  guide scientists   development  theoretical models rather    arbiter  published models   scientific method occams razor   considered  irrefutable principle  logic   scientific result  preference  simplicity   scientific method  based   falsifiability criterion   accepted explanation   phenomenon  may   extremely large perhaps even incomprehensible number  possible   complex alternatives  one can always burden failing explanations  ad hoc hypotheses  prevent    falsified therefore simpler theories  preferable   complex ones     testable history  term occams razor   appear    centuries  william  ockhams death  libert froidmont    christian philosophy   soul takes credit   phrase speaking  novacula occami ockham   invent  principle   razorits association   may  due   frequency  effectiveness    used  ockham stated  principle  various ways    popular version entities must   multiplied beyond necessity non sunt multiplicanda entia sine necessitate  formulated   irish franciscan philosopher john punch   commentary   works  duns scotus formulations  ockham  origins    come   known  occams razor  traceable   works  earlier philosophers   john duns scotus robert grosseteste maimonides moses benmaimon  even aristotle bc aristotle writes   posterior analytics  may assume  superiority ceteris paribus  things  equal   demonstration  derives  fewer postulates  hypotheses ptolemy c ad c ad stated  consider   good principle  explain  phenomena   simplest hypothesis possible phrases     vain      can  done  fewer   plurality     posited without necessity  commonplace  thcentury scholastic writing robert grosseteste  commentary  aristotles  posterior analytics books commentarius  posteriorum analyticorum libros c declares   better   valuable  requires fewer  circumstances  equal   one thing  demonstrated  many  another thing  fewer equally known premises clearly   better    fewer   makes us know quickly just   universal demonstration  better  particular   produces knowledge  fewer premises similarly  natural science  moral science   metaphysics  best    needs  premises   better   needs  fewer  circumstances  equal  summa theologica  thomas aquinas states    superfluous  suppose   can  accounted     principles   produced  many aquinas uses  principle  construct  objection  gods existence  objection    turn answers  refutes generally cf quinque viae  specifically   argument based  causality hence aquinas acknowledges  principle  today  known  occams razor  prefers causal explanations   simple explanations cf also correlation   imply causation  indian hindu philosopher madhva  verse   vishnutattvanirnaya says dvidhakalpane kalpanagauravamiti  make two suppositions  one  enough   err  way  excessive supposition ockham william  ockham circa   english franciscan friar  theologian  influential medieval philosopher   nominalist  popular fame   great logician rests chiefly   maxim attributed    known  ockhams razor  term razor refers  distinguishing  two hypotheses either  shaving away unnecessary assumptions  cutting apart two similar conclusions     claimed  ockhams razor   found     writings one can cite statements   plurality must never  posited without necessity  occurs   theological work   sentences  peter lombard quaestiones et decisiones  quattuor libros sententiarum petri lombardi ed lugd  dist qu k nevertheless  precise words sometimes attributed  ockham entia non sunt multiplicanda praeter necessitatem entities must   multiplied beyond necessity  absent   extant works  particular phrasing comes  john punch  described  principle   common axiom axioma vulgare   scholastics ockhams contribution seems    restrict  operation   principle  matters pertaining  miracles  gods power    eucharist  plurality  miracles  possible simply   pleases god  principle  sometimes phrased  pluralitas non est ponenda sine necessitate plurality    posited without necessity   summa totius logicae  ockham cites  principle  economy frustra fit per plura quod potest fieri per pauciora   futile     things   can  done  fewer thorburn pp kneale  kneale p later formulations  quote isaac newton    admit   causes  natural things      true  sufficient  explain  appearances therefore    natural effects  must  far  possible assign   causes bertrand russell offers  particular version  occams razor whenever possible substitute constructions   known entities  inferences  unknown entities around ray solomonoff founded  theory  universal inductive inference  theory  prediction based  observations  example predicting  next symbol based upon  given series  symbols   assumption    environment follows  unknown  computable probability distribution  theory   mathematical formalization  occams razor another technical approach  occams razor  ontological parsimony  widespread laymans formulation   simplest explanation  usually  correct one appears    derived  occams razor justifications beginning   th century epistemological justifications based  induction logic pragmatism  especially probability theory  become  popular among philosophers aesthetic prior   th century    commonly held belief  nature   simple   simpler hypotheses  nature  thus  likely   true  notion  deeply rooted   aesthetic value  simplicity holds  human thought   justifications presented   often drew  theology thomas aquinas made  argument   th century writing   thing can  done adequately  means  one   superfluous     means  several   observe  nature   employ two instruments  one suffices empirical occams razor  gained strong empirical support  helping  converge  better theories see applications section    examples   related concept  overfitting excessively complex models  affected  statistical noise  problem also known   biasvariance tradeoff whereas simpler models may capture  underlying structure better  may thus  better predictive performance   however often difficult  deduce  part   data  noise cf model selection test set minimum description length bayesian inference etc testing  razor  razors statement   things  equal simpler explanations  generally better   complex ones  amenable  empirical testing another interpretation   razors statement    simpler hypotheses  conclusions ie explanations  generally better   complex ones  procedure  test  former interpretation  compare  track records  simple  comparatively complex explanations  one accepts  first interpretation  validity  occams razor   tool      rejected    complex explanations   often correct   less complex ones   converse  lend support   use   latter interpretation  accepted  validity  occams razor   tool  possibly  accepted   simpler hypotheses led  correct conclusions  often     history  competing hypotheses  simpler hypotheses  led  mathematically rigorous  empirically verifiable theories   history  competing explanations     caseat least  generally  increases  complexity  sometimes necessary   remains  justified general bias toward  simpler  two competing explanations  understand  consider    accepted explanation   phenomenon   always  infinite number  possible  complex  ultimately incorrect alternatives     one can always burden failing explanations  ad hoc hypothesis ad hoc hypotheses  justifications  prevent theories   falsified even  empirical criteria   consilience can never truly eliminate  explanations  competition  true explanation  may   many alternatives   simpler  false  also  infinite number  alternatives    complex  false    alternate ad hoc hypothesis  indeed justifiable  implicit conclusions   empirically verifiable   commonly accepted repeatability principle  alternate theories  never  observed  continue  escape observation  addition one   say  explanation  true     withstood  principle put another way  new  even  complex theory can still possibly  true  example   individual makes supernatural claims  leprechauns  responsible  breaking  vase  simpler explanation      mistaken  ongoing ad hoc justifications eg  thats     film  tampered    successfully prevent outright falsification  endless supply  elaborate competing explanations called saving hypotheses   ruled outexcept  using occams razor  study   predictive validity  occams razor found published papers  included comparisons  forecasts  simple  complex forecasting methods none   papers provided  balance  evidence  complexity  method improved forecast accuracy   papers  quantitative comparisons complexity increased forecast errors   average  percent practical considerations  pragmatism  common form   razor used  distinguish  equally explanatory hypotheses may  supported   practical fact  simpler theories  easier  understand  argue  occams razor    inferencedriven model   heuristic maxim  choosing among  models  instead underlies induction alternatively  one wants   reasonable discussion one may  practically forced  accept occams razor    way one  simply forced  accept  laws  thought  inductive reasoning given  problem  induction philosopher elliott sober states   even reason  can  justified   reasonable grounds    must start  first principles   kind otherwise  infinite regress occurs  pragmatist may go   david hume    topic  induction     satisfying alternative  granting  premise though one may claim  occams razor  invalid   premise  helps regulate theories putting  doubt  practice  mean doubting whether every step forward will result  locomotion   nuclear explosion   words whats  alternative mathematical one justification  occams razor   direct result  basic probability theory  definition  assumptions introduce possibilities  error   assumption   improve  accuracy   theory   effect   increase  probability   overall theory  wrong   also   attempts  derive occams razor  probability theory including notable attempts made  harold jeffreys  e t jaynes  probabilistic bayesian basis  occams razor  elaborated  david j c mackay  chapter   book information theory inference  learning algorithms   emphasises   prior bias  favour  simpler models   required william h jefferys  relation  harold jeffreys  james o berger generalize  quantify  original formulations assumptions concept   degree    proposition  unnecessarily accommodating  possible observable data  state  hypothesis  fewer adjustable parameters will automatically   enhanced posterior probability due   fact   predictions  makes  sharp  model  propose balances  precision   theorys predictions   sharpnesspreferring theories  sharply make correct predictions  theories  accommodate  wide range   possible results   reflects  mathematical relationship  key concepts  bayesian inference namely marginal probability conditional probability  posterior probability  philosophers karl popper karl popper argues   preference  simple theories need  appeal  practical  aesthetic considerations  preference  simplicity may  justified   falsifiability criterion  prefer simpler theories   complex ones   empirical content  greater     better testable popper  idea     simple theory applies   cases    complex one   thus  easily falsifiable    comparing  simple theory    complex theory   explain  data equally well elliott sober  philosopher  science elliott sober  argued along   lines  popper tying simplicity  informativeness  simplest theory    informative   sense   requires less information   question   since rejected  account  simplicity purportedly   fails  provide  epistemic justification  simplicity  now believes  simplicity considerations  considerations  parsimony  particular   count unless  reflect something  fundamental philosophers  suggests may  made  error  hypostatizing simplicity ie endowed    sui generis existence    meaning   embedded   specific context sober   fail  justify simplicity considerations   basis   context    use   may   noncircular justification just   question   rational may   noncircular answer   may  true   question   simplicity  considered  evaluating  plausibility  hypotheses richard swinburne richard swinburne argues  simplicity  logical grounds according  swinburne since  choice  theory   determined  data see underdetermination  quineduhem thesis  must rely   criterion  determine  theory  use since   absurd    logical method  settling  one hypothesis amongst  infinite number  equally datacompliant hypotheses   choose  simplest theory either science  irrational   way  judges theories  predictions probable   principle  simplicity   fundamental synthetic  priori truth swinburne ludwig wittgenstein   tractatus logicophilosophicus    related concept  simplicity applications science   scientific method  science occams razor  used   heuristic  guide scientists  developing theoretical models rather    arbiter  published models  physics parsimony   important heuristic  albert einsteins formulation  special relativity   development  application   principle  least action  pierre louis maupertuis  leonhard euler    development  quantum mechanics  max planck werner heisenberg  louis de broglie  chemistry occams razor  often  important heuristic  developing  model   reaction mechanism although   useful   heuristic  developing models  reaction mechanisms    shown  fail   criterion  selecting among  selected published models   context einstein  expressed caution   formulated einsteins constraint  can scarcely  denied   supreme goal   theory   make  irreducible basic elements  simple     possible without   surrender  adequate representation   single datum  experience  oftenquoted version   constraint    verified  posited  einstein  says everything   kept  simple  possible   simpler   scientific method parsimony   epistemological metaphysical  heuristic preference   irrefutable principle  logic   scientific result   logical principle occams razor  demand  scientists accept  simplest possible theoretical explanation  existing data however science  shown repeatedly  future data often support  complex theories   existing data science prefers  simplest explanation   consistent   data available   given time   simplest explanation may  ruled   new data become available   science  open   possibility  future experiments might support  complex theories  demanded  current data    interested  designing experiments  discriminate  competing theories  favoring one theory  another based merely  philosophical principles  scientists use  idea  parsimony   meaning     specific context  inquiry several background assumptions  required  parsimony  connect  plausibility   particular research problem  reasonableness  parsimony  one research context may  nothing     reasonableness  another    mistake  think     single global principle  spans diverse subject matter    suggested  occams razor   widely accepted example  extraevidential consideration even though   entirely  metaphysical assumption   little empirical evidence   world  actually simple   simple accounts   likely   true  complex ones    time occams razor   conservative tool cutting  crazy complicated constructions  assuring  hypotheses  grounded   science   day thus yielding normal science models  explanation  prediction   however notable exceptions  occams razor turns  conservative scientist   reluctant revolutionary  example max planck interpolated   wien  jeans radiation laws  used occams razor logic  formulate  quantum hypothesis even resisting  hypothesis   became  obvious    correct appeals  simplicity  used  argue   phenomena  meteorites ball lightning continental drift  reverse transcriptase one can argue  atomic building blocks  matter   provides  simpler explanation   observed reversibility   mixing  chemical reactions  simple separation  rearrangements  atomic building blocks   time however  atomic theory  considered  complex   implied  existence  invisible particles     directly detected ernst mach   logical positivists rejected john daltons atomic theory   reality  atoms   evident  brownian motion  shown  albert einstein    way postulating  aether   complex  transmission  light   vacuum   time however  known waves propagated   physical medium   seemed simpler  postulate  existence   medium   theorize  wave propagation without  medium likewise newtons idea  light particles seemed simpler  christiaan huygenss idea  waves  many favored    case   turned  neither  wavenor  particleexplanation alone suffices  light behaves like waves  like particles three axioms presupposed   scientific method  realism  existence  objective reality  existence  natural laws   constancy  natural law rather  depend  provability   axioms science depends   fact      objectively falsified occams razor  parsimony support    prove  axioms  science  general principle  science   theories  models  natural law must  consistent  repeatable experimental observations  ultimate arbiter selection criterion rests upon  axioms mentioned    examples  occams razor   favored  wrong theory given  available data simplicity principles  useful philosophical preferences  choosing   likely theory  among several possibilities    consistent  available data  single instance  occams razor favoring  wrong theory falsifies  razor   general principle michael lee  others provide cases    parsimonious approach   guarantee  correct conclusion   based  incorrect working hypotheses  interpretations  incomplete data may even strongly support  false conclusion lee states  parsimony ceases    guideline   instead elevated   ex cathedra pronouncement parsimony analysis ceases   science  multiple models  natural law make exactly   testable predictions   equivalent     need  parsimony  choose  preferred one  example newtonian hamiltonian  lagrangian classical mechanics  equivalent physicists   interest  using occams razor  say   two  wrong likewise    demand  simplicity principles  arbitrate  wave  matrix formulations  quantum mechanics science often   demand arbitration  selection criteria  models  make   testable predictions biology biologists  philosophers  biology use occams razor  either  two contexts   evolutionary biology  units  selection controversy  systematics george c williams   book adaptation  natural selection argues   best way  explain altruism among animals  based  lowlevel ie individual selection  opposed  highlevel group selection altruism  defined   evolutionary biologists eg r alexander w d hamilton  behavior   beneficial  others    group   cost   individual  many posit individual selection   mechanism  explains altruism solely  terms   behaviors  individual organisms acting    selfinterest    interest   genes via kin selection williams  arguing   perspective  others  propose selection   level   group   evolutionary mechanism  selects  altruistic traits eg d s wilson e o wilson  basis  williams contention     two individual selection    parsimonious theory      invoking  variant  occams razor known  morgans canon   case   animal activity   interpreted  terms  higher psychological processes   can  fairly interpreted  terms  processes  stand lower   scale  psychological evolution  development morgan however  recent biological analyses   richard dawkins  selfish gene  contended  morgans canon    simplest   basic explanation dawkins argues  way evolution works    genes propagated   copies end  determining  development   particular species ie natural selection turns   select specific genes    really  fundamental underlying principle  automatically gives individual  group selection  emergent features  evolution zoology provides  example muskoxen  threatened  wolves form  circle   males   outside   females  young   inside    example   behavior   males  seems   altruistic  behavior  disadvantageous   individually  beneficial   group   whole   thus seen    support  group selection theory however  much better explanation immediately offers   one considers  natural selection works  genes   male musk ox runs  leaving  offspring   wolves  genes   propagate  however  fights  genes may live    offspring thus  stayandfight gene prevails    example  kin selection  underlying general principle thus offers  much simpler explanation without retreating  special principles  group selection systematics   branch  biology  attempts  establish genealogical relationships among organisms   also concerned   classification   three primary camps  systematics cladists pheneticists  evolutionary taxonomists  cladists hold  genealogy alone  determine classification  pheneticists contend  similarity  propinquity  descent   determining criterion  evolutionary taxonomists say   genealogy  similarity count  classification   among  cladists  occams razor    found although  term    cladistic parsimony cladistic parsimony  maximum parsimony   method  phylogenetic inference   construction  types  phylogenetic trees  specifically cladograms cladograms  branching treelike structures used  represent lines  descent based  one   evolutionary changes cladistic parsimony  used  support  hypotheses  require  fewest evolutionary changes   types  tree  consistently produces  wrong results regardless   much data  collected   called long branch attraction   full treatment  cladistic parsimony see elliott sobers reconstructing  past parsimony evolution  inference   discussion   uses  occams razor  biology see sobers article lets razor ockhams razor  methods  inferring evolutionary relationships use parsimony    traditional way likelihood methods  phylogeny use parsimony      likelihood tests  hypotheses requiring  differing parameters ie numbers  different rates  character change  different frequencies  character state transitions  treated   hypotheses relative  hypotheses requiring many differing parameters thus complex hypotheses must predict data much better   simple hypotheses  researchers reject  simple hypotheses recent advances employ information theory  close cousin  likelihood  uses occams razor    way francis crick  commented  potential limitations  occams razor  biology  advances  argument   biological systems   products   ongoing natural selection  mechanisms   necessarily optimal   obvious sense  cautions  ockhams razor   useful tool   physical sciences  can    dangerous implement  biology   thus  rash  use simplicity  elegance   guide  biological research  biogeography parsimony  used  infer ancient migrations  species  populations  observing  geographic distribution  relationships  existing organisms given  phylogenetic tree ancestral migrations  inferred     require  minimum amount  total movement medicine  discussing occams razor  contemporary medicine doctors  philosophers  medicine speak  diagnostic parsimony diagnostic parsimony advocates   diagnosing  given injury ailment illness  disease  doctor  strive  look   fewest possible causes  account    symptoms  philosophy  one  several demonstrated   popular medical adage   hear hoofbeats behind  think horses  zebras  diagnostic parsimony might often  beneficial credence  also  given   counterargument modernly known  hickams dictum  succinctly states  patients can   many diseases   damn well please   often statistically  likely   patient  several common diseases rather   single rarer disease  explains myriad symptoms also independently  statistical likelihood  patients   fact turn    multiple diseases   common sense ifies  approach  insisting  explain  given collection  symptoms  one disease  misgivings emerge  simple probability theorywhich  already taken  account  many modern variations   razorand   fact   loss function  much greater  medicine     general science  misdiagnosis can result   loss   persons health  potentially life   considered better  test  pursue  reasonable theories even     theory  appears   likely diagnostic parsimony   counterbalance  finds  hickams dictum   important implications  medical practice  set  symptoms   indicative   range  possible diseases  disease combinations though   point   diagnosis rejected  accepted just   basis  one disease appearing  likely  another  continuous flow  hypothesis formulation testing  modification benefits greatly  estimates regarding  diseases  sets  diseases  relatively  likely responsible   set  symptoms given  patients environment habits medical history     example   hypothetical patients immediately apparent symptoms include fatigue  cirrhosis   test negative  hepatitis c  doctor might formulate  working hypothesis   cirrhosis  caused   drinking problem   seek symptoms  perform tests  formulate  rule  hypotheses      causing  fatigue    doctor    discover   patients breath inexplicably smells  garlic    suffering  pulmonary edema  might decide  test   relatively rare condition  selenium poisoning religion   philosophy  religion occams razor  sometimes applied   existence  god william  ockham    christian  believed  god    authority  scripture  writes  nothing    posited without  reason given unless   selfevident literally known    known  experience  proved   authority  sacred scripture ockham believed   explanation   sufficient basis  reality     harmonize  reason experience   bible however unlike many theologians   time ockham   believe god   logically proven  arguments  ockham science   matter  discovery  theology   matter  revelation  faith  states  faith gives us access  theological truths  ways  god   open  reason  god  ly chosen  create  world  establish  way  salvation within  apart   necessary laws  human logic  rationality can uncover st thomas aquinas   summa theologica uses  formulation  occams razor  construct  objection   idea  god exists   refutes directly   counterargument    superfluous  suppose   can  accounted     principles   produced  many   seems  everything  see   world can  accounted    principles supposing god   exist   natural things can  reduced  one principle   nature   voluntary things can  reduced  one principle   human reason  will therefore    need  suppose gods existence  turn aquinas answers    quinque viae  addresses  particular objection    following answer since nature works   determinate end   direction   higher agent whatever  done  nature must needs  traced back  god    first cause  also whatever  done voluntarily must also  traced back   higher cause   human reason  will since  can change  fail   things   changeable  capable  defect must  traced back   immovable  selfnecessary first principle   shown   body   article rather  argue   necessity   god  theists base  belief upon grounds independent   prior  reason making occams razor irrelevant    stance  s ren kierkegaard  viewed belief  god   leap  faith  sometimes directly opposed reason   also  doctrine  gordon clarks presuppositional apologetics   exception  clark never thought  leap  faith  contrary  reason see also fideism various arguments  favor  god establish god   useful  even necessary assumption contrastingly  atheists hold firmly   belief  assuming  existence  god introduces unnecessary complexity schmitt eg  ultimate boeing gambit taking  nuanced position philosopher del ratzsch suggests   application   razor  god may    simple least      comparing  hypothesis  theories postulating multiple invisible universes another application   principle    found   work  george berkeley berkeley   idealist  believed    reality   explained  terms   mind alone  invoked occams razor  materialism stating  matter   required   metaphysic   thus eliminable one potential problem   belief    possible given berkeleys position  find solipsism    line   razor   godmediated world beyond  single thinker occams razor may also  recognized   apocryphal story   exchange  pierresimon laplace  napoleon   said   praising laplace  one   recent publications  emperor asked      name  god  featured  frequently   writings  lagrange appeared nowhere  laplaces     said   replied      need   hypothesis though  point   story  illustrating laplaces atheism  careful consideration suggests   may instead  intended merely  illustrate  power  methodological naturalism  even simply   fewer logical premises one assumes  stronger  ones conclusion   article sensations  brain processes j j c smart invoked occams razor   aim  justify  preference   mindbrain identity theory  spiritbody dualism dualists state    two kinds  substances   universe physical including  body  spiritual   nonphysical  contrast identity theorists state  everything  physical including consciousness     nothing nonphysical though   impossible  appreciate  spiritual  limiting oneself   physical smart maintained  identity theory explains  phenomena  assuming   physical reality subsequently smart   severely criticized   use  misuse  occams razor  ultimately retracted  advocacy     context paul churchland states    occams razor  inconclusive regarding duality   similar way dale jacquette stated  occams razor   used  attempts  justify eliminativism  reductionism   philosophy  mind eliminativism   thesis   ontology  folk psychology including  entities  pain joy desire fear etc  eliminable  favor   ontology   completed neuroscience penal ethics  penal theory   philosophy  punishment parsimony refers specifically  taking care   distribution  punishment  order  avoid excessive punishment   utilitarian approach   philosophy  punishment jeremy benthams parsimony principle states   punishment greater   required  achieve  end  unjust  concept  related   identical   legal concept  proportionality parsimony   key consideration   modern restorative justice    component  utilitarian approaches  punishment  well   prison abolition movement bentham believed  true parsimony  require punishment   individualised  take account   sensibility   individualan individual  sensitive  punishment   given  proportionately lesser one since otherwise needless pain   inflicted later utilitarian writers  tended  abandon  idea  large part due   impracticality  determining  alleged criminals relative sensitivity  specific punishments probability theory  statistics marcus hutters universal artificial intelligence builds upon solomonoffs mathematical formalization   razor  calculate  expected value   action   various papers  scholarly journals deriving formal versions  occams razor  probability theory applying   statistical inference  using   come   criteria  penalizing complexity  statistical inference papers  suggested  connection  occams razor  kolmogorov complexity one   problems   original formulation   razor     applies  models    explanatory power ie   tells us  prefer  simplest  equally good models   general form   razor can  derived  bayesian model comparison   based  bayes factors  can  used  compare models  dont fit  observations equally well  methods can sometimes optimally balance  complexity  power   model generally  exact occam factor  intractable  approximations   akaike information criterion bayesian information criterion variational bayesian methods false discovery rate  laplaces method  used many artificial intelligence researchers  now employing  techniques  instance  work  occam learning   generally    energy principle statistical versions  occams razor    rigorous formulation   philosophical discussions produce  particular  must   specific definition   term simplicity   definition can vary  example   kolmogorovchaitin minimum description length approach  subject must pick  turing machine whose operations describe  basic operations believed  represent simplicity   subject however one  always choose  turing machine   simple operation  happened  construct ones entire theory   hence score highly   razor   led  two opposing camps one  believes occams razor  objective  one  believes   subjective objective razor  minimum instruction set   universal turing machine requires approximately   length description across different formulations   small compared   kolmogorov complexity   practical theories marcus hutter  used  consistency  define  natural turing machine  small size   proper basis  excluding arbitrarily complex instruction sets   formulation  razors describing  program   universal program   hypothesis   representation   evidence  program data    formally proven  zermelofraenkel set theory   sum   log universal probability   model plus  log   probability   data given  model   minimized interpreting   minimising  total length   twopart message encoding model followed  data given model gives us  minimum message length mml principle one possible conclusion  mixing  concepts  kolmogorov complexity  occams razor    ideal data compressor  also   scientific explanationformulation generator  attempts   made  rederive known laws  considerations  simplicity  compressibility according  j rgen schmidhuber  appropriate mathematical theory  occams razor already exists namely solomonoffs theory  optimal inductive inference   extensions see discussions  david l dowes foreword re c s wallace   subtle distinctions   algorithmic probability work  solomonoff   mml work  chris wallace  see dowes mml hybrid bayesian network graphical models statistical consistency invariance  uniqueness    discussions    section discussions  mml  occams razor   specific example  mml  occams razor   problem  decision tree induction see dowe  needhams message length   effective ockhams razor  decision tree induction controversial aspects   razor occams razor    embargo   positing   kind  entity   recommendation   simplest theory come  may occams razor  used  adjudicate  theories   already passed theoretical scrutiny tests   equally wellsupported  evidence furthermore  may  used  prioritize empirical testing  two equally plausible  unequally testable hypotheses thereby minimizing costs  wastes  increasing chances  falsification   simplertotest hypothesis another contentious aspect   razor    theory can become  complex  terms   structure  syntax   ontology  semantics becomes simpler  vice versa quine   discussion  definition referred   two perspectives  economy  practical expression  economy  grammar  vocabulary respectively  theory  relativity  often given   example   proliferation  complex words  describe  simple concept galileo galilei lampooned  misuse  occams razor   dialogue  principle  represented   dialogue  simplicio  telling point  galileo presented ironically    one really wanted  start   small number  entities one  always consider  letters   alphabet   fundamental entities since one  construct  whole  human knowledge    antirazors occams razor  met  opposition  people   considered   extreme  rash walter chatton c   contemporary  william  ockham c  took exception  occams razor  ockhams use    response  devised   antirazor  three things   enough  verify  affirmative proposition  things  fourth must  added    although     number  philosophers   formulated similar antirazors since chattons time  one antirazor  perpetuated   much notability  chattons antirazor although     case   late renaissance italian motto  unknown attribution se non   vero   ben trovato even     true   well conceived  referred   particularly artful explanation   information see ockhams razor  chattons antirazor  armand maurer antirazors  also  created  gottfried wilhelm leibniz immanuel kant  karl menger leibnizs version took  form   principle  plenitude  arthur lovejoy  called   idea   god created   varied  populous  possible worlds kant felt  need  moderate  effects  occams razor  thus created   counterrazor  variety  beings   rashly  diminished karl menger found mathematicians    parsimonious  regard  variables   formulated  law  miserliness  took one  two forms entities must   reduced   point  inadequacy    vain    fewer  requires   less serious   might say even  extremist antirazor  pataphysics  science  imaginary solutions developed  alfred jarry perhaps  ultimate  antireductionism pataphysics seeks  less   view  event   universe  completely unique subject   laws    variations   theme  subsequently explored   argentine writer jorge luis borges   storymockessay tl n uqbar orbis tertius   also crabtrees bludgeon  cynically states   set  mutually inconsistent observations can exist    human intellect  conceive  coherent explanation however complicated\r\n"}
{"index":{"_id":95}}
{"conceptLabelTag":"sarsa","conceptLabel":"sarsa","conceptDescription":"stateactionrewardstateaction stateactionrewardstateaction sarsa   algorithm  learning  markov decision process policy used   reinforcement learning area  machine learning   introduced   technical note   alternative name sarsa   mentioned   footnote  name simply reflects  fact   main function  updating  qvalue depends   current state   agent s  action  agent chooses   reward r  agent gets  choosing  action  state s   agent will now    taking  action  finally  next action   agent will choose   new state taking every letter   quintuple s  r s  yields  word sarsa algorithm  sarsa agent will interact   environment  update  policy based  actions taken known   onpolicy learning algorithm  expressed   q value   stateaction  updated   error adjusted   learning rate alpha q values represent  possible reward received   next time step  taking action   state s plus  discounted future reward received   next stateaction observation watkins qlearning  created   alternative   existing temporal difference technique   updates  policy based   maximum reward  available actions  difference may  explained  sarsa learns  q values associated  taking  policy  follows   watkins qlearning learns  q values associated  taking  exploitation policy  following  explorationexploitation policy   information   explorationexploitation trade  see reinforcement learning  optimizations  watkins qlearning may also  applied  sarsa  example   paper fast online q wiering  schmidhuber  small differences needed  sarsa implementations  described   arise influence  variables   algorithm learning rate alpha  learning rate determines   extent  newly acquired information will override  old information  factor  will make  agent  learn anything   factor   make  agent consider    recent information discount factor gamma  discount factor determines  importance  future rewards  factor  will make  agent opportunistic   considering current rewards   factor approaching will make  strive   longterm high reward   discount factor meets  exceeds  formula values may diverge initial conditions since sarsa   iterative algorithm  implicitly assumes  initial condition   first update occurs  low infinite initial value also known  optimistic initial conditions can encourage exploration  matter  action will take place  update rule will cause    higher values    alternative thus increasing  choice probability recently   suggested   first reward   used  reset  initial conditions according   idea  first time  action  taken  reward  used  set  value   will allow immediate learning  case  fix deterministic rewards surprisingly  resettingofinitialconditions ric approach seems   consistent  human behavior  repeated binary choice experiments\r\n"}
{"index":{"_id":96}}
{"conceptLabelTag":"radial basis function","conceptLabel":"radial basis function","conceptDescription":"radial basis function  radial basis function rbf   realvalued function whose value depends    distance   origin   formula  alternatively   distance    point c called  center   formula  function formula  satisfies  property formula   radial function  norm  usually euclidean distance although  distance functions  also possible sums  radial basis functions  typically used  approximate given functions  approximation process can also  interpreted   simple kind  neural network    context    originally surfaced  work  david broomhead  david lowe   stemmed  michael j d powells seminal research  rbfs  also used   kernel  support vector classification types commonly used types  radial basis functions include writing formula approximation radial basis functions  typically used  build  function approximations   form   approximating function yx  represented   sum  n radial basis functions  associated   different center x  weighted   appropriate coefficient w  weights w can  estimated using  matrix methods  linear least squares   approximating function  linear   weights approximation schemes   kind   particularly used  time series prediction  control  nonlinear systems exhibiting sufficiently simple chaotic behaviour d reconstruction  computer graphics  example hierarchical rbf  pose space deformation rbf network  sum can also  interpreted   rather simple singlelayer type  artificial neural network called  radial basis function network   radial basis functions taking   role   activation functions   network  can  shown   continuous function   compact interval can  principle  interpolated  arbitrary accuracy   sum   form   sufficiently large number n  radial basis functions  used  approximant yx  differentiable  respect   weights w  weights  thus  learned using    standard iterative methods  neural networks using radial basis functions   manner yields  reasonable interpolation approach provided   fitting set   chosen    covers  entire range systematically equidistant data points  ideal however without  polynomial term   orthogonal   radial basis functions estimates outside  fitting set tend  perform poorly\r\n"}
{"index":{"_id":97}}
{"conceptLabelTag":"exploratory data analysis","conceptLabel":"exploratory data analysis","conceptDescription":"exploratory data analysis  statistics exploratory data analysis eda   approach  analyzing data sets  summarize  main characteristics often  visual methods  statistical model can  used    primarily eda   seeing   data can tell us beyond  formal modeling  hypothesis testing task exploratory data analysis  promoted  john tukey  encourage statisticians  explore  data  possibly formulate hypotheses   lead  new data collection  experiments eda  different  initial data analysis ida  focuses  narrowly  checking assumptions required  model fitting  hypothesis testing  handling missing values  making transformations  variables  needed eda encompasses ida overview tukey defined data analysis   procedures  analyzing data techniques  interpreting  results   procedures ways  planning  gathering  data  make  analysis easier  precise   accurate    machinery  results  mathematical statistics  apply  analyzing data tukeys championing  eda encouraged  development  statistical computing packages especially s  bell labs  s programming language inspired  systems splus  r  family  statisticalcomputing environments featured vastly improved dynamic visualization capabilities  allowed statisticians  identify outliers trends  patterns  data  merited  study tukeys eda  related  two  developments  statistical theory robust statistics  nonparametric statistics    tried  reduce  sensitivity  statistical inferences  errors  formulating statistical models tukey promoted  use  five number summary  numerical datathe two extremes maximum  minimum  median   quartilesbecause  median  quartiles  functions   empirical distribution  defined   distributions unlike  mean  standard deviation moreover  quartiles  median   robust  skewed  heavytailed distributions  traditional summaries  mean  standard deviation  packages s splus  r included routines using resampling statistics   quenouille  tukeys jackknife  efron bootstrap   nonparametric  robust  many problems exploratory data analysis robust statistics nonparametric statistics   development  statistical programming languages facilitated statisticians work  scientific  engineering problems  problems included  fabrication  semiconductors   understanding  communications networks  concerned bell labs  statistical developments  championed  tukey  designed  complement  analytic theory  testing statistical hypotheses particularly  laplacian traditions emphasis  exponential families development john w tukey wrote  book exploratory data analysis  tukey held   much emphasis  statistics  placed  statistical hypothesis testing confirmatory data analysis  emphasis needed   placed  using data  suggest hypotheses  test  particular  held  confusing  two types  analyses  employing     set  data can lead  systematic bias owing   issues inherent  testing hypotheses suggested   data  objectives  eda   many eda techniques   adopted  data mining  well   big data analytics   also  taught  young students   way  introduce   statistical thinking techniques    number  tools   useful  eda  eda  characterized    attitude taken   particular techniques typical graphical techniques used  eda  typical quantitative techniques  history many eda ideas can  traced back  earlier authors  example  open university course statistics  society mdst took   ideas  merged   gottfried noethers work  introduced statistical inference via cointossing   median test example findings  eda  often orthogonal   primary analysis task  illustrate consider  example  cook et al   analysis task   find  variables  best predict  tip   dining party will give   waiter  variables available   data collected   task   tip amount total bill payer gender smokingnonsmoking section time  day day   week  size   party  primary analysis task  approached  fitting  regression model   tip rate   response variable  fitted model   says    size   dining party increases  one person leading   higher bill  tip rate will decrease  however exploring  data reveals  interesting features  described   model   learned   plots  different    illustrated   regression model even though  experiment   designed  investigate     trends  patterns found  exploring  data suggest hypotheses  tipping  may    anticipated  advance    lead  interesting followup experiments   hypotheses  formally stated  tested  collecting new data\r\n"}
{"index":{"_id":98}}
{"conceptLabelTag":"bioinformatics","conceptLabel":"bioinformatics","conceptDescription":"bioinformatics bioinformatics   interdisciplinary field  develops methods  software tools  understanding biological data   interdisciplinary field  science bioinformatics combines computer science statistics mathematics  engineering  analyze  interpret biological data bioinformatics   used   silico analyses  biological queries using mathematical  statistical techniques bioinformatics    umbrella term   body  biological studies  use computer programming  part   methodology  well   reference  specific analysis pipelines   repeatedly used particularly   field  genomics common uses  bioinformatics include  identification  candidate genes  nucleotides snps often  identification  made   aim  better understanding  genetic basis  disease unique adaptations desirable properties esp  agricultural species  differences  populations   less formal way bioinformatics also tries  understand  organisational principles within nucleic acid  protein sequences called proteomics introduction bioinformatics  become  important part  many areas  biology  experimental molecular biology bioinformatics techniques   image  signal processing allow extraction  useful results  large amounts  raw data   field  genetics  genomics  aids  sequencing  annotating genomes   observed mutations  plays  role   text mining  biological literature   development  biological  gene ontologies  organize  query biological data  also plays  role   analysis  gene  protein expression  regulation bioinformatics tools aid   comparison  genetic  genomic data   generally   understanding  evolutionary aspects  molecular biology    integrative level  helps analyze  catalogue  biological pathways  networks    important part  systems biology  structural biology  aids   simulation  modeling  dna rna proteins  well  biomolecular interactions history historically  term bioinformatics   mean   means today paulien hogeweg  ben hesper coined    refer   study  information processes  biotic systems  definition placed bioinformatics   field parallel  biophysics  study  physical processes  biological systems  biochemistry  study  chemical processes  biological systems sequences computers became essential  molecular biology  protein sequences became available  frederick sanger determined  sequence  insulin   early s comparing multiple sequences manually turned    impractical  pioneer   field  margaret oakley dayhoff    hailed  david lipman director   national center  biotechnology information   mother  father  bioinformatics dayhoff compiled one   first protein sequence databases initially published  books  pioneered methods  sequence alignment  molecular evolution another early contributor  bioinformatics  elvin  kabat  pioneered biological sequence analysis    comprehensive volumes  antibody sequences released  tai te wu   goals  study  normal cellular activities  altered  different disease states  biological data must  combined  form  comprehensive picture   activities therefore  field  bioinformatics  evolved     pressing task now involves  analysis  interpretation  various types  data  includes nucleotide  amino acid sequences protein domains  protein structures  actual process  analyzing  interpreting data  referred   computational biology important subdisciplines within bioinformatics  computational biology include  primary goal  bioinformatics   increase  understanding  biological processes  sets  apart   approaches however   focus  developing  applying computationally intensive techniques  achieve  goal examples include pattern recognition data mining machine learning algorithms  visualization major research efforts   field include sequence alignment gene finding genome assembly drug design drug discovery protein structure alignment protein structure prediction prediction  gene expression  proteinprotein interactions genomewide association studies  modeling  evolution  cell divisionmitosis bioinformatics now entails  creation  advancement  databases algorithms computational  statistical techniques  theory  solve formal  practical problems arising   management  analysis  biological data   past  decades rapid developments  genomic   molecular research technologies  developments  information technologies  combined  produce  tremendous amount  information related  molecular biology bioinformatics   name given   mathematical  computing approaches used  glean understanding  biological processes common activities  bioinformatics include mapping  analyzing dna  protein sequences aligning dna  protein sequences  compare   creating  viewing d models  protein structures relation   fields bioinformatics   science field   similar   distinct  biological computation    often considered synonymous  computational biology biological computation uses bioengineering  biology  build biological computers whereas bioinformatics uses computation  better understand biology bioinformatics  computational biology involve  analysis  biological data particularly dna rna  protein sequences  field  bioinformatics experienced explosive growth starting   mids driven largely   human genome project   rapid advances  dna sequencing technology analyzing biological data  produce meaningful information involves writing  running software programs  use algorithms  graph theory artificial intelligence soft computing data mining image processing  computer simulation  algorithms  turn depend  theoretical foundations   discrete mathematics control theory system theory information theory  statistics sequence analysis since  phage x  sequenced   dna sequences  thousands  organisms   decoded  stored  databases  sequence information  analyzed  determine genes  encode proteins rna genes regulatory sequences structural motifs  repetitive sequences  comparison  genes within  species   different species can show similarities  protein functions  relations  species  use  molecular systematics  construct phylogenetic trees   growing amount  data  long ago became impractical  analyze dna sequences manually today computer programs   blast  used daily  search sequences    organisms containing  billion nucleotides  programs can compensate  mutations exchanged deleted  inserted bases   dna sequence  identify sequences   related   identical  variant   sequence alignment  used   sequencing process  dna sequencing  sequences can  analyzed     obtained dna sequencing  still  nontrivial problem   raw data may  noisy  afflicted  weak signals algorithms   developed  base calling   various experimental approaches  dna sequencing sequence assembly  dna sequencing techniques produce short fragments  sequence  need   assembled  obtain complete gene  genome sequences  socalled shotgun sequencing technique   used  example   institute  genomic research tigr  sequence  first bacterial genome haemophilus influenzae generates  sequences  many thousands  small dna fragments ranging   nucleotides long depending   sequencing technology  ends   fragments overlap   aligned properly   genome assembly program can  used  reconstruct  complete genome shotgun sequencing yields sequence data quickly   task  assembling  fragments can  quite complicated  larger genomes   genome  large   human genome  may take many days  cpu time  largememory multiprocessor computers  assemble  fragments   resulting assembly usually contains numerous gaps  must  filled  later shotgun sequencing   method  choice  virtually  genomes sequenced today  genome assembly algorithms   critical area  bioinformatics research genome annotation   context  genomics annotation   process  marking  genes   biological features   dna sequence  process needs   automated   genomes   large  annotate  hand   mention  desire  annotate  many genomes  possible   rate  sequencing  ceased  pose  bottleneck annotation  made possible   fact  genes  recognisable start  stop regions although  exact sequence found   regions can vary  genes  first genome annotation software system  designed   owen white   part   team   institute  genomic research  sequenced  analyzed  first genome   living organism   decoded  bacterium haemophilus influenzae white built  software system  find  genes fragments  genomic sequence  encode proteins  transfer rnas   make initial assignments  function   genes  current genome annotation systems work similarly   programs available  analysis  genomic dna    genemark program trained  used  find proteincoding genes  haemophilus influenzae  constantly changing  improving following  goals   human genome project left  achieve   closure   new project developed   national human genome research institute   us appeared  socalled encode project   collaborative data collection   functional elements   human genome  uses nextgeneration dnasequencing technologies  genomic tiling arrays technologies able  generate automatically large amounts  data  lower research costs     quality  viability computational evolutionary biology evolutionary biology   study   origin  descent  species  well   change  time informatics  assisted evolutionary biologists  enabling researchers  future work endeavours  reconstruct  now  complex tree  life  area  research within computer science  uses genetic algorithms  sometimes confused  computational evolutionary biology   two areas   necessarily related comparative genomics  core  comparative genome analysis   establishment   correspondence  genes orthology analysis   genomic features  different organisms    intergenomic maps  make  possible  trace  evolutionary processes responsible   divergence  two genomes  multitude  evolutionary events acting  various organizational levels shape genome evolution   lowest level point mutations affect individual nucleotides   higher level large chromosomal segments undergo duplication lateral transfer inversion transposition deletion  insertion ultimately whole genomes  involved  processes  hybridization polyploidization  endosymbiosis often leading  rapid speciation  complexity  genome evolution poses many exciting challenges  developers  mathematical models  algorithms   recourse   spectra  algorithmic statistical  mathematical techniques ranging  exact heuristics fixed parameter  approximation algorithms  problems based  parsimony models  markov chain monte carlo algorithms  bayesian analysis  problems based  probabilistic models many   studies  based   homology detection  protein families computation pan genomics pan genomics   concept introduced   tettelin  medini  eventually took root  bioinformatics pan genome   complete gene repertoire   particular taxonomic group although initially applied  closely related strains   species  can  applied   larger context like genus phylum etc   divided  two parts  core genome set  genes common    genomes  study   often housekeeping genes vital  survival   dispensableflexible genome set  genes  present    one   genomes  study  bioinformatics tool bpga can  used  characterize  pan genome  bacterial species genetics  disease   advent  nextgeneration sequencing   obtaining enough sequence data  map  genes  complex diseases   diabetes infertility breast cancer  alzheimers disease genomewide association studies   useful approach  pinpoint  mutations responsible   complex diseases   studies thousands  dna variants   identified   associated  similar diseases  traits furthermore  possibility  genes   used  prognosis diagnosis  treatment  one    essential applications many studies  discussing   promising ways  choose  genes   used   problems  pitfalls  using genes  predict disease presence  prognosis analysis  mutations  cancer  cancer  genomes  affected cells  rearranged  complex  even unpredictable ways massive sequencing efforts  used  identify previously unknown point mutations   variety  genes  cancer bioinformaticians continue  produce specialized automated systems  manage  sheer volume  sequence data produced   create new algorithms  software  compare  sequencing results   growing collection  human genome sequences  germline polymorphisms new physical detection technologies  employed   oligonucleotide microarrays  identify chromosomal gains  losses called comparative genomic hybridization  singlenucleotide polymorphism arrays  detect known point mutations  detection methods simultaneously measure several hundred thousand sites throughout  genome   used  highthroughput  measure thousands  samples generate terabytes  data per experiment   massive amounts  new types  data generate new opportunities  bioinformaticians  data  often found  contain considerable variability  noise  thus hidden markov model  changepoint analysis methods   developed  infer real copy number changes   breakthroughs   nextgeneration sequencing technology  providing   field  bioinformatics cancer genomics  drastically change  new methods  software allow bioinformaticians  sequence many cancer genomes quickly  affordably   create   flexible process  classifying types  cancer  analysis  cancer driven mutations   genome furthermore tracking  patients   disease progresses may  possible   future   sequence  cancer samples another type  data  requires novel informatics development   analysis  lesions found   recurrent among many tumors gene  protein expression analysis  gene expression  expression  many genes can  determined  measuring mrna levels  multiple techniques including microarrays expressed cdna sequence tag est sequencing serial analysis  gene expression sage tag sequencing massively parallel signature sequencing mpss rnaseq also known  whole transcriptome shotgun sequencing wtss  various applications  multiplexed insitu hybridization    techniques  extremely noiseprone andor subject  bias   biological measurement   major research area  computational biology involves developing statistical tools  separate signal  noise  highthroughput gene expression studies  studies  often used  determine  genes implicated   disorder one might compare microarray data  cancerous epithelial cells  data  noncancerous cells  determine  transcripts   upregulated  downregulated   particular population  cancer cells analysis  protein expression protein microarrays  high throughput ht mass spectrometry ms can provide  snapshot   proteins present   biological sample bioinformatics   much involved  making sense  protein microarray  ht ms data  former approach faces similar problems   microarrays targeted  mrna  latter involves  problem  matching large amounts  mass data  predicted masses  protein sequence databases   complicated statistical analysis  samples  multiple  incomplete peptides   protein  detected analysis  regulation regulation   complex orchestration  events    signal potentially  extracellular signal    hormone eventually leads   increase  decrease   activity  one   proteins bioinformatics techniques   applied  explore various steps   process  example gene expression can  regulated  nearby elements   genome promoter analysis involves  identification  study  sequence motifs   dna surrounding  coding region   gene  motifs influence  extent    region  transcribed  mrna enhancer elements far away   promoter can also regulate gene expression  threedimensional looping interactions  interactions can  determined  bioinformatic analysis  chromosome conformation capture experiments expression data can  used  infer gene regulation one might compare microarray data   wide variety  states   organism  form hypotheses   genes involved   state   singlecell organism one might compare stages   cell cycle along  various stress conditions heat shock starvation etc one can  apply clustering algorithms   expression data  determine  genes  coexpressed  example  upstream regions promoters  coexpressed genes can  searched  overrepresented regulatory elements examples  clustering algorithms applied  gene clustering  kmeans clustering selforganizing maps soms hierarchical clustering  consensus clustering methods analysis  cellular organization several approaches   developed  analyze  location  organelles genes proteins   components within cells   relevant   location   components affects  events within  cell  thus helps us  predict  behavior  biological systems  gene ontology category cellular compartment   devised  capture subcellular localization  many biological databases microscopy  image analysis microscopic pictures allow us  locate  organelles  well  molecules  may also help us  distinguish  normal  abnormal cells eg  cancer protein localization  localization  proteins helps us  evaluate  role   protein  instance   protein  found   nucleus  may  involved  gene regulation  splicing  contrast   protein  found  mitochondria  may  involved  respiration   metabolic processes protein localization  thus  important component  protein function prediction nuclear organisation  chromatin data  highthroughput chromosome conformation capture experiments   hic experiment  chiapet can provide information   spatial proximity  dna loci analysis   experiments can determine  threedimensional structure  nuclear organization  chromatin bioinformatic challenges   field include partitioning  genome  domains   topologically associating domains tads   organised together  threedimensional space structural bioinformatics protein structure prediction  another important application  bioinformatics  amino acid sequence   protein  socalled primary structure can  easily determined   sequence   gene  codes     vast majority  cases  primary structure uniquely determines  structure   native environment  course   exceptions    bovine spongiform encephalopathy aka mad cow disease prion knowledge   structure  vital  understanding  function   protein structural information  usually classified  one  secondary tertiary  quaternary structure  viable general solution   predictions remains  open problem  efforts   far  directed towards heuristics  work    time one   key ideas  bioinformatics   notion  homology   genomic branch  bioinformatics homology  used  predict  function   gene   sequence  gene  whose function  known  homologous   sequence  gene b whose function  unknown one  infer  b may share  function   structural branch  bioinformatics homology  used  determine  parts   protein  important  structure formation  interaction   proteins   technique called homology modeling  information  used  predict  structure   protein   structure   homologous protein  known  currently remains   way  predict protein structures reliably one example     similar protein homology  hemoglobin  humans   hemoglobin  legumes leghemoglobin  serve   purpose  transporting oxygen   organism though    proteins  completely different amino acid sequences  protein structures  virtually identical  reflects  near identical purposes  techniques  predicting protein structure include protein threading  de novo  scratch physicsbased modeling network  systems biology network analysis seeks  understand  relationships within biological networks   metabolic  proteinprotein interaction networks although biological networks can  constructed   single type  molecule  entity   genes network biology often attempts  integrate many different data types   proteins small molecules gene expression data  others    connected physically functionally   systems biology involves  use  computer simulations  cellular subsystems    networks  metabolites  enzymes  comprise metabolism signal transduction pathways  gene regulatory networks   analyze  visualize  complex connections   cellular processes artificial life  virtual evolution attempts  understand evolutionary processes via  computer simulation  simple artificial life forms molecular interaction networks tens  thousands  threedimensional protein structures   determined  xray crystallography  protein nuclear magnetic resonance spectroscopy protein nmr   central question  structural bioinformatics  whether   practical  predict possible proteinprotein interactions  based   d shapes without performing proteinprotein interaction experiments  variety  methods   developed  tackle  proteinprotein docking problem though  seems    still much work   done   field  interactions encountered   field include proteinligand including drug  proteinpeptide molecular dynamic simulation  movement  atoms  rotatable bonds   fundamental principle behind computational algorithms termed docking algorithms  studying molecular interactions others literature analysis  growth   number  published literature makes  virtually impossible  read every paper resulting  disjointed subfields  research literature analysis aims  employ computational  statistical linguistics  mine  growing library  text resources  example  area  research draws  statistics  computational linguistics highthroughput image analysis computational technologies  used  accelerate  fully automate  processing quantification  analysis  large amounts  highinformationcontent biomedical imagery modern image analysis systems augment  observers ability  make measurements   large  complex set  images  improving accuracy objectivity  speed  fully developed analysis system may completely replace  observer although  systems   unique  biomedical imagery biomedical imaging  becoming  important   diagnostics  research  examples  highthroughput single cell data analysis computational techniques  used  analyse highthroughput lowmeasurement single cell data    obtained  flow cytometry  methods typically involve finding populations  cells   relevant   particular disease state  experimental condition biodiversity informatics biodiversity informatics deals   collection  analysis  biodiversity data   taxonomic databases  microbiome data examples   analyses include phylogenetics niche modelling species richness mapping  species identification tools databases databases  essential  bioinformatics research  applications many databases exist covering various information types  example dna  protein sequences molecular structures phenotypes  biodiversity databases may contain empirical data obtained directly  experiments predicted data obtained  analysis   commonly   may  specific   particular organism pathway  molecule  interest alternatively  can incorporate data compiled  multiple  databases  databases vary   format way  accession  whether   public       commonly used databases  listed     comprehensive list please check  link   beginning   subsection software  tools software tools  bioinformatics range  simple commandline tools   complex graphical programs  standalone webservices available  various bioinformatics companies  public institutions opensource bioinformatics software many   opensource software tools  existed  continued  grow since  s  combination   continued need  new algorithms   analysis  emerging types  biological readouts  potential  innovative  silico experiments  ly available open code bases  helped  create opportunities   research groups  contribute   bioinformatics   range  opensource software available regardless   funding arrangements  open source tools often act  incubators  ideas  communitysupported plugins  commercial applications  may also provide de facto standards  shared object models  assisting   challenge  bioinformation integration  range  opensource software packages includes titles   bioconductor bioperl biopython biojava biojs bioruby bioclipse emboss net bio orange   bioinformatics addon apache taverna ugene  genocad  maintain  tradition  create  opportunities  nonprofit open bioinformatics foundation  supported  annual bioinformatics open source conference bosc since  alternative method  build public bioinformatics databases   use  mediawiki engine   wikiopener extension  system allows  database   accessed  updated   experts   field web services  bioinformatics soap  restbased interfaces   developed   wide variety  bioinformatics applications allowing  application running  one computer  one part   world  use algorithms data  computing resources  servers   parts   world  main advantages derive   fact  end users     deal  software  database maintenance overheads basic bioinformatics services  classified   ebi  three categories sss sequence search services msa multiple sequence alignment  bsa biological sequence analysis  availability   serviceoriented bioinformatics resources demonstrate  applicability  webbased bioinformatics solutions  range   collection  standalone tools   common data format   single standalone  webbased interface  integrative distributed  extensible bioinformatics workflow management systems bioinformatics workflow management systems  bioinformatics workflow management system   specialized form   workflow management system designed specifically  compose  execute  series  computational  data manipulation steps   workflow   bioinformatics application  systems  designed     platforms giving  service galaxy kepler taverna ugene anduril education platforms software platforms designed  teach bioinformatics concepts  methods include rosalind  online courses offered   swiss institute  bioinformatics training portal  canadian bioinformatics workshops provides videos  slides  training workshops   website   creative commons license  project  pi project also offers open source educational materials    course runs  low cost raspberry pi computers    used  teach adults  school pupils  actively developed   consortium  academics  research staff   run research level bioinformatics using raspberry pi computers   operating system mooc platforms also provide online certifications  bioinformatics  related disciplines including courseras bioinformatics specialization uc san diego  genomic data science specialization johns hopkins  well  edxs data analysis  life sciences xseries harvard conferences   several large conferences   concerned  bioinformatics     notable examples  intelligent systems  molecular biology ismb european conference  computational biology eccb  research  computational molecular biology recomb\r\n"}
{"index":{"_id":99}}
{"conceptLabelTag":"learning vector quantization","conceptLabel":"learning vector quantization","conceptDescription":"learning vector quantization  computer science learning vector quantization lvq   prototypebased supervised classification algorithm lvq   supervised counterpart  vector quantization systems overview lvq can  understood   special case   artificial neural network  precisely  applies  winnertakeall hebbian learningbased approach    precursor  selforganizing maps som  related  neural gas    knearest neighbor algorithm knn lvq  invented  teuvo kohonen  lvq system  represented  prototypes formula   defined   feature space  observed data  winnertakeall training algorithms one determines   data point  prototype   closest   input according   given distance measure  position   socalled winner prototype   adapted ie  winner  moved closer   correctly classifies  data point  moved away   classifies  data point incorrectly  advantage  lvq    creates prototypes   easy  interpret  experts   respective application domain lvq systems can  applied  multiclass classification problems   natural way   used   variety  practical applications see httpliinwwwiraukadebibliographyneuralsomlvqhtml   extensive bibliography  key issue  lvq   choice   appropriate measure  distance  similarity  training  classification recently techniques   developed  adapt  parameterized distance measure   course  training  system see eg schneider biehl  hammer  references therein lvq can   source  great help  classifying text documents algorithm  follows  informal description  algorithm consists  basic steps  algorithms input   algorithms flow  note formula  formula  vectors  feature space   formal description can  found  httpjsalatasictprogrimplementationofcompetitivelearningnetworksforweka\r\n"}
{"index":{"_id":100}}
{"conceptLabelTag":"hierarchical clustering","conceptLabel":"hierarchical clustering","conceptDescription":"hierarchical clustering  data mining  statistics hierarchical clustering also called hierarchical cluster analysis  hca   method  cluster analysis  seeks  build  hierarchy  clusters strategies  hierarchical clustering generally fall  two types  general  merges  splits  determined   greedy manner  results  hierarchical clustering  usually presented   dendrogram   general case  complexity  agglomerative clustering  formula  makes   slow  large data sets divisive clustering   exhaustive search  formula   even worse however   special cases optimal efficient agglomerative methods  complexity formula  known slink  singlelinkage  clink  completelinkage clustering cluster dissimilarity  order  decide  clusters   combined  agglomerative    cluster   split  divisive  measure  dissimilarity  sets  observations  required   methods  hierarchical clustering   achieved  use   appropriate metric  measure  distance  pairs  observations   linkage criterion  specifies  dissimilarity  sets   function   pairwise distances  observations   sets metric  choice   appropriate metric will influence  shape   clusters   elements may  close  one another according  one distance  farther away according  another  example   dimensional space  distance   point   origin  always according   usual norms   distance   point   origin can   manhattan distance formula  euclidean distance   maximum distance  commonly used metrics  hierarchical clustering   text   nonnumeric data metrics    hamming distance  levenshtein distance  often used  review  cluster analysis  health psychology research found    common distance measure  published studies   research area   euclidean distance   squared euclidean distance linkage criteria  linkage criterion determines  distance  sets  observations   function   pairwise distances  observations  commonly used linkage criteria  two sets  observations   b   d   chosen metric  linkage criteria include discussion hierarchical clustering   distinct advantage   valid measure  distance can  used  fact  observations    required    used   matrix  distances agglomerative clustering example  example suppose  data    clustered   euclidean distance   distance metric cutting  tree   given height will give  partitioning clustering   selected precision   example cutting   second row   dendrogram will yield clusters  b c d e f cutting   third row will yield clusters  b c d e f    coarser clustering   smaller number  larger clusters  hierarchical clustering dendrogram      method builds  hierarchy   individual elements  progressively merging clusters   example   six elements  b c d e  f  first step   determine  elements  merge   cluster usually  want  take  two closest elements according   chosen distance optionally one can also construct  distance matrix   stage   number   ith row jth column   distance   ith  jth elements   clustering progresses rows  columns  merged   clusters  merged   distances updated    common way  implement  type  clustering    benefit  caching distances  clusters  simple agglomerative clustering algorithm  described   singlelinkage clustering page  can easily  adapted  different types  linkage see  suppose   merged  two closest elements b  c  now   following clusters  b c d e  f  want  merge       need  take  distance    b c  therefore define  distance  two clusters usually  distance  two clusters formula  formula  one   following  agglomeration occurs   greater distance  clusters   previous agglomeration  one can decide  stop clustering either   clusters   far apart   merged distance criterion      sufficiently small number  clusters number criterion divisive clustering  basic principle  divisive clustering  published   diana divisive analysis clustering algorithm initially  data     cluster   largest cluster  split  every object  separate   exist formula ways  splitting  cluster heuristics  needed diana chooses  object   maximum average dissimilarity   moves  objects   cluster    similar   new cluster    remainder  obvious alternate choice  kmeans clustering  formula    clustering algorithm can  used  always produces  least two clusters\r\n"}
{"index":{"_id":101}}
{"conceptLabelTag":"bayesian inference","conceptLabel":"bayesian inference","conceptDescription":"bayesian inference bayesian inference   method  statistical inference   bayes theorem  used  update  probability   hypothesis   evidence  information becomes available bayesian inference   important technique  statistics  especially  mathematical statistics bayesian updating  particularly important   dynamic analysis   sequence  data bayesian inference  found application   wide range  activities including science engineering philosophy medicine sport  law   philosophy  decision theory bayesian inference  closely related  subjective probability often called bayesian probability introduction  bayes rule formal bayesian inference derives  posterior probability   consequence  two antecedents  prior probability   likelihood function derived   statistical model   observed data bayesian inference computes  posterior probability according  bayes theorem   different values  formula   factors formula  formula    numerator affect  value  formula  posterior probability   hypothesis  proportional   prior probability  inherent likeliness   newly acquired likelihood  compatibility   new observed evidence bayes rule can also  written  follows   factor formula can  interpreted   impact  formula   probability  formula informal   evidence   match    hypothesis one  reject  hypothesis    hypothesis  extremely unlikely  priori one  also reject  even   evidence  appear  match   example  one   know whether  newborn baby next door   boy   girl  color  decorations   crib  front   door may support  hypothesis  one gender       front   door instead   crib  dog kennel  found  posterior probability   family next door gave birth   dog remains small  spite   evidence since ones prior belief    hypothesis  already extremely small  critical point  bayesian inference     provides  principled way  combining new evidence  prior beliefs   application  bayes rule contrast   frequentist inference  relies    evidence   whole   reference  prior beliefs furthermore bayes rule can  applied iteratively  observing  evidence  resulting posterior probability can   treated   prior probability   new posterior probability computed  new evidence  allows  bayesian principles   applied  various kinds  evidence whether viewed      time  procedure  termed bayesian updating alternatives  bayesian updating bayesian updating  widely used  computationally convenient however      updating rule  might  considered rational ian hacking noted  traditional dutch book arguments   specify bayesian updating  left open  possibility  nonbayesian updating rules  avoid dutch books hacking wrote  neither  dutch book argument      personalist arsenal  proofs   probability axioms entails  dynamic assumption  one entails bayesianism   personalist requires  dynamic assumption   bayesian   true   consistency  personalist  abandon  bayesian model  learning  experience salt  lose  savour indeed   nonbayesian updating rules  also avoid dutch books  discussed   literature  probability kinematics following  publication  richard c jeffreys rule  applies bayes rule   case   evidence   assigned  probability  additional hypotheses needed  uniquely require bayesian updating   deemed   substantial complicated  unsatisfactory note    expressed  words  posterior  proportional  likelihood times prior  sometimes  posterior likelihood times prior  evidence bayesian theory calls   use   posterior predictive distribution   predictive inference ie  predict  distribution   new unobserved data point   instead   fixed point   prediction  distribution  possible points  returned   way   entire posterior distribution   parameters used  comparison prediction  frequentist statistics often involves finding  optimum point estimate   parameterseg  maximum likelihood  maximum  posteriori estimation mapand  plugging  estimate   formula   distribution   data point    disadvantage     account   uncertainty   value   parameter  hence will underestimate  variance   predictive distribution formal description  bayesian inference bayesian prediction note   types  predictive distributions   form   compound probability distribution    marginal likelihood  fact   prior distribution   conjugate prior  hence  prior  posterior distributions come    family  can easily  seen   prior  posterior predictive distributions also come    family  compound distributions   difference    posterior predictive distribution uses  updated values   hyperparameters applying  bayesian update rules given   conjugate prior article   prior predictive distribution uses  values   hyperparameters  appear   prior distribution inference  exclusive  exhaustive possibilities  evidence  simultaneously used  update belief   set  exclusive  exhaustive propositions bayesian inference may  thought   acting   belief distribution   whole general formulation suppose  process  generating independent  identically distributed events formula   probability distribution  unknown let  event space formula represent  current state  belief   process  model  represented  event formula  conditional probabilities formula  specified  define  models formula   degree  belief  formula   first inference step formula   set  initial prior probabilities  must sum    otherwise arbitrary suppose   process  observed  generate formula   formula  prior formula  updated   posterior formula  bayes theorem upon observation   evidence  procedure may  repeated multiple observations   sequence  independent  identically distributed observations formula  can  shown  induction  repeated application     equivalent   parametric formulation  parameterizing  space  models  belief   models may  updated   single step  distribution  belief   model space may   thought    distribution  belief   parameter space  distributions   section  expressed  continuous represented  probability densities     usual situation  technique  however equally applicable  discrete distributions let  vector formula span  parameter space let  initial prior distribution  formula  formula  formula   set  parameters   prior   hyperparameters let formula   sequence  independent  identically distributed event observations   formula  distributed  formula   formula bayes theorem  applied  find  posterior distribution  formula  mathematical properties interpretation  factor formula     model  true  evidence    likely   predicted   current state  belief  reverse applies   decrease  belief   belief   change formula    evidence  independent   model   model  true  evidence   exactly  likely  predicted   current state  belief cromwells rule  formula  formula  formula  formula  can  interpreted  mean  hard convictions  insensitive  counterevidence  former follows directly  bayes theorem  latter can  derived  applying  first rule   event  formula  place  formula yielding  formula  formula    result immediately follows asymptotic behaviour  posterior consider  behaviour   belief distribution    updated  large number  times  independent  identically distributed trials  sufficiently nice prior probabilities  bernsteinvon mises theorem gives    limit  infinite trials  posterior converges   gaussian distribution independent   initial prior   conditions firstly outlined  rigorously proven  joseph l doob  namely   random variable  consideration   finite probability space   general results  obtained later   statistician david  dman  published  two seminal research papers       circumstances  asymptotic behaviour  posterior  guaranteed  paper treats like doob  finite case  comes   satisfactory conclusion however   random variable   infinite  countable probability space ie corresponding   die  infinite many faces  paper demonstrates    dense subset  priors  bernsteinvon mises theorem   applicable   case   almost surely  asymptotic convergence later   s  s dman  persi diaconis continued  work   case  infinite countable probability spaces  summarise  may  insufficient trials  suppress  effects   initial choice  especially  large  finite systems  convergence might   slow conjugate priors  parameterized form  prior distribution  often assumed  come   family  distributions called conjugate priors  usefulness   conjugate prior    corresponding posterior distribution will     family   calculation may  expressed  closed form estimates  parameters  predictions   often desired  use  posterior distribution  estimate  parameter  variable several methods  bayesian estimation select measurements  central tendency   posterior distribution  onedimensional problems  unique median exists  practical continuous problems  posterior median  attractive   robust estimator   exists  finite mean   posterior distribution   posterior mean   method  estimation taking  value   greatest probability defines maximum  posteriori map estimates   examples   maximum  attained   case  set  map estimates  empty    methods  estimation  minimize  posterior risk expectedposterior loss  respect   loss function     interest  statistical decision theory using  sampling distribution frequentist statistics  posterior predictive distribution   new observation formula   independent  previous observations  determined  examples probability   hypothesis suppose   two full bowls  cookies bowl  chocolate chip  plain cookies  bowl     friend fred picks  bowl  random   picks  cookie  random  may assume    reason  believe fred treats one bowl differently  another likewise   cookies  cookie turns     plain one  probable    fred picked    bowl intuitively  seems clear   answer      half since    plain cookies  bowl  precise answer  given  bayes theorem let formula correspond  bowl  formula  bowl   given   bowls  identical  freds point  view thus formula   two must add      equal   event formula   observation   plain cookie   contents   bowls  know  formula  formula bayes formula  yields   observed  cookie  probability  assigned  fred  chosen bowl   prior probability formula    observing  cookie  must revise  probability  formula   making  prediction  archaeologist  working   site thought     medieval period   th century   th century however   uncertain exactly    period  site  inhabited fragments  pottery  found     glazed      decorated   expected    site  inhabited   early medieval period    pottery   glazed    area decorated whereas     inhabited   late medieval period    glazed    area decorated  confident can  archaeologist    date  inhabitation  fragments  unearthed  degree  belief   continuous variable formula century    calculated   discrete set  events formula  evidence assuming linear variation  glaze  decoration  time    variables  independent assume  uniform prior  formula   trials  independent  identically distributed   new fragment  type formula  discovered bayes theorem  applied  update  degree  belief   formula formula  computer simulation   changing belief  fragments  unearthed  shown   graph   simulation  site  inhabited around  formula  calculating  area   relevant portion   graph  trials  archaeologist can say    practically  chance  site  inhabited   th  th centuries  chance    inhabited   th century chance   th century    th century note   bernsteinvon mises theorem asserts   asymptotic convergence   true distribution   probability space corresponding   discrete set  events formula  finite see  section  asymptotic behaviour   posterior  frequentist statistics  decision theory  decisiontheoretic justification   use  bayesian inference  given  abraham wald  proved  every unique bayesian procedure  admissible conversely every admissible statistical procedure  either  bayesian procedure   limit  bayesian procedures wald characterized admissible procedures  bayesian procedures  limits  bayesian procedures making  bayesian formalism  central technique   areas  frequentist inference  parameter estimation hypothesis testing  computing confidence intervals  example applications computer applications bayesian inference  applications  artificial intelligence  expert systems bayesian inference techniques    fundamental part  computerized pattern recognition techniques since  late s   also  ever growing connection  bayesian methods  simulationbased monte carlo techniques since complex models   processed  closed form   bayesian analysis   graphical model structure may allow  efficient simulation algorithms like  gibbs sampling   metropolishastings algorithm schemes recently bayesian inference  gained popularity amongst  phylogenetics community   reasons  number  applications allow many demographic  evolutionary parameters   estimated simultaneously  applied  statistical classification bayesian inference   used  recent years  develop algorithms  identifying email spam applications  make use  bayesian inference  spam filtering include crm dspam bogofilter spamassassin spambayes mozilla xeams  others spam classification  treated   detail   article   naive bayes classifier solomonoffs inductive inference   theory  prediction based  observations  example predicting  next symbol based upon  given series  symbols   assumption    environment follows  unknown  computable probability distribution    formal inductive framework  combines two wellstudied principles  inductive inference bayesian statistics  occams razor solomonoffs universal prior probability   prefix p   computable sequence x   sum   probabilities   programs   universal computer  compute something starting  p given  p   computable  unknown probability distribution   x  sampled  universal prior  bayes theorem can  used  predict  yet unseen parts  x  optimal fashion   courtroom bayesian inference can  used  jurors  coherently accumulate  evidence     defendant   see whether  totality  meets  personal threshold  beyond  reasonable doubt bayes theorem  applied successively   evidence presented   posterior  one stage becoming  prior   next  benefit   bayesian approach    gives  juror  unbiased rational mechanism  combining evidence  may  appropriate  explain bayes theorem  jurors  odds form  betting odds   widely understood  probabilities alternatively  logarithmic approach replacing multiplication  addition might  easier   jury  handle   existence   crime    doubt   identity   culprit    suggested   prior   uniform   qualifying population  example  people   committed  crime  prior probability  guilt    use  bayes theorem  jurors  controversial   united kingdom  defence expert witness explained bayes theorem   jury  r v adams  jury convicted   case went  appeal   basis   means  accumulating evidence   provided  jurors    wish  use bayes theorem  court  appeal upheld  conviction   also gave  opinion   introduce bayes theorem   similar method   criminal trial plunges  jury  inappropriate  unnecessary realms  theory  complexity deflecting    proper task gardnermedwin argues   criterion    verdict   criminal trial   based    probability  guilt  rather  probability   evidence given   defendant  innocent akin   frequentist pvalue  argues    posterior probability  guilt    computed  bayes theorem  prior probability  guilt must  known  will depend   incidence   crime    unusual piece  evidence  consider   criminal trial consider  following three propositions gardnermedwin argues   jury  believe    notb  order  convict   notb implies  truth  c   reverse   true   possible  b  c   true    case  argues   jury  acquit even though  know   will  letting  guilty people go  see also lindleys paradox bayesian epistemology bayesian epistemology   movement  advocates  bayesian inference   means  justifying  rules  inductive logic karl popper  david miller  rejected  alleged rationality  bayesianism ie using bayes rule  make epistemological inferences   prone    vicious circle    justificationist epistemology   presupposes   attempts  justify according   view  rational interpretation  bayesian inference  see  merely   probabilistic version  falsification rejecting  belief commonly held  bayesians  high likelihood achieved   series  bayesian updates  prove  hypothesis beyond  reasonable doubt  even  likelihood greater  bayes  bayesian inference  problem considered  bayes  proposition   essay  essay towards solving  problem   doctrine  chances   posterior distribution   parameter   success rate   binomial distribution history  term bayesian refers  thomas bayes  proved  special case    now called bayes theorem however   pierresimon laplace  introduced  general version   theorem  used   approach problems  celestial mechanics medical statistics reliability  jurisprudence early bayesian inference  used uniform priors following laplaces principle  insufficient reason  called inverse probability   infers backwards  observations  parameters   effects  causes   s inverse probability  largely supplanted   collection  methods  came   called frequentist statistics   th century  ideas  laplace   developed  two different directions giving rise  objective  subjective currents  bayesian practice   objective  noninformative current  statistical analysis depends    model assumed  data analyzed   method assigning  prior  differs  one objective bayesian  another objective bayesian   subjective  informative current  specification   prior depends   belief   propositions    analysis  prepared  act  can summarize information  experts previous studies etc   s    dramatic growth  research  applications  bayesian methods mostly attributed   discovery  markov chain monte carlo methods  removed many   computational problems   increasing interest  nonstandard complex applications despite growth  bayesian research  undergraduate teaching  still based  frequentist statistics nonetheless bayesian methods  widely accepted  used    example   field  machine learning  reading elementary  following books  listed  ascending order  probabilistic sophistication\r\n"}
{"index":{"_id":102}}
{"conceptLabelTag":"instance based learning","conceptLabel":"instance based learning","conceptDescription":"instancebased learning  machine learning instancebased learning sometimes called memorybased learning   family  learning algorithms  instead  performing explicit generalization compares new problem instances  instances seen  training    stored  memory   called instancebased   constructs hypotheses directly   training instances   means   hypothesis complexity can grow   data   worst case  hypothesis   list  n training items   computational complexity  classifying  single new instance   one advantage  instancebased learning    methods  machine learning   ability  adapt  model  previously unseen data instancebased learners may simply store  new instance  throw  old instance away examples  instancebased learning algorithm   knearest neighbor algorithm kernel machines  rbf networks  store  subset   training set  predicting  valueclass   new instance  compute distances  similarities   instance   training instances  make  decision  battle  memory complexity  storing  training instances  well   risk  overfitting  noise   training set instance reduction algorithms   proposed gagliardi applies  family  classifiers  medical field  secondopinion diagnostic tools   tools   knowledge extraction phase   process  knowledge discovery  databases one   classifiers called prototype exemplar learning classifier pelc  able  extract  mixture  abstracted prototypical cases   syndromes  selected atypical clinical cases\r\n"}
{"index":{"_id":103}}
{"conceptLabelTag":"confusion matrix","conceptLabel":"confusion matrix","conceptDescription":"confusion matrix   field  machine learning  specifically  problem  statistical classification  confusion matrix also known   error matrix   specific table layout  allows visualization   performance   algorithm typically  supervised learning one  unsupervised learning   usually called  matching matrix  column   matrix represents  instances   predicted class   row represents  instances   actual class  vice versa  name stems   fact   makes  easy  see   system  confusing two classes ie commonly mislabelling one  another    special kind  contingency table  two dimensions actual  predicted  identical sets  classes   dimensions  combination  dimension  class   variable   contingency table example   classification system   trained  distinguish  cats dogs  rabbits  confusion matrix will summarize  results  testing  algorithm   inspection assuming  sample  animals cats dogs  rabbits  resulting confusion matrix  look like  table  table  confusion  predictive analytics  table  confusion sometimes also called  confusion matrix   table  two rows  two columns  reports  number  false positives false negatives true positives  true negatives  allows  detailed analysis  mere proportion  correct guesses accuracy accuracy    reliable metric   real performance   classifier   will yield misleading results   data set  unbalanced     number  samples  different classes vary greatly  example    cats   dogs   data set  classifier  easily  biased  classifying   samples  cats  overall accuracy     practice  classifier    recognition rate   cat class   recognition rate   dog class assuming  confusion matrix   corresponding table  confusion   cat class    final table  confusion  contain  average values   classes combined let us define  experiment  p positive instances  n negative instances   condition  four outcomes can  formulated   confusion matrix  follows\r\n"}
{"index":{"_id":104}}
{"conceptLabelTag":"single linkage clustering","conceptLabel":"single linkage clustering","conceptDescription":"singlelinkage clustering  statistics singlelinkage clustering  one  several methods  hierarchical clustering   based  grouping clusters  bottomup fashion agglomerative clustering   step combining two clusters  contain  closest pair  elements  yet belonging    cluster     drawback   method    tends  produce long thin clusters   nearby elements    cluster  small distances  elements  opposite ends   cluster may  much farther      elements   clusters  may lead  difficulties  defining classes   usefully subdivide  data overview  agglomerative clustering methods   beginning   agglomerative clustering process  element    cluster     clusters   sequentially combined  larger clusters   elements end      cluster   step  two clusters separated   shortest distance  combined  definition  shortest distance   differentiates   different agglomerative clustering methods  singlelinkage clustering  distance  two clusters  determined   single element pair namely  two elements one   cluster   closest     shortest   links  remains   step causes  fusion   two clusters whose elements  involved  method  also known  nearest neighbour clustering  result   clustering can  visualized   dendrogram  shows  sequence  cluster fusion   distance    fusion took place mathematically  linkage function  distance dxy  clusters x  y  described   expression  x  y   two sets  elements considered  clusters  dxy denotes  distance   two elements x  y naive algorithm  following algorithm   agglomerative scheme  erases rows  columns   proximity matrix  old clusters  merged  new ones  formula proximity matrix d contains  distances dij  clusterings  assigned sequence numbers n  lk   level   kth clustering  cluster  sequence number m  denoted m   proximity  clusters r  s  denoted drs  algorithm  composed   following steps  linkages  naive algorithm  single linkage clustering  essentially    kruskals algorithm  minimum spanning trees however  single linkage clustering  order   clusters  formed  important   minimum spanning trees  matters   set  pairs  points  form distances chosen   algorithm alternative linkage schemes include complete linkage clustering average linkage clustering  wards method   naive algorithm  agglomerative clustering implementing  different linkage scheme may  accomplished simply  using  different formula  calculate intercluster distances   algorithm  formula    adjusted   highlighted using bold text however  efficient algorithms    one described    generalize   linkage schemes    way faster algorithms  naive algorithm  singlelinkage clustering  easy  understand  slow  time complexity formula  r sibson proposed  algorithm  time complexity formula  space complexity formula  optimal known  slink  slink algorithm represents  clustering   set  formula numbered items  two functions  functions   determined  finding  smallest cluster formula  contains  item formula   least one largernumbered item  first function formula maps item formula   largestnumbered item  cluster formula  second function formula maps item formula   distance associated   creation  cluster formula storing  functions  two arrays  map  item number   function value takes space formula   information  sufficient  determine  clustering   sibson shows   new item  added   set  items  updated functions representing  new singlelinkage clustering   augmented set represented    way can  constructed   old clustering  time formula  slink algorithm  loops   items one  one adding    representation   clustering  alternative algorithm running    optimal time  space bounds  based   equivalence   naive algorithm  kruskals algorithm  minimum spanning trees instead  using kruskals algorithm one can use prims algorithm   variation without binary heaps  takes time formula  space formula  construct  minimum spanning tree    clustering   given items  distances  applying kruskals algorithm   sparse graph formed   edges   minimum spanning tree produces  clustering    additional time formula  space formula\r\n"}
{"index":{"_id":105}}
{"conceptLabelTag":"training set","conceptLabel":"training set","conceptDescription":"test set  many areas  information science finding predictive relationships  data    important task initial discovery  relationships  usually done   training set   test set  validation set  used  evaluating whether  discovered relationships hold  formally  training set   set  data used  discover potentially predictive relationships  test set   set  data used  assess  strength  utility   predictive relationship test  training sets  used  intelligent systems machine learning genetic programming  statistics rationale regression analysis  one   earliest  approaches   developed  data used  construct  discover  predictive relationship  called  training data set  approaches  search  training data  empirical relationships tend  overfit  data meaning   can identify apparent relationships   training data    hold  general  test set   set  data   independent   training data   follows   probability distribution   training data   model fit   training set also fits  test set well minimal overfitting  taken place  better fitting   training set  opposed   test set usually points  overfitting validation set  order  avoid overfitting   classification parameter needs   adjusted   necessary    validation set  addition   training  test sets  example    suitable classifier   problem  sought  training set  used  train  candidate algorithms  validation set  used  compare  performances  decide  one  take  finally  test set  used  obtain  performance characteristics   accuracy sensitivity specificity fmeasure     validation set functions   hybrid   training data used  testing  neither  part   lowlevel training   part   final testing  simply part   training set can  set aside  used   validation set   known    common proportions  trainingvalidation alternatively  process can  repeated repeatedly partitioning  original training set   training set   validation set   known  crossvalidation  repeated partitions can  done  various ways   dividing  equal sets  using   trainingvalidation   validationtraining  repeatedly selecting  random subset   validation set  can  defined   basic process  using  validation set  model selection  part  training set validation set  test set   application   process   early stopping   candidate models  successive iterations    network  training stops   error   validation set grows choosing  previous model  one  minimum error sometimes  training set  validation set  referred  collectively   first part   design set   training set  second part   validation step hierarchical classification another example  parameter adjustment  hierarchical classification sometimes referred   instance space decomposition  splits  complete multiclass problem   set  smaller classication problems  serves  learning  accurate concepts due  simpler classication boundaries  subtasks  individual feature selection procedures  subtasks   classication decomposition  central choice   order  combination  smaller classication steps called  classication path depending   application  can  derived   confusion matrix  uncovering  reasons  typical errors  finding ways  prevent  system make    future  example   validation set one can see  classes   frequently mutually confused   system    instance space decomposition  done  follows firstly  classification  done among well recognizable classes   difficult  separate classes  treated   single joint class  finally   second classification step  joint class  classified   two initially mutually confused classes use  artificial intelligence machine learning  statistics  artificial intelligence  machine learning  training set consists   input vector   answer vector   used together   supervised learning method  train  knowledge database eg  neural net   naive bayes classifier used   ai machine validation sets can  used  regularization  early stopping stop training   error   validation set increases     sign  overfitting   training set  simple procedure  complicated  practice   fact   validation error may fluctuate  training producing multiple local minima  complication  led   creation  many adhoc rules  deciding  overfitting  truly begun  statistical modeling  training set  used  fit  model  can  used  predict  response value  one   predictors  fitting can include  variable selection  parameter estimation statistical models used  prediction  often called regression models   linear regression  logistic regression  two examples   fields  major emphasis  placed  avoiding overfitting    achieve  best possible performance   independent test set  follows   probability distribution   training set use  intelligent systems  general  intelligent system consists   function taking one   arguments  results   output vector   learning methods task   run  system    input vector   arguments calculating  output vector comparing    answer vector   changing somewhat  order  get  output vector  like  answer vector next time  system  simulated\r\n"}
{"index":{"_id":106}}
{"conceptLabelTag":"conceptual clustering","conceptLabel":"conceptual clustering","conceptDescription":"conceptual clustering conceptual clustering   machine learning paradigm  unsupervised classification developed mainly   s   distinguished  ordinary data clustering  generating  concept description   generated class  conceptual clustering methods  capable  generating hierarchical category structures see categorization   information  hierarchy conceptual clustering  closely related  formal concept analysis decision tree learning  mixture model learning conceptual clustering vs data clustering conceptual clustering  obviously closely related  data clustering however  conceptual clustering      inherent structure   data  drives cluster formation  also  description language   available   learner thus  statistically strong grouping   data may fail   extracted   learner   prevailing concept description language  incapable  describing  particular regularity   implementations  description language   limited  feature conjunction although  cobweb see   feature language  probabilistic list  published algorithms  fair number  algorithms   proposed  conceptual clustering  examples  given   general discussions  reviews  conceptual clustering can  found   following publications example  basic conceptual clustering algorithm  section discusses  rudiments   conceptual clustering algorithm cobweb   many  algorithms using different heuristics  category goodness  category evaluation criteria  cobweb  one   best known  reader  referred   bibliography   methods knowledge representation  cobweb data structure   hierarchy tree wherein  node represents  given concept  concept represents  set actually  multiset  bag  objects  object  represented   binaryvalued property list  data associated   tree node ie concept   integer property counts   objects   concept  example see figure let  concept formula contain  following four objects repeated objects  permitted  three properties might   example codice    stored   concept node   property count codice indicating    objects   concept  male   objects  wings    objects  nocturnal  concept description   categoryconditional probability likelihood   properties   node thus given   object   member  category concept formula  likelihood    male  formula likewise  likelihood   object  wings  likelihood   object  nocturnal    formula  concept description can therefore simply  given  codice  corresponds   formulaconditional feature likelihood ie formula  figure   right shows  concept tree  five concepts formula   root concept  contains  ten objects   data set concepts formula  formula   children  formula  former containing four objects   later containing six objects concept formula  also  parent  concepts formula formula  formula  contain three two  one object respectively note   parent node relative superordinate concept contains   objects contained   child nodes relative subordinate concepts  fishers description  cobweb  indicates    total attribute counts  conditional probabilities   object lists  stored   nodes  probabilities  computed   attribute counts  needed  cobweb language  description language  cobweb   language    loose sense   fully probabilistic   capable  describing  concept however  constraints  placed   probability ranges  concepts may represent   stronger language  obtained  example  might permit  concepts wherein  least one probability differs     formula   constraint  formula  concept   codice    constructed   learner however  concept   codice   accessible   least one probability differs     formula thus  constraints     obtain something like  traditional concept language   limiting case  formula  every feature  thus every probability   concept must    result   feature language base  conjunction   every concept  can  represented can   described   conjunction  features   negations  concepts    described   way   represented evaluation criterion  fishers description  cobweb  measure  uses  evaluate  quality   hierarchy  gluck  corters category utility cu measure   rederives   paper  motivation   measure  highly similar   information gain measure introduced  quinlan  decision tree learning   previously  shown   cu  featurebased classification      mutual information   feature variables   class variable gluck corter corter gluck  since  measure  much better known  proceed   mutual information   measure  category goodness   wish  evaluate   overall utility  grouping  objects   particular hierarchical categorization structure given  set  possible classification structures  need  determine whether one  better  another\r\n"}
{"index":{"_id":107}}
{"conceptLabelTag":"gibbs sampling","conceptLabel":"gibbs sampling","conceptDescription":"gibbs sampling  statistics gibbs sampling   gibbs sampler   markov chain monte carlo mcmc algorithm  obtaining  sequence  observations   approximated   specified multivariate probability distribution  direct sampling  difficult  sequence can  used  approximate  joint distribution eg  generate  histogram   distribution  approximate  marginal distribution  one   variables   subset   variables  example  unknown parameters  latent variables   compute  integral    expected value  one   variables typically    variables correspond  observations whose values  known  hence   need   sampled gibbs sampling  commonly used   means  statistical inference especially bayesian inference    randomized algorithm ie  algorithm  makes use  random numbers    alternative  deterministic algorithms  statistical inference    expectationmaximization algorithm em    mcmc algorithms gibbs sampling generates  markov chain  samples     correlated  nearby samples   result care must  taken  independent samples  desired typically  thinning  resulting chain  samples   taking every nth value eg every th value  addition samples   beginning   chain  burnin period may  accurately represent  desired distribution introduction gibbs sampling  named   physicist josiah willard gibbs  reference   analogy   sampling algorithm  statistical physics  algorithm  described  brothers stuart  donald geman   eight decades   death  gibbs   basic version gibbs sampling   special case   metropolishastings algorithm however   extended versions see   can  considered  general framework  sampling   large set  variables  sampling  variable    cases  group  variables  turn  can incorporate  metropolishastings algorithm   sophisticated methods   slice sampling adaptive rejection sampling  adaptive rejection metropolis algorithms  implement one     sampling steps gibbs sampling  applicable   joint distribution   known explicitly   difficult  sample  directly   conditional distribution   variable  known   easy   least easier  sample   gibbs sampling algorithm generates  instance   distribution   variable  turn conditional   current values    variables  can  shown see  example gelman et al   sequence  samples constitutes  markov chain   stationary distribution   markov chain  just  soughtafter joint distribution gibbs sampling  particularly welladapted  sampling  posterior distribution   bayesian network since bayesian networks  typically specified   collection  conditional distributions implementation gibbs sampling   basic incarnation   special case   metropolishastings algorithm  point  gibbs sampling   given  multivariate distribution   simpler  sample   conditional distribution   marginalize  integrating   joint distribution suppose  want  obtain formula samples  formula   joint distribution formula denote  formulath sample  formula  proceed  follows   sampling  performed  important facts hold  performing  sampling relation  conditional distribution  joint distribution furthermore  conditional distribution  one variable given  others  proportional   joint distribution proportional    case means   denominator    function  formula  thus      values  formula  forms part   normalization constant   distribution  formula  practice  determine  nature   conditional distribution   factor formula   easiest  factor  joint distribution according   individual conditional distributions defined   graphical model   variables ignore  factors    functions  formula    together   denominator  constitute  normalization constant   reinstate  normalization constant   end  necessary  practice  means  one  three things inference gibbs sampling  commonly used  statistical inference eg determining  best value   parameter   determining  number  people likely  shop   particular store   given day  candidate  voter will  likely vote  etc  idea   observed data  incorporated   sampling process  creating separate variables   piece  observed data  fixing  variables  question   observed values rather  sampling   variables  distribution   remaining variables   effectively  posterior distribution conditioned   observed data   likely value   desired parameter  mode   simply  selected  choosing  sample value  occurs  commonly   essentially equivalent  maximum  posteriori estimation   parameter since  parameters  usually continuous   often necessary  bin  sampled values  one   finite number  ranges  bins  order  get  meaningful estimate   mode  commonly however  expected value mean  average   sampled values  chosen    bayes estimator  takes advantage   additional data   entire distribution   available  bayesian sampling whereas  maximization algorithm   expectation maximization em  capable   returning  single point   distribution  example   unimodal distribution  mean expected value  usually similar   mode  common value    distribution  skewed  one direction  mean will  moved   direction  effectively accounts   extra probability mass   direction note however    distribution  multimodal  expected value may  return  meaningful point     modes  typically  better choice although    variables typically correspond  parameters  interest others  uninteresting nuisance variables introduced   model  properly express  relationships among variables although  sampled values represent  joint distribution   variables  nuisance variables can simply  ignored  computing expected values  modes   equivalent  marginalizing   nuisance variables   value  multiple variables  desired  expected value  simply computed   variable separately  computing  mode however  variables must  considered together supervised learning unsupervised learning  semisupervised learning aka learning  missing values can   handled  simply fixing  values   variables whose values  known  sampling   remainder  observed data  will  one variable   observationrather   example one variable corresponding   sample mean  sample variance   set  observations  fact  generally will   variables   corresponding  concepts   sample mean  sample variance instead    case  will  variables representing  unknown true mean  true variance   determination  sample values   variables results automatically   operation   gibbs sampler generalized linear models ie variations  linear regression can sometimes  handled  gibbs sampling  well  example probit regression  determining  probability   given binary yesno choice  normally distributed priors placed   regression coefficients can  implemented  gibbs sampling    possible  add additional variables  take advantage  conjugacy however logistic regression   handled  way one possibility   approximate  logistic function   mixture typically  normal distributions  commonly however metropolishastings  used instead  gibbs sampling mathematical background suppose   sample formula  taken   distribution depending   parameter vector formula  length formula  prior distribution formula  may   formula   large   numerical integration  find  marginal densities   formula   computationally expensive   alternative method  calculating  marginal densities   create  markov chain   space formula  repeating  two steps  steps define  reversible markov chain   desired invariant distribution formula  can  proved  follows define formula  formula   formula  let formula denote  probability     formula  formula   transition probabilities   since formula   equivalence relation thus  detailed balance equations  satisfied implying  chain  reversible    invariant distribution formula  practice  suffix formula   chosen  random   chain cycles   suffixes  order  general  gives  nonstationary markov process   individual step will still  reversible   overall process will still   desired stationary distribution  long   chain can access  states   fixed ordering variations  extensions numerous variations   basic gibbs sampler exist  goal   variations   reduce  autocorrelation  samples sufficiently  overcome  added computational costs collapsed gibbs sampler implementing  collapsed gibbs sampler collapsing dirichlet distributions  hierarchical bayesian models  categorical variables   latent dirichlet allocation  various  models used  natural language processing   quite common  collapse   dirichlet distributions   typically used  prior distributions   categorical variables  result   collapsing introduces dependencies among   categorical variables dependent   given dirichlet prior   joint distribution   variables  collapsing   dirichletmultinomial distribution  conditional distribution   given categorical variable   distribution conditioned   others assumes  extremely simple form  makes gibbs sampling even easier    collapsing    done  rules   follows collapsing  conjugate priors  general  conjugate prior can  collapsed     children  distributions conjugate    relevant math  discussed   article  compound distributions     one child node  result will often assume  known distribution  example collapsing  inversegammadistributed variance    network   single gaussian child will yield  students tdistribution   matter collapsing   mean  variance   single gaussian child will still yield  students tdistribution provided   conjugate ie gaussian mean inversegamma variance    multiple child nodes  will  become dependent    dirichletcategorical case  resulting joint distribution will   closed form  resembles   ways  compound distribution although  will   product   number  factors one   child node    addition   importantly  resulting conditional distribution  one   child nodes given  others  also given  parents   collapsed nodes   given  children   child nodes will    density   posterior predictive distribution    remaining child nodes furthermore  posterior predictive distribution    density   basic compound distribution   single node although  different parameters  general formula  given   article  compound distributions  example given  bayes network   set  conditionally independent identically distributed gaussiandistributed nodes  conjugate prior distributions placed   mean  variance  conditional distribution  one node given  others  compounding    mean  variance will   students tdistribution similarly  result  compounding   gamma prior   number  poissondistributed nodes causes  conditional distribution  one node given  others  assume  negative binomial distribution   cases  compounding produces  wellknown distribution efficient sampling procedures often exist  using  will often although  necessarily   efficient   collapsing  instead sampling  prior  child nodes separately however   case   compound distribution   wellknown  may   easy  sample  since  generally will  belong   exponential family  typically will   logconcave   make  easy  sample using adaptive rejection sampling since  closed form always exists   case   child nodes   collapsed nodes   children  conditional distribution  one   child nodes given   nodes   graph will   take  account  distribution   secondlevel children  particular  resulting conditional distribution will  proportional   product   compound distribution  defined    conditional distributions     child nodes given  parents   given   children  follows   fact   full conditional distribution  proportional   joint distribution   child nodes   collapsed nodes  continuous  distribution will generally     known form  may well  difficult  sample  despite  fact   closed form can  written    reasons  described   nonwellknown compound distributions however   particular case   child nodes  discrete sampling  feasible regardless  whether  children   child nodes  continuous  discrete  fact  principle involved   described  fair detail   article   dirichletmultinomial distribution samplerswithingibbs   extensions   also possible  extend gibbs sampling  various ways  example   case  variables whose conditional distribution   easy  sample   single iteration  slice sampling   metropolishastings algorithm can  used  sample   variables  question   efficient alternative   application   adaptive rejection sampling ars methods  sampling   fullconditional densities   ars techniques   applied  adaptive rejection metropolis sampling algorithms  often employed furthermore  alternatives can  found  literature   also possible  incorporate variables    random variables  whose value  deterministically computed   variables generalized linear models eg logistic regression aka maximum entropy models can  incorporated   fashion bugs  example allows  type  mixing  models failure modes   two ways  gibbs sampling can fail  first     islands  highprobability states   paths    example consider  probability distribution  bit vectors   vectors    probability    two vectors   probability zero gibbs sampling will become trapped  one   two highprobability vectors  will never reach   one  generally   distribution  highdimensional realvalued vectors  two particular elements   vector  perfectly correlated  perfectly anticorrelated  two elements will become stuck  gibbs sampling will never  able  change   second problem can happen even   states  nonzero probability      single island  highprobability states  example consider  probability distribution  bit vectors   allzeros vector occurs  probability    vectors  equally probable     probability  formula    want  estimate  probability   zero vector    sufficient  take  samples   true distribution    likely give  answer  close     probably   take   formula samples  gibbs sampling  get   result  computer      lifetime  problem occurs  matter  long  burnin period       true distribution  zero vector occurs half  time   occurrences  randomly mixed    nonzero vectors even  small sample will see  zero  nonzero vectors  gibbs sampling will alternate  returning   zero vector  long periods  formula   row   nonzero vectors  long periods  formula   row thus convergence   true distribution  extremely slow requiring much   formula steps taking  many steps   computationally feasible   reasonable time period  slow convergence  can  seen   consequence   curse  dimensionality note   problem like  can  solved  block sampling  entire bit vector    assumes   bit vector  part   larger set  variables   vector    thing  sampled  block sampling  equivalent    gibbs sampling     hypothesis   difficult software  openbugs software bayesian inference using gibbs sampling   bayesian analysis  complex statistical models using markov chain monte carlo jags just another gibbs sampler   gpl program  analysis  bayesian hierarchical models using markov chain monte carlo church   software  performing gibbs inference  arbitrary distributions   specified  probabilistic programs pymc   open source python library  bayesian learning  general probabilistic graphical model  advanced features  easy  use interface iarms   matlab code   independent doubly adaptive rejection metropolis sampling method  drawing   fullconditional densities\r\n"}
{"index":{"_id":108}}
{"conceptLabelTag":"feed forward","conceptLabel":"feed forward","conceptDescription":"feedforward neural network  feedforward neural network   artificial neural network wherein connections   units   form  cycle     different  recurrent neural networks  feedforward neural network   first  simplest type  artificial neural network devised   network  information moves   one direction forward   input nodes   hidden nodes      output nodes    cycles  loops   network singlelayer perceptron  simplest kind  neural network   singlelayer perceptron network  consists   single layer  output nodes  inputs  fed directly   outputs via  series  weights   way  can  considered  simplest kind  feedforward network  sum   products   weights   inputs  calculated   node    value    threshold typically  neuron fires  takes  activated value typically otherwise  takes  deactivated value typically neurons   kind  activation function  also called artificial neurons  linear threshold units   literature  term perceptron often refers  networks consisting  just one   units  similar neuron  described  warren mcculloch  walter pitts   s  perceptron can  created using  values   activated  deactivated states  long   threshold value lies   two  perceptrons  outputs     threshold      evidence   networks can  trained  quickly  networks created  nodes  different activation  deactivation values perceptrons can  trained   simple learning algorithm   usually called  delta rule  calculates  errors  calculated output  sample output data  uses   create  adjustment   weights thus implementing  form  gradient descent singleunit perceptrons   capable  learning linearly separable patterns    famous monograph entitled perceptrons marvin minsky  seymour papert showed    impossible   singlelayer perceptron network  learn  xor function nonetheless   known  multilayer perceptrons  capable  producing  possible boolean function although  single threshold unit  quite limited   computational power    shown  networks  parallel threshold units can approximate  continuous function   compact interval   real numbers   interval  result can  found  peter auer harald burgsteiner  wolfgang maass  learning rule   simple universal approximators consisting   single layer  perceptrons  multilayer neural network can compute  continuous output instead   step function  common choice   socalled logistic function   choice  singlelayer network  identical   logistic regression model widely used  statistical modeling  logistic function  also known   sigmoid function    continuous derivative  allows    used  backpropagation  function  also preferred   derivative  easily calculated multilayer perceptron  class  networks consists  multiple layers  computational units usually interconnected   feedforward way  neuron  one layer  directed connections   neurons   subsequent layer  many applications  units   networks apply  sigmoid function   activation function  universal approximation theorem  neural networks states  every continuous function  maps intervals  real numbers   output interval  real numbers can  approximated arbitrarily closely   multilayer perceptron  just one hidden layer  result holds   wide range  activation functions eg   sigmoidal functions multilayer networks use  variety  learning techniques   popular  backpropagation   output values  compared   correct answer  compute  value   predefined errorfunction  various techniques  error   fed back   network using  information  algorithm adjusts  weights   connection  order  reduce  value   error function   small amount  repeating  process   sufficiently large number  training cycles  network will usually converge   state   error   calculations  small   case one  say   network  learned  certain target function  adjust weights properly one applies  general method  nonlinear optimization   called gradient descent    network calculates  derivative   error function  respect   network weights  changes  weights    error decreases thus going downhill   surface   error function   reason backpropagation can   applied  networks  differentiable activation functions  general  problem  teaching  network  perform well even  samples    used  training samples   quite subtle issue  requires additional techniques   especially important  cases    limited numbers  training samples  available  danger    network overfits  training data  fails  capture  true statistical process generating  data computational learning theory  concerned  training classifiers   limited amount  data   context  neural networks  simple heuristic called early stopping often ensures   network will generalize well  examples    training set  typical problems   backpropagation algorithm   speed  convergence   possibility  ending    local minimum   error function today   practical methods  make backpropagation  multilayer perceptrons  tool  choice  many machine learning tasks one also can use  series  independent neural networks moderated   intermediary  similar behavior  happens  brain  neurons can perform separably  handle  large task   results can  finally combined\r\n"}
{"index":{"_id":109}}
{"conceptLabelTag":"dbscan","conceptLabel":"dbscan","conceptDescription":"dbscan densitybased spatial clustering  applications  noise dbscan   data clustering algorithm proposed  martin ester hanspeter kriegel j rg sander  xiaowei xu     densitybased clustering algorithm given  set  points   space  groups together points   closely packed together points  many nearby neighbors marking  outliers points  lie alone  lowdensity regions whose nearest neighbors   far away dbscan  one    common clustering algorithms  also  cited  scientific literature   algorithm  awarded  test  time award  award given  algorithms   received substantial attention  theory  practice   leading data mining conference kdd preliminary consider  set  points   space   clustered   purpose  dbscan clustering  points  classified  core points densityreachable points  outliers  follows now    core point   forms  cluster together   points core  noncore   reachable    cluster contains  least one core point noncore points can  part   cluster   form  edge since    used  reach  points reachability    symmetric relation since  definition  point may  reachable   noncore point regardless  distance   noncore point may  reachable  nothing can  reached   therefore   notion  connectedness  needed  formally define  extent   clusters found  dbscan two points   densityconnected     point      densityreachable  densityconnectedness  symmetric  cluster  satisfies two properties algorithm dbscan requires two parameters eps   minimum number  points required  form  dense region minpts  starts   arbitrary starting point     visited  points neighborhood  retrieved    contains sufficiently many points  cluster  started otherwise  point  labeled  noise note   point might later  found   sufficiently sized environment   different point  hence  made part   cluster   point  found    dense part   cluster  neighborhood  also part   cluster hence  points   found within  neighborhood  added     neighborhood    also dense  process continues   densityconnected cluster  completely found   new unvisited point  retrieved  processed leading   discovery    cluster  noise  algorithm can  expressed  follows  pseudocode following  original published nomenclature note   algorithm can  simplified  merging  perpoint   visited  belongs  cluster c logic  well   inlining  contents   expandcluster subroutine    called  one place  simplifications   omitted    pseudocode  order  reflect  originally published version additionally  regionquery function need  return p   list  points   visited  long    otherwise still counted   local density estimate complexity dbscan visits  point   database possibly multiple times eg  candidates  different clusters  practical considerations however  time complexity  mostly governed   number  regionquery invocations dbscan executes exactly one  query   point    indexing structure  used  executes  neighborhood query   overall average runtime complexity   obtained  parameter  chosen   meaningful way ie    average  points  returned without  use   accelerating index structure   degenerated data eg  points within  distance less   worst case run time complexity remains  distance matrix  size can  materialized  avoid distance recomputations   needs memory whereas  nonmatrix based implementation  dbscan  needs memory see  section   extensions  algorithmic modifications  handle  issues parameter estimation every data mining task   problem  parameters every parameter influences  algorithm  specific ways  dbscan  parameters  minpts  needed  parameters must  specified   user ideally  value   given   problem  solve eg  physical distance  minpts    desired minimum cluster size optics can  seen   generalization  dbscan  replaces  parameter   maximum value  mostly affects performance minpts  essentially becomes  minimum cluster size  find   algorithm  much easier  parameterize  dbscan  results   bit  difficult  use   will usually produce  hierarchical clustering instead   simple data partitioning  dbscan produces recently one   original authors  dbscan  revisited dbscan  optics  published  refined version  hierarchical dbscan hdbscan   longer   notion  border points extensions generalized dbscan gdbscan   generalization    authors  arbitrary neighborhood  dense predicates   minpts parameters  removed   original algorithm  moved   predicates  example  polygon data  neighborhood    intersecting polygon whereas  density predicate uses  polygon areas instead  just  object count various extensions   dbscan algorithm   proposed including methods  parallelization parameter estimation  support  uncertain data  basic idea   extended  hierarchical clustering   optics algorithm dbscan  also used  part  subspace clustering algorithms like predecon  subclu hdbscan   hierarchical version  dbscan   also faster  optics    flat partition consisting    prominent clusters can  extracted   hierarchy availability different implementations    algorithm  found  exhibit enormous performance differences   fastest   test data set finishing  seconds  slowest taking seconds  differences can  attributed  implementation quality language  compiler differences   use  indexes  acceleration\r\n"}
{"index":{"_id":110}}
{"conceptLabelTag":"pac learning","conceptLabel":"pac learning","conceptDescription":"probably approximately correct learning  computational learning theory probably approximately correct learning pac learning   framework  mathematical analysis  machine learning   proposed   leslie valiant   framework  learner receives samples  must select  generalization function called  hypothesis   certain class  possible functions  goal    high probability  probably part  selected function will  low generalization error  approximately correct part  learner must  able  learn  concept given  arbitrary approximation ratio probability  success  distribution   samples  model  later extended  treat noise misclassified samples  important innovation   pac framework   introduction  computational complexity theory concepts  machine learning  particular  learner  expected  find efficient functions time  space requirements bounded   polynomial   example size   learner  must implement  efficient procedure requiring  example count bounded   polynomial   concept size modified   approximation  likelihood bounds definitions  terminology  order  give  definition  something   paclearnable  first   introduce  terminology   following definitions two examples will  used  first   problem  character recognition given  array  formula bits encoding  binaryvalued image   example   problem  finding  interval  will correctly classify points within  interval  positive   points outside   range  negative let formula   set called  instance space   encoding    samples   instance  length assigned   character recognition problem  instance space  formula   interval problem  instance space  formula  formula denotes  set   real numbers  concept   subset formula one concept   set   patterns  bits  formula  encode  picture   letter p  example concept   second example   set     numbers  formula  formula  concept class formula   set  concepts  formula     set   subsets   array  bits   skeletonized connected width   font  let formula   procedure  draws  example formula using  probability distribution formula  gives  correct label formula    formula  otherwise now given formula assume    algorithm formula   polynomial formula  formula   relevant parameters   class formula   given  sample  size p drawn according  formula   probability   least formula formula outputs  hypothesis formula    average error less   equal  formula  formula    distribution formula     statement  algorithm formula  true  every concept formula   every distribution formula  formula    formula  formula  efficiently pac learnable  distribution pac learnable  can also say  formula   pac learning algorithm  formula equivalence   regularity conditions  three conditions  equivalent\r\n"}
{"index":{"_id":111}}
{"conceptLabelTag":"kullback leibler divergence","conceptLabel":"kullback leibler divergence","conceptDescription":"kullbackleibler divergence  probability theory  information theory  kullbackleibler divergence also called information divergence information gain relative entropy klic  kl divergence   measure    metric   nonsymmetric difference  two probability distributions p  q  kullbackleibler divergence  originally introduced  solomon kullback  richard leibler    directed divergence  two distributions kullback  preferred  name discrimination information   discussed  kullbacks historic text information theory  statistics expressed   language  bayesian inference  kullbackleibler divergence  q  p denoted dpq   measure   information gained  one revises ones beliefs   prior probability distribution q   posterior probability distribution p   words    amount  information lost  q  used  approximate p  applications p typically represents  true distribution  data observations   precisely calculated theoretical distribution  q typically represents  theory model description  approximation  p  kullbackleibler divergence   special case   broader class  divergences called fdivergences  well   class  bregman divergences      divergence  probabilities    member   classes although   often intuited   way  measuring  distance  probability distributions  kullbackleibler divergence    true metric    obey  triangle inequality   general dpq   equal dqp however  infinitesimal form specifically  hessian gives  metric tensor known   fisher information metric   context  machine learning  kullbackleibler divergence  often called  information gain achieved  p  used instead  q  analogy  information theory   also called  relative entropy  p  respect  q   context  coding theory kullbackleibler divergence can  construed  measure  expected number  extra bits required  code samples  p using  code optimized  q rather   code optimized  p definition  discrete probability distributions p  q  kullbackleibler divergence  q  p  defined    words    expectation   logarithmic difference   probabilities p  q   expectation  taken using  probabilities p  kullbackleibler divergence  defined   qi implies pi    absolute continuity whenever pi  zero  contribution   ith term  interpreted  zero  formula  distributions p  q   continuous random variable  kullbackleibler divergence  defined    integral  p  q denote  densities  p  q  generally  p  q  probability measures   set x  p  absolutely continuous  respect  q   kullbackleibler divergence  q  p  defined    latter stands   usual convergence  total variation following r nyi fisher information metric however  kullbackleibler divergence  rather directly related   metric specifically  fisher information metric  can  made explicit  follows assume   probability distributions p  q   parameterized   possibly multidimensional parameter formula consider  two close  values  formula  formula    parameter formula differs    small amount   parameter value formula specifically   first order one  using  einstein summation convention  formula  small change  formula   j direction  formula  corresponding rate  change   probability distribution since  kullbackleibler divergence   absolute minimum  p q ie formula  changes   second order   small parameters formula  formally    minimum  first derivatives   divergence vanish    taylor expansion one    second order   hessian matrix   divergence must  positive semidefinite letting formula vary  dropping  subindex  hessian formula defines  possibly degenerate riemannian metric   formula parameter space called  fisher information metric relation   quantities  information theory many    quantities  information theory can  interpreted  applications   kullbackleibler divergence  specific cases  selfinformation   kullbackleibler divergence   probability distribution pi   kronecker delta representing certainty   m ie  number  extra bits  must  transmitted  identify     probability distribution pi  available   receiver   fact   m  mutual information   kullbackleibler divergence   product pxpy   two marginal probability distributions   joint probability distribution pxy ie  expected number  extra bits  must  transmitted  identify x  y    coded using   marginal distributions instead   joint distribution equivalently   joint probability pxy  known    expected number  extra bits  must  average  sent  identify y   value  x   already known   receiver  shannon entropy   number  bits      transmitted  identify x  n equally likely possibilities less  kullbackleibler divergence   uniform distribution px   true distribution px ie less  expected number  bits saved       sent   value  x  coded according   uniform distribution px rather   true distribution px  conditional entropy   number  bits      transmitted  identify x  n equally likely possibilities less  kullbackleibler divergence   product distribution px py   true joint distribution pxy ie less  expected number  bits saved       sent   value  x  coded according   uniform distribution px rather   conditional distribution px y  x given y  cross entropy  two probability distributions measures  average number  bits needed  identify  event   set  possibilities   coding scheme  used based   given probability distribution formula rather   true distribution formula  cross entropy  two distributions formula  formula    probability space  thus defined  follows kullbackleibler divergence  bayesian updating  bayesian statistics  kullbackleibler divergence can  used   measure   information gain  moving   prior distribution   posterior distribution formula   new fact formula  discovered  can  used  update  posterior distribution  formula  formula   new posterior distribution formula using bayes theorem  distribution   new entropy  may  less   greater   original entropy formula however   standpoint   new probability distribution one can estimate    used  original code based  formula instead   new code based  formula   added  expected number  bits   message length  therefore represents  amount  useful information  information gain  formula   can estimate   learned  discovering formula    piece  data formula subsequently comes   probability distribution  formula can  updated   give  new best guess formula  one reinvestigates  information gain  using formula rather  formula  turns    may  either greater  less  previously estimated    combined information gain   obey  triangle inequality  one can say    average averaging using formula  two sides will average  bayesian experimental design  common goal  bayesian experimental design   maximise  expected kullbackleibler divergence   prior   posterior  posteriors  approximated   gaussian distributions  design maximising  expected kullbackleibler divergence  called bayes doptimal discrimination information  kullbackleibler divergence d pxh pxh can also  interpreted   expected discrimination information  h  h  mean information per sample  discriminating  favor   hypothesis h   hypothesis h  hypothesis h  true another name   quantity given    ij good   expected weight  evidence  h  h   expected   sample  expected weight  evidence  h  h       information gain expected per sample   probability distribution ph   hypotheses either   two quantities can  used   utility function  bayesian experimental design  choose  optimal next question  investigate   will  general lead  rather different experimental strategies   entropy scale  information gain    little difference  near certainty  absolute certaintycoding according   near certainty requires hardly   bits  coding according   absolute certainty    hand   logit scale implied  weight  evidence  difference   two  enormous infinite perhaps  might reflect  difference   almost sure   probabilistic level  say  riemann hypothesis  correct compared   certain    correct  one   mathematical proof  two different scales  loss function  uncertainty   useful according   well  reflects  particular circumstances   problem  question principle  minimum discrimination information  idea  kullbackleibler divergence  discrimination information led kullback  propose  principle  minimum discrimination information mdi given new facts  new distribution f   chosen    hard  discriminate   original distribution f  possible    new data produces  small  information gain d f f  possible  example  one   prior distribution pxa  x    subsequently learnt  true distribution    ua  kullbackleibler divergence   new joint distribution  x   qxa ua   earlier prior distribution   ie  sum   kullbackleibler divergence  pa  prior distribution     updated distribution ua plus  expected value using  probability distribution ua   kullbackleibler divergence   prior conditional distribution pxa   new conditional distribution qxa note  often  later expected value  called  conditional kullbackleibler divergence  conditional relative entropy  denoted  dqxapxa   minimized  qxa pxa   whole support  ua   note   result incorporates bayes theorem   new distribution ua   fact  function representing certainty    one particular value mdi can  seen   extension  laplaces principle  insufficient reason   principle  maximum entropy  et jaynes  particular    natural extension   principle  maximum entropy  discrete  continuous distributions   shannon entropy ceases    useful see differential entropy   kullbackleibler divergence continues   just  relevant   engineering literature mdi  sometimes called  principle  minimum crossentropy mce  minxent  short minimising  kullbackleibler divergence  m  p  respect  m  equivalent  minimizing  crossentropy  p  m since   appropriate  one  trying  choose  adequate approximation  p however   just  often   task one  trying  achieve instead just  often   m    fixed prior reference measure  p  one  attempting  optimise  minimising dpm subject   constraint   led   ambiguity   literature   authors attempting  resolve  inconsistency  redefining crossentropy   dpm rather  hpm relationship  available work surprisals add  probabilities multiply  surprisal   event  probability formula  defined  formula  formula  formula  surprisal   formulanats bits  formula    instance   formula bits  surprisal  landing  heads   toss  formula coins bestguess states eg  atoms   gas  inferred  maximizing  average surprisal formula entropy   given set  control parameters like pressure formula  volume formula  constrained entropy maximization  classically  quantum mechanically minimizes gibbs availability  entropy units formula  formula   constrained multiplicity  partition function  temperature formula  fixed  energy formula  also minimized thus  formula  number  molecules formula  constant  helmholtz  energy formula  formula  energy  minimized   system equilibrates  formula  formula  held constant say  processes   body  gibbs  energy formula  minimized instead  change   energy   conditions   measure  available work  might  done   process thus available work   ideal gas  constant temperature formula  pressure formula  formula  formula  formula see also gibbs inequality  generally  work available relative   ambient  obtained  multiplying ambient temperature formula  kullbackleibler divergence  net surprisal formula defined   average value  formula  formula   probability   given state  ambient conditions  instance  work available  equilibrating  monatomic ideal gas  ambient values  formula  formula  thus formula  kullbackleibler divergence formula  resulting contours  constant kullbackleibler divergence shown  right   mole  argon  standard temperature  pressure  example put limits   conversion  hot  cold   flamepowered airconditioning    unpowered device  convert boilingwater  icewater discussed  thus kullbackleibler divergence measures thermodynamic availability  bits quantum information theory  density matrices p  q   hilbert space  kl divergence  quantum relative entropy    often called   case  q  p  defined    quantum information science  minimum  formula   separable states q can also  used   measure  entanglement   state p relationship  models  reality just  kullbackleibler divergence  actual  ambient measures thermodynamic availability kullbackleibler divergence  reality   model  also useful even    clues    reality   experimental measurements   former case kullbackleibler divergence describes distance  equilibrium   multiplied  ambient temperature  amount  available work    latter case  tells   surprises  reality    sleeve    words  much  model  yet  learn although  tool  evaluating models  systems   accessible experimentally may  applied   field  application  selecting  statistical model via akaike information criterion  particularly well described  papers   book  burnham  anderson   nutshell  kullbackleibler divergence  reality   model may  estimated  within  constant additive term   function like  squares summed   deviations observed  data   models predictions estimates   divergence  models  share   additive term can  turn  used  select among models  trying  fit parametrized models  data   various estimators  attempt  minimize kullbackleibler divergence   maximum likelihood  maximum spacing estimators symmetrised divergence kullback  leibler  actually defined  divergence    symmetric  nonnegative  quantity  sometimes  used  feature selection  classification problems  p  q   conditional pdfs   feature  two different classes  alternative  given via  divergence  can  interpreted   expected information gain  x  discovering  probability distribution x  drawn  p  q   currently  probabilities  respectively  value gives  jensenshannon divergence defined   m   average   two distributions d can also  interpreted   capacity   noisy information channel  two inputs giving  output distributions p  q  jensenshannon divergence like  fdivergences  locally proportional   fisher information metric   similar   hellinger metric   sense  induces   affine connection   statistical manifold  equal  onehalf  socalled jeffreys divergence relationship   probabilitydistance measures   many  important measures  probability distance     particularly connected   kullbackleibler divergence  example  notable measures  distance include  hellinger distance histogram intersection chisquared statistic quadratic form distance match distance kolmogorovsmirnov distance  earth movers distance data differencing just  absolute entropy serves  theoretical background  data compression relative entropy serves  theoretical background  data differencing  absolute entropy   set  data   sense   data required  reconstruct  minimum compressed size   relative entropy   target set  data given  source set  data   data required  reconstruct  target given  source minimum size   patch\r\n"}
{"index":{"_id":112}}
{"conceptLabelTag":"sigmoid function","conceptLabel":"sigmoid function","conceptDescription":"sigmoid function  sigmoid function   mathematical function   s shaped curve sigmoid curve often sigmoid function refers   special case   logistic function shown   first figure  defined   formula  examples  similar shapes include  gompertz curve used  modeling systems  saturate  large values  t   ogee curve used   spillway   dams sigmoid functions  finite limits  negative infinity  infinity  often going either      depending  convention  wide variety  sigmoid functions   used   activation function  artificial neurons including  logistic  hyperbolic tangent functions sigmoid curves  also common  statistics  cumulative distribution functions  go      integrals   logistic distribution  normal distribution  students t probability density functions definition  sigmoid function   bounded differentiable real function   defined   real input values    positive derivative   point properties  general  sigmoid function  realvalued  differentiable  either  nonnegative  nonpositive first derivative   bell shaped   also  pair  horizontal asymptotes  formula  differential equation formula   inclusion   boundary condition providing  third degree  dom formula provides  class  functions   type  logistic function    important property   derivative can  expressed   function  examples many natural processes     complex system learning curves exhibit  progression  small beginnings  accelerates  approaches  climax  time   detailed description  lacking  sigmoid function  often used besides  logistic function sigmoid functions include  ordinary arctangent  hyperbolic tangent  gudermannian function   error function  also  generalised logistic function  algebraic functions like formula  integral   smooth positive bumpshaped function will  sigmoidal thus  cumulative distribution functions  many common probability distributions  sigmoidal   famous  example   error function   related   cumulative distribution function cdf   normal distribution hard sigmoid simpler approximations  smooth sigmoid functions especially piecewise linear functions  piecewise constant functions  preferred   applications  speed  computation   important  precision extreme forms  known   hard sigmoid   particularly found  artificial intelligence especially computer vision  artificial neural networks   extreme examples   sign function  heaviside step function  go       use depends  normalization   example  theano library provides two approximations codice  codice    part linear approximation output line  slope output \r\n"}
{"index":{"_id":113}}
{"conceptLabelTag":"markov random field","conceptLabel":"markov random field","conceptDescription":"markov random field   domain  physics  probability  markov random field often abbreviated  mrf markov network  undirected graphical model   set  random variables   markov property described   undirected graph   words  random field  said   markov random field   satisfies markov properties  markov network  mrf  similar   bayesian network   representation  dependencies  differences   bayesian networks  directed  acyclic whereas markov networks  undirected  may  cyclic thus  markov network can represent certain dependencies   bayesian network    cyclic dependencies    hand  cant represent certain dependencies   bayesian network can   induced dependencies  underlying graph   markov random field may  finite  infinite   joint probability density   random variables  strictly positive   also referred    gibbs random field  according   hammersleyclifford theorem  can   represented   gibbs measure   appropriate locally defined energy function  prototypical markov random field   ising model indeed  markov random field  introduced   general setting   ising model   domain  artificial intelligence  markov random field  used  model various low  midlevel tasks  image processing  computer vision definition given  undirected graph formula  set  random variables formula indexed  formula form  markov random field  respect  formula   satisfy  local markov properties   three markov properties   equivalent  global markov property  stronger   local markov property   turn  stronger   pairwise one clique factorization   markov properties   arbitrary probability distribution can  difficult  establish  commonly used class  markov random fields    can  factorized according   cliques   graph given  set  random variables formula let formula   probability   particular field configuration formula  formula   formula   probability  finding   random variables formula take   particular value formula  formula   set  probability  formula   understood   taken  respect   joint distribution   formula   joint density can  factorized   cliques  formula  formula forms  markov random field  respect  formula  formula   set  cliques  formula  definition  equivalent   maximal cliques  used  functions  sometimes referred   factor potentials  clique potentials note however conflicting terminology   use  word potential  often applied   logarithm      statistical mechanics log   direct interpretation   potential energy   configuration formula although  mrfs   factorize  simple example can  constructed   cycle  nodes  certain cases  can  shown   equivalent given certain conditions    factorization  exist   possible  construct  factor graph   network logistic model  markov random field   strictly positive density can  written  loglinear model  feature functions formula    fulljoint distribution can  written    notation  simply  dot product  field configurations  z   partition function  formula denotes  set   possible assignments  values    networks random variables usually  feature functions formula  defined     indicators   cliques configuration ie formula  formula corresponds   ith possible configuration   kth clique  otherwise  model  equivalent   clique factorization model given   formula   cardinality   clique   weight   feature formula corresponds   logarithm   corresponding clique factor ie formula  formula   ith possible configuration   kth clique ie  ith value   domain   clique formula  probability p  often called  gibbs measure  expression   markov field   logistic model   possible   clique factors  nonzero ie  none   elements  formula  assigned  probability   allows techniques  matrix algebra   applied eg   trace   matrix  log   determinant   matrix representation   graph arising   graphs incidence matrix  importance   partition function z   many concepts  statistical mechanics   entropy directly generalize   case  markov networks   intuitive understanding can thereby  gained  addition  partition function allows variational methods   applied   solution   problem one can attach  driving force  one     random variables  explore  reaction   network  response   perturbation thus  example one may add  driving term j   vertex v   graph   partition function  get formally differentiating  respect  j gives  expectation value   random variable x associated   vertex v correlation functions  computed likewise  twopoint correlation  loglinear models  especially convenient   interpretation  loglinear model can provide  much  compact representation  many distributions especially  variables  large domains   convenient    negative log likelihoods  convex unfortunately though  likelihood   logistic markov network  convex evaluating  likelihood  gradient   likelihood   model requires inference   model   generally computationally infeasible see inference  examples gaussian  multivariate normal distribution forms  markov random field  respect   graph formula   missing edges correspond  zeros   precision matrix  inverse covariance matrix   inference    bayesian network one may calculate  conditional distribution   set  nodes formula given values  another set  nodes formula   markov random field  summing   possible assignments  formula   called exact inference however exact inference   pcomplete problem  thus computationally intractable   general case approximation techniques   markov chain monte carlo  loopy belief propagation  often  feasible  practice  particular subclasses  mrfs   trees see chowliu tree  polynomialtime inference algorithms discovering  subclasses   active research topic   also subclasses  mrfs  permit efficient map   likely assignment inference examples   include associative networks another interesting subclass   one  decomposable models   graph  chordal   closedform   mle   possible  discover  consistent structure  hundreds  variables conditional random fields one notable variant   markov random field   conditional random field    random variable may also  conditioned upon  set  global observations formula   model  function formula   mapping   assignments    clique k   observations formula   nonnegative real numbers  form   markov network may   appropriate  producing discriminative classifiers    model  distribution   observations crfs  proposed  john d lafferty andrew mccallum  fernando cn pereira  varied applications markov random fields find application   variety  fields ranging  computer graphics  computer vision  machine learning mrfs  used  image processing  generate textures   can  used  generate flexible  stochastic image models  image modelling  task   find  suitable intensity distribution   given image  suitability depends   kind  task  mrfs  flexible enough   used  image  texture synthesis image compression  restoration image segmentation surface reconstruction image registration texture synthesis superresolution stereo matching  information retrieval  can  used  solve various computer vision problems  can  posed  energy minimization problems  problems  different regions    distinguished using  set  discriminating features within  markov random field framework  predict  category   region markov random fields   generalization   ising model   since   used widely  combinatorial optimizations  networks\r\n"}
{"index":{"_id":114}}
{"conceptLabelTag":"adaptive resonance theory","conceptLabel":"adaptive resonance theory","conceptDescription":"adaptive resonance theory adaptive resonance theory art   theory developed  stephen grossberg  gail carpenter  aspects    brain processes information  describes  number  neural network models  use supervised  unsupervised learning methods  address problems   pattern recognition  prediction  primary intuition behind  art model   object identification  recognition generally occur   result   interaction  topdown observer expectations  bottomup sensory information  model postulates  topdown expectations take  form   memory template  prototype    compared   actual features   object  detected   senses  comparison gives rise   measure  category belongingness  long   difference  sensation  expectation   exceed  set threshold called  vigilance parameter  sensed object will  considered  member   expected class  system thus offers  solution   plasticitystability problem ie  problem  acquiring new knowledge without disrupting existing knowledge learning model  basic art system   unsupervised learning model  typically consists   comparison field   recognition field composed  neurons  vigilance parameter threshold  recognition   reset module  comparison field takes  input vector  onedimensional array  values  transfers    best match   recognition field  best match   single neuron whose set  weights weight vector  closely matches  input vector  recognition field neuron outputs  negative signal proportional   neurons quality  match   input vector      recognition field neurons  thus inhibits  output   way  recognition field exhibits lateral inhibition allowing  neuron    represent  category   input vectors  classified   input vector  classified  reset module compares  strength   recognition match   vigilance parameter   vigilance parameter  overcome ie  input vector  within  normal range seen  previous input vectors training commences  weights   winning recognition neuron  adjusted towards  features   input vector otherwise   match level    vigilance parameter ie  input vectors match  outside  normal expected range   neuron  winning recognition neuron  inhibited   search procedure  carried    search procedure recognition neurons  disabled one  one   reset function   vigilance parameter  overcome   recognition match  particular   cycle   search procedure   active recognition neuron  selected   switched    activation    vigilance parameter note   thus releases  remaining recognition neurons   inhibition   committed recognition neurons match overcomes  vigilance parameter   uncommitted neuron  committed   weights  adjusted towards matching  input vector  vigilance parameter  considerable influence   system higher vigilance produces highly detailed memories many finegrained categories  lower vigilance results   general memories fewer moregeneral categories training   two basic methods  training artbased neural networks slow  fast   slow learning method  degree  training   recognition neurons weights towards  input vector  calculated  continuous values  differential equations   thus dependent   length  time  input vector  presented  fast learning algebraic equations  used  calculate degree  weight adjustments   made  binary values  used  fast learning  effective  efficient   variety  tasks  slow learning method   biologically plausible  can  used  continuoustime networks ie   input vector can vary continuously types  art art   simplest variety  art networks accepting  binary inputs art extends network capabilities  support continuous inputs art    streamlined form  art   drastically accelerated runtime   qualitative results   rarely inferior   full art implementation art builds  art  simulating rudimentary neurotransmitter regulation  synaptic activity  incorporating simulated sodium na  calcium ca ion concentrations   systems equations  results    physiologically realistic means  partially inhibiting categories  trigger mismatch resets fuzzy art implements fuzzy logic  arts pattern recognition thus enhancing generalizability  optional   useful feature  fuzzy art  complement coding  means  incorporating  absence  features  pattern classifications  goes  long way towards preventing inefficient  unnecessary category proliferation artmap also known  predictive art combines two slightly modified art  art units   supervised learning structure   first unit takes  input data   second unit takes  correct output data  used  make  minimum possible adjustment   vigilance parameter   first unit  order  make  correct classification fuzzy artmap  merely artmap using fuzzy art units resulting   corresponding increase  efficacy criticism    noted  results  fuzzy art  art depend critically upon  order    training data  processed  effect can  reduced   extent  using  slower learning rate   present regardless   size   input data set hence fuzzy art  art estimates   possess  statistical property  consistency references wasserman philip d neural computing theory  practice new york van nostrand reinhold isbn \r\n"}
{"index":{"_id":115}}
{"conceptLabelTag":"anomaly detection","conceptLabel":"anomaly detection","conceptDescription":"anomaly detection  data mining anomaly detection also outlier detection   identification  items events  observations    conform   expected pattern   items   dataset typically  anomalous items will translate   kind  problem   bank fraud  structural defect medical problems  errors   text anomalies  also referred   outliers novelties noise deviations  exceptions  particular   context  abuse  network intrusion detection  interesting objects  often  rare objects  unexpected bursts  activity  pattern   adhere   common statistical definition   outlier   rare object  many outlier detection methods  particular unsupervised methods will fail   data unless    aggregated appropriately instead  cluster analysis algorithm may  able  detect  micro clusters formed   patterns three broad categories  anomaly detection techniques exist unsupervised anomaly detection techniques detect anomalies   unlabeled test data set   assumption   majority   instances   data set  normal  looking  instances  seem  fit least   remainder   data set supervised anomaly detection techniques require  data set    labeled  normal  abnormal  involves training  classifier  key difference  many  statistical classification problems   inherent unbalanced nature  outlier detection semisupervised anomaly detection techniques construct  model representing normal behavior   given normal training data set   testing  likelihood   test instance   generated   learnt model applications anomaly detection  applicable   variety  domains   intrusion detection fraud detection fault detection system health monitoring event detection  sensor networks  detecting ecosystem disturbances   often used  preprocessing  remove anomalous data   dataset  supervised learning removing  anomalous data   dataset often results   statistically significant increase  accuracy popular techniques several anomaly detection techniques   proposed  literature    popular techniques   performance  different methods depends  lot   data set  parameters  methods  little systematic advantages  another  compared across many data sets  parameters application  data security anomaly detection  proposed  intrusion detection systems ids  dorothy denning  anomaly detection  ids  normally accomplished  thresholds  statistics  can also  done  soft computing  inductive learning types  statistics proposed  included profiles  users workstations networks remote hosts groups  users  programs based  frequencies means variances covariances  standard deviations  counterpart  anomaly detection  intrusion detection  misuse detection\r\n"}
{"index":{"_id":116}}
{"conceptLabelTag":"optics","conceptLabel":"optics","conceptDescription":"optics algorithm ordering points  identify  clustering structure optics   algorithm  finding densitybased clusters  spatial data   presented  mihael ankerst markus m breunig hanspeter kriegel  j rg sander  basic idea  similar  dbscan   addresses one  dbscans major weaknesses  problem  detecting meaningful clusters  data  varying density  order     points   database  linearly ordered   points   spatially closest become neighbors   ordering additionally  special distance  stored   point  represents  density  needs   accepted   cluster  order    points belong    cluster   represented   dendrogram basic idea like dbscan optics requires two parameters formula  describes  maximum distance radius  consider  formula describing  number  points required  form  cluster  point formula   core point   least formula points  found within  formulaneighborhood formula contrary  dbscan optics also considers points   part    densely packed cluster   point  assigned  core distance  describes  distance   formulath closest point  reachabilitydistance  another point formula   point formula  either  distance  formula  formula   core distance  formula whichever  bigger  formula  formula  nearest neighbors    formula  need  assume  order   formula  formula belong    cluster   coredistance   reachabilitydistance  undefined   sufficiently dense cluster wrt formula  available given  sufficiently large formula  will never happen   every formulaneighborhood query will return  entire database resulting  formula runtime hence  formula parameter  required  cut   density  clusters    longer considered   interesting   speed   algorithm  way  parameter formula  strictly speaking  necessary  can simply  set   maximum possible value   spatial index  available however   play  practical role  regards  complexity optics abstracts  dbscan  removing  parameter  least   extent     give  maximum value pseudocode  basic approach  optics  similar  dbscan  instead  maintaining  set  known   far unprocessed cluster members  priority queue eg using  indexed heap  used  update  priority queue seeds  updated   formulaneighborhood  formula  formula respectively optics hence outputs  points   particular ordering annotated   smallest reachability distance   original algorithm  core distance  also exported     required   processing extracting  clusters using  reachabilityplot  special kind  dendrogram  hierarchical structure   clusters can  obtained easily    d plot   ordering   points  processed  optics   xaxis   reachability distance   yaxis since points belonging   cluster   low reachability distance   nearest neighbor  clusters show   valleys   reachability plot  deeper  valley  denser  cluster  image  illustrates  concept   upper left area  synthetic example data set  shown  upper right part visualizes  spanning tree produced  optics   lower part shows  reachability plot  computed  optics colors   plot  labels   computed   algorithm    well visible   valleys   plot correspond   clusters   data set  yellow points   image  considered noise   valley  found   reachability plot  will usually   assigned  clusters except  omnipresent  data cluster   hierarchical result extracting clusters   plot can  done manually  selecting  range   xaxis  visual inspection  selecting  threshold   yaxis  result will   similar   dbscan clustering result    formula  minpts parameters   value  may yield good results   different algorithms  try  detect  valleys  steepness knee detection  local maxima clusterings obtained  way usually  hierarchical    achieved   single dbscan run complexity like dbscan optics processes  point   performs one formulaneighborhood query   processing given  spatial index  grants  neighborhood query  formula runtime  overall runtime  formula  obtained  authors   original optics paper report  actual constant slowdown factor  compared  dbscan note   value  formula might heavily influence  cost   algorithm since  value  large might raise  cost   neighborhood query  linear complexity  particular choosing formula larger   maximum distance   data set  possible  will obviously lead  quadratic complexity since every neighborhood query will return  full data set even   spatial index  available  comes  additional cost  managing  heap therefore formula   chosen appropriately   data set extensions opticsof   outlier detection algorithm based  optics  main use   extraction  outliers   existing run  optics  low cost compared  using  different outlier detection method  better known version lof  based    concepts deliclu densitylinkclustering combines ideas  singlelinkage clustering  optics eliminating  formula parameter  offering performance improvements  optics hisc   hierarchical subspace clustering axisparallel method based  optics hico   hierarchical correlation clustering algorithm based  optics dish   improvement  hisc  can find  complex hierarchies foptics   faster implementation using random projections hdbscan  based   refinement  dbscan excluding borderpoints   clusters  thus following  strictly  basic definition  densitylevels  hartigan availability java implementations  optics opticsof deliclu hisc hico  dish  available   elki data mining framework  index acceleration  several distance functions   automatic cluster extraction using  extraction method  java implementations include spmf    weka extensions  support  cluster extraction  r package dbscan includes  c implementation  optics   traditional dbscanlike  cluster extraction using  kd tree  index acceleration  euclidean distance \r\n"}
{"index":{"_id":117}}
{"conceptLabelTag":"deep learning","conceptLabel":"deep learning","conceptDescription":"deep learning deep learning also known  deep structured learning hierarchical learning  deep machine learning   branch  machine learning based   set  algorithms  attempt  model high level abstractions  data   simple case  might  two sets  neurons ones  receive  input signal  ones  send  output signal   input layer receives  input  passes   modified version   input   next layer   deep network   many layers   input  output   layers   made  neurons   can help  think    way allowing  algorithm  use multiple processing layers composed  multiple linear  nonlinear transformations deep learning  part   broader family  machine learning methods based  learning representations  data  observation eg  image can  represented  many ways    vector  intensity values per pixel     abstract way   set  edges regions  particular shape etc  representations  better  others  simplifying  learning task eg face recognition  facial expression recognition one   promises  deep learning  replacing handcrafted features  efficient algorithms  unsupervised  semisupervised feature learning  hierarchical feature extraction research   area attempts  make better representations  create models  learn  representations  largescale unlabeled data    representations  inspired  advances  neuroscience   loosely based  interpretation  information processing  communication patterns   nervous system   neural coding  attempts  define  relationship  various stimuli  associated neuronal responses   brain various deep learning architectures   deep neural networks convolutional deep neural networks deep belief networks  recurrent neural networks   applied  fields like computer vision automatic speech recognition natural language processing audio recognition  bioinformatics     shown  produce stateoftheart results  various tasks deep learning   characterized   buzzword   rebranding  neural networks introduction definitions deep learning  characterized   class  machine learning algorithms   definitions   common multiple layers  nonlinear processing units   supervised  unsupervised learning  feature representations   layer   layers forming  hierarchy  lowlevel  highlevel features  composition   layer  nonlinear processing units used   deep learning algorithm depends   problem   solved layers    used  deep learning include hidden layers   artificial neural network  sets  complicated propositional formulas  may also include latent variables organized layerwise  deep generative models    nodes  deep belief networks  deep boltzmann machines deep learning algorithms transform  inputs   layers  shallow learning algorithms   layer  signal  transformed   processing unit like  artificial neuron whose parameters  learned  training  chain  transformations  input  output   credit assignment path cap caps describe potentially causal connections  input  output  may vary  length   feedforward neural network  depth   caps thus   network   number  hidden layers plus one   output layer  also parameterized   recurrent neural networks    signal may propagate   layer     cap  potentially unlimited  length    universally agreed upon threshold  depth dividing shallow learning  deep learning   researchers   field agree  deep learning  multiple nonlinear layers cap  juergen schmidhuber considers cap    deep learning fundamental concepts deep learning algorithms  based  distributed representations  underlying assumption behind distributed representations   observed data  generated   interactions  factors organized  layers deep learning adds  assumption   layers  factors correspond  levels  abstraction  composition varying numbers  layers  layer sizes can  used  provide different amounts  abstraction deep learning exploits  idea  hierarchical explanatory factors  higher level  abstract concepts  learned   lower level ones  architectures  often constructed   greedy layerbylayer method deep learning helps  disentangle  abstractions  pick   features  useful  learning  supervised learning tasks deep learning methods obviate feature engineering  translating  data  compact intermediate representations akin  principal components  derive layered structures  remove redundancy  representation many deep learning algorithms  applied  unsupervised learning tasks    important benefit  unlabeled data  usually  abundant  labeled data examples  deep structures  can  trained   unsupervised manner  neural history compressors  deep belief networks interpretations deep neural networks  generally interpreted  terms  universal approximation theorem  probabilistic inference universal approximation theorem interpretation  universal approximation theorem concerns  capacity  feedforward neural networks   single hidden layer  finite size  approximate continuous functions   first proof  published  george cybenko  sigmoid activation functions   generalised  feedforward multilayer architectures   kurt hornik probabilistic interpretation  probabilistic interpretation derives   field  machine learning  features inference  well   optimization concepts  training  testing related  fitting  generalization respectively  specifically  probabilistic interpretation considers  activation nonlinearity   cumulative distribution function see deep belief network  probabilistic interpretation led   introduction  dropout  regularizer  neural networks  probabilistic interpretation  introduced  popularized  geoff hinton yoshua bengio yann lecun  juergen schmidhuber history  first general working learning algorithm  supervised deep feedforward multilayer perceptrons  published  ivakhnenko  lapa   paper described  deep network  layers trained   group method  data handling algorithm   still popular   current millennium  ideas  implemented   computer identification system alpha  demonstrated  learning process  deep learning working architectures specifically  built  artificial neural networks ann date back   neocognitron introduced  kunihiko fukushima   anns  date back even   challenge    train networks  multiple layers  yann lecun et al  able  apply  standard backpropagation algorithm    around   reverse mode  automatic differentiation since   deep neural network   purpose  recognizing handwritten zip codes  mail despite  success  applying  algorithm  time  train  network   dataset  approximately days making  impractical  general use  j rgen schmidhubers neural history compressor implemented   unsupervised stack  recurrent neural networks rnns solved   deep learning task  requires   subsequent layers   rnn unfolded  time  andre c p l f de carvalho together  mike c fairhurst  david bisset published  work  experimental results   several layers boolean neural network also known  weightless neural network composed  two modules  selforganising feature extraction network module followed   classification module   independently trained  brendan frey demonstrated    possible  train  network containing six fully connected layers  several hundred hidden units using  wakesleep algorithm   codeveloped  peter dayan  geoffrey hinton however training took two days many factors contribute   slow speed one   vanishing gradient problem analyzed   sepp hochreiter    neural networks  used  recognizing isolated d handwritten digits recognizing d objects  done  matching d images   handcrafted d object model juyang weng et al suggested   human brain   use  monolithic d object model    published cresceptron  method  performing d object recognition directly  cluttered scenes cresceptron   cascade  layers similar  neocognitron   neocognitron required  human programmer  handmerge features cresceptron automatically learned  open number  unsupervised features   layer   feature  represented   convolution kernel cresceptron also segmented  learned object   cluttered scene  backanalysis   network max pooling now often adopted  deep neural networks eg imagenet tests  first used  cresceptron  reduce  position resolution   factor  x    cascade  better generalization despite  advantages simpler models  use taskspecific handcrafted features   gabor filters  support vector machines svms   popular choice   s  s    computational cost  anns   time   great lack  understanding    brain autonomously wires  biological networks   long history  speech recognition  shallow  deep learning eg recurrent nets  artificial neural networks   explored  many years   methods never won   nonuniform internalhandcrafting gaussian mixture modelhidden markov model gmmhmm technology based  generative models  speech trained discriminatively  number  key difficulties   methodologically analyzed including gradient diminishing  weak temporal correlation structure   neural predictive models additional difficulties   lack  big training data  weaker computing power   early days thus  speech recognition researchers  understood  barriers moved away  neural nets  pursue generative modeling  exception   sri international   late s funded   us governments nsa  darpa sri conducted research  deep neural networks  speech  speaker recognition  speaker recognition team led  larry heck achieved  first significant success  deep neural networks  speech processing  demonstrated   nist national institute  standards  technology speaker recognition evaluation  later published   journal  speech communication  sri established success  deep neural networks  speaker recognition   unsuccessful  demonstrating similar success  speech recognition hinton et al  deng et al reviewed part   recent history    collaboration       colleagues across four groups university  toronto microsoft google  ibm ignited  renaissance  deep feedforward neural networks  speech recognition today however many aspects  speech recognition   taken    deep learning method called long shortterm memory lstm  recurrent neural network published  sepp hochreiter j rgen schmidhuber  lstm rnns avoid  vanishing gradient problem  can learn  deep learning tasks  require memories  events  happened thousands  discrete time steps ago   important  speech  lstm started  become competitive  traditional speech recognizers  certain tasks later   combined  ctc  stacks  lstm rnns  googles speech recognition reportedly experienced  dramatic performance    ctctrained lstm   now available  google voice search   smartphone users   become  show case  deep learning  use   expression deep learning   context  artificial neural networks  introduced  igor aizenberg  colleagues   google ngram chart shows   usage   term  gained traction actually  taken  since   publication  geoffrey hinton  ruslan salakhutdinov drew additional attention  showing  manylayered feedforward neural network   effectively pretrained one layer   time treating  layer  turn   unsupervised restricted boltzmann machine  finetuning  using supervised backpropagation  schmidhuber  already implemented   similar idea    general case  unsupervised deep hierarchies  recurrent neural networks  also experimentally shown  benefits  speeding  supervised learning since  resurgence deep learning  become part  many stateoftheart systems  various disciplines particularly computer vision  automatic speech recognition asr results  commonly used evaluation sets   timit asr  mnist image classification  well   range  largevocabulary speech recognition tasks  constantly  improved  new applications  deep learning recently   shown  deep learning architectures   form  convolutional neural networks   nearly best performing however    widely used  computer vision   asr  modern large scale speech recognition  typically based  ctc  lstm  real impact  deep learning  industry apparently began   early s  cnns already processed  estimated     checks written   us   early s according  yann lecun industrial applications  deep learning  largescale speech recognition started around  late li deng invited geoffrey hinton  work    colleagues  microsoft research  redmond washington  apply deep learning  speech recognition  coorganized  nips workshop  deep learning  speech recognition  workshop  motivated   limitations  deep generative models  speech   possibility   bigcompute bigdata era warranted  serious try  deep neural nets dnn   believed  pretraining dnns using generative models  deep belief nets dbn  overcome  main difficulties  neural nets encountered   s however early   research  microsoft   discovered  without pretraining  using large amounts  training data  especially dnns designed  corresponding large contextdependent output layers produced error rates dramatically lower  thenstateoftheart gmmhmm  also   advanced generative modelbased speech recognition systems  finding  verified  several  major speech recognition research groups   nature  recognition errors produced   two types  systems  found   characteristically different offering technical insights    integrate deep learning   existing highly efficient runtime speech decoding system deployed   major players  speech recognition industry  history   significant development  deep learning   described  analyzed  recent books  articles advances  hardware  also  important  enabling  renewed interest  deep learning  particular powerful graphics processing units gpus  wellsuited   kind  number crunching matrixvector math involved  machine learning gpus   shown  speed  training algorithms  orders  magnitude bringing running times  weeks back  days artificial neural networks     successful deep learning methods involve artificial neural networks artificial neural networks  inspired   biological model proposed  nobel laureates david h hubel torsten wiesel  found two types  cells   primary visual cortex simple cells  complex cells many artificial neural networks can  viewed  cascading models  cell types inspired   biological observations fukushimas neocognitron introduced convolutional neural networks partially trained  unsupervised learning  humandirected features   neural plane yann lecun et al applied supervised backpropagation   architectures weng et al published convolutional neural networks cresceptron  d object recognition  images  cluttered scenes  segmentation   objects  images  obvious need  recognizing general d objects  least shift invariance  tolerance  deformation maxpooling appeared   first proposed  cresceptron  enable  network  tolerate smalltolarge deformation   hierarchical way  using convolution maxpooling helps    guarantee shiftinvariance   pixel level   advent   backpropagation algorithm based  automatic differentiation many researchers tried  train supervised deep artificial neural networks  scratch initially  little success sepp hochreiters diploma thesis  formally identified  reason   failure   vanishing gradient problem  affects manylayered feedforward networks  recurrent neural networks recurrent networks  trained  unfolding    deep feedforward networks   new layer  created   time step   input sequence processed   network  errors propagate  layer  layer  shrink exponentially   number  layers impeding  tuning  neuron weights   based   errors  overcome  problem several methods  proposed one  j rgen schmidhubers multilevel hierarchy  networks pretrained one level   time  unsupervised learning finetuned  backpropagation   level learns  compressed representation   observations   fed   next level another method   long shortterm memory lstm network  hochreiter schmidhuber  deep multidimensional lstm networks won three icdar competitions  connected handwriting recognition without  prior knowledge   three languages   learned sven behnke  relied    sign   gradient rprop  training  neural abstraction pyramid  solve problems like image reconstruction  face localization  methods also use unsupervised pretraining  structure  neural network making  first learn generally useful feature detectors   network  trained   supervised backpropagation  classify labeled data  deep model  hinton et al involves learning  distribution   highlevel representation using successive layers  binary  realvalued latent variables  uses  restricted boltzmann machine smolensky  model  new layer  higher level features  new layer guarantees  increase   lowerbound   log likelihood   data thus improving  model  trained properly  sufficiently many layers   learned  deep architecture may  used   generative model  reproducing  data  sampling   model  ancestral pass   top level feature activations hinton reports   models  effective feature extractors  highdimensional structured data   google brain team led  andrew ng  jeff dean created  neural network  learned  recognize higherlevel concepts   cats   watching unlabeled images taken  youtube videos  methods rely   sheer processing power  modern computers  particular gpus  dan ciresan  colleagues  j rgen schmidhubers group   swiss ai lab idsia showed  despite  abovementioned vanishing gradient problem  superior processing power  gpus makes plain backpropagation feasible  deep feedforward neural networks  many layers  method outperformed   machine learning techniques   old famous mnist handwritten digits problem  yann lecun  colleagues  nyu     time  late deep learning feedforward networks made inroads  speech recognition  marked   nips workshop  deep learning  speech recognition intensive collaborative work  microsoft research  university  toronto researchers demonstrated  mid  redmond  deep neural networks interfaced   hidden markov model  contextdependent states  define  neural network output layer can drastically reduce errors  largevocabulary speech recognition tasks   voice search   deep neural net model  shown  scale   switchboard tasks  one year later  microsoft research asia even earlier  lstm trained  ctc started  get excellent results  certain applications  method  now widely used  example  googles greatly improved speech recognition   smartphone users    state   art  deep learning feedforward networks alternates convolutional layers  maxpooling layers topped  several fully connected  sparsely connected layer followed   final classification layer training  usually done without  unsupervised pretraining since gpubased implementations   approach won many pattern recognition contests including  ijcnn traffic sign recognition competition  isbi segmentation  neuronal structures  em stacks challenge  imagenet competition  others  supervised deep learning methods also   first artificial pattern recognizers  achieve humancompetitive performance  certain tasks  overcome  barriers  weak ai represented  deep learning   necessary  go beyond deep learning architectures  biological brains use  shallow  deep circuits  reported  brain anatomy displaying  wide variety  invariance weng argued   brain selfwires largely according  signal statistics  therefore  serial cascade  catch  major statistical dependencies anns  able  guarantee shift invariance  deal  small  large natural objects  large cluttered scenes   invariance extended beyond shift   annlearned concepts   location type object class label scale lighting   realized  developmental networks dns whose embodiments  wherewhat networks wwn  wwn deep neural network architectures   huge number  variants  deep architectures     branched   original parent architectures    always possible  compare  performance  multiple architectures  together      evaluated    data sets deep learning   fastgrowing field  new architectures variants  algorithms appear every  weeks brief discussion  deep neural networks  deep neural network dnn   artificial neural network ann  multiple hidden layers  units   input  output layers similar  shallow anns dnns can model complex nonlinear relationships dnn architectures eg  object detection  parsing generate compositional models   object  expressed   layered composition  image primitives  extra layers enable composition  features  lower layers giving  potential  modeling complex data  fewer units   similarly performing shallow network dnns  typically designed  feedforward networks  research   successfully applied recurrent neural networks especially lstm  applications   language modeling convolutional deep neural networks cnns  used  computer vision   success  welldocumented cnns also   applied  acoustic modeling  automatic speech recognition asr    shown success  previous models  simplicity  look  training dnns  given  backpropagation  dnn can  discriminatively trained   standard backpropagation algorithm according  various sources basics  continuous backpropagation  derived   context  control theory  henry j kelley    arthur e bryson  using principles  dynamic programming  stuart dreyfus published  simpler derivation based    chain rule vapnik cites reference   book  support vector machines arthur e bryson  yuchi ho described    multistage dynamic system optimization method   seppo linnainmaa finally published  general method  automatic differentiation ad  discrete connected networks  nested differentiable functions  corresponds   modern version  backpropagation   efficient even   networks  sparse  stuart dreyfus used backpropagation  adapt parameters  controllers  proportion  error gradients  paul werbos mentioned  possibility  applying  principle  artificial neural networks    applied linnainmaas ad method  neural networks   way   widely used today  david e rumelhart geoffrey e hinton  ronald j williams showed  computer experiments   method can generate useful internal representations  incoming data  hidden layers  neural networks  eric  wan   first  win  international pattern recognition contest  backpropagation  weight updates  backpropagation can  done via stochastic gradient descent using  following equation  formula   learning rate formula   cost function  formula  stochastic term  choice   cost function depends  factors    learning type supervised unsupervised reinforcement etc   activation function  example  performing supervised learning   multiclass classification problem common choices   activation function  cost function   softmax function  cross entropy function respectively  softmax function  defined  formula  formula represents  class probability output   unit formula  formula  formula represent  total input  units formula  formula    level respectively cross entropy  defined  formula  formula represents  target probability  output unit formula  formula   probability output  formula  applying  activation function  can  used  output object bounding boxes   form   binary mask   also used  multiscale regression  increase localization precision dnnbased regression can learn features  capture geometric information  addition    good classifier  remove  limitation  designing  model  will capture parts   relations explicitly  helps  learn  wide variety  objects  model consists  multiple layers      rectified linear unit  nonlinear transformation  layers  convolutional  others  fully connected every convolutional layer   additional max pooling  network  trained  minimize l error  predicting  mask ranging   entire training set containing bounding boxes represented  masks problems  deep neural networks   anns many issues can arise  dnns    naively trained two common issues  overfitting  computation time dnns  prone  overfitting    added layers  abstraction  allow   model rare dependencies   training data regularization methods   ivakhnenkos unit pruning  weight decay formularegularization  sparsity formularegularization can  applied  training  help combat overfitting   recent regularization method applied  dnns  dropout regularization  dropout  number  units  randomly omitted   hidden layers  training  helps  break  rare dependencies  can occur   training data  dominant method  training  structures   errorcorrection training   backpropagation  gradient descent due   ease  implementation   tendency  converge  better local optima   training methods however  methods can  computationally expensive especially  dnns   many training parameters   considered   dnn    size number  layers  number  units per layer  learning rate  initial weights sweeping   parameter space  optimal parameters may   feasible due   cost  time  computational resources various tricks   using minibatching computing  gradient  several training examples   rather  individual examples   shown  speed  computation  large processing throughput  gpus  produced significant speedups  training due   matrix  vector computations required  well suited  gpus radical alternatives  backprop   extreme learning machines noprop networks training without backtracking weightless networks  nonconnectionist neural networks  gaining attention first deep learning networks  gmdh according   historic survey  first functional deep learning networks  many layers  published  alexey grigorevich ivakhnenko  v g lapa   learning algorithm  called  group method  data handling  gmdh gmdh features fully automatic structural  parametric optimization  models  activation functions   network nodes  kolmogorovgabor polynomials  permit additions  multiplications ivakhnenkos paper describes  learning   deep feedforward multilayer perceptron  eight layers already much deeper  many later networks  supervised learning network  grown layer  layer   layer  trained  regression analysis  time  time useless neurons  detected using  validation set  pruned  regularization  size  depth   resulting network depends   problem variants   method  still  used today convolutional neural networks cnns  become  method  choice  processing visual   twodimensional data  cnn  composed  one   convolutional layers  fully connected layers matching   typical artificial neural networks  top  also uses tied weights  pooling layers  particular maxpooling  often used  fukushimas convolutional architecture  architecture allows cnns  take advantage   d structure  input data  comparison   deep architectures convolutional neural networks  shown superior results   image  speech applications  can also  trained  standard backpropagation cnns  easier  train   regular deep feedforward neural networks   many fewer parameters  estimate making   highly attractive architecture  use examples  applications  computer vision include deepdream see  main article  convolutional neural networks  numerous additional references neural history compressor  vanishing gradient problem  automatic differentiation  backpropagation  neural networks  partially overcome    early generative model called  neural history compressor implemented   unsupervised stack  recurrent neural networks rnns  rnn   input level learns  predict  next input   previous input history  unpredictable inputs   rnn   hierarchy become inputs   next higher level rnn  therefore recomputes  internal state  rarely  higher level rnn thus learns  compressed representation   information   rnn    done    input sequence can  precisely reconstructed   sequence representation   highest level  system effectively minimises  description length   negative logarithm   probability   data     lot  learnable predictability   incoming data sequence   highest level rnn can use supervised learning  easily classify even deep sequences   long time intervals  important events    system already solved   deep learning task  requires   subsequent layers   rnn unfolded  time   also possible  distill  entire rnn hierarchy   two rnns called  conscious chunker higher level   subconscious automatizer lower level   chunker  learned  predict  compress inputs   still unpredictable   automatizer  automatizer  forced   next learning phase  predict  imitate  special additional units  hidden units    slowly changing chunker  makes  easy   automatizer  learn appropriate rarely changing memories across  long time intervals   turn helps  automatizer  make many    unpredictable inputs predictable    chunker can focus   remaining still unpredictable events  compress  data even  recursive neural networks  recursive neural network  created  applying   set  weights recursively   differentiable graphlike structure  traversing  structure  topological order  networks  typically also trained   reverse mode  automatic differentiation   introduced  learn distributed representations  structure   logical terms  special case  recursive neural networks   rnn  whose structure corresponds   linear chain recursive neural networks   applied  natural language processing  recursive neural tensor network uses  tensorbased composition function   nodes   tree long shortterm memory numerous researchers now use variants   deep learning rnn called  long shortterm memory lstm network published  hochreiter schmidhuber     system  unlike traditional rnns doesnt   vanishing gradient problem lstm  normally augmented  recurrent gates called forget gates lstm rnns prevent backpropagated errors  vanishing  exploding instead errors can flow backwards  unlimited numbers  virtual layers  lstm rnns unfolded  space   lstm can learn  deep learning tasks  require memories  events  happened thousands  even millions  discrete time steps ago problemspecific lstmlike topologies can  evolved lstm works even    long delays   can handle signals    mix  low  high frequency components today many applications use stacks  lstm rnns  train   connectionist temporal classification ctc  find  rnn weight matrix  maximizes  probability   label sequences   training set given  corresponding input sequences ctc achieves  alignment  recognition  ctctrained lstm   first rnn  win pattern recognition contests   won several competitions  connected handwriting recognition already  lstm started  become competitive  traditional speech recognizers  certain tasks   combination  ctc achieved first good results  speech data since   approach  revolutionised speech recognition   chinese search giant baidu used ctctrained rnns  break  switchboard hub speech recognition benchmark without using  traditional speech processing methods lstm also improved largevocabulary speech recognition texttospeech synthesis also  google android  photoreal talking heads  googles speech recognition reportedly experienced  dramatic performance    ctctrained lstm   now available  google voice  billions  smartphone users lstm  also become  popular   field  natural language processing unlike previous models based  hmms  similar concepts lstm can learn  recognise contextsensitive languages lstm improved machine translation language modeling  multilingual language processing lstm combined  convolutional neural networks cnns also improved automatic image captioning   plethora   applications deep belief networks  deep belief network dbn   probabilistic generative model made   multiple layers  hidden units  can  considered  composition  simple learning modules  make   layer  dbn can  used  generatively pretrain  dnn  using  learned dbn weights   initial dnn weights backpropagation   discriminative algorithms can   applied  finetuning   weights   particularly helpful  limited training data  available  poorly initialized weights can significantly hinder  learned models performance  pretrained weights    region   weight space   closer   optimal weights   randomly chosen initial weights  allows   improved modeling  faster convergence   finetuning phase  dbn can  efficiently trained   unsupervised layerbylayer manner   layers  typically made  restricted boltzmann machines rbm  rbm   undirected generative energybased model   visible input layer   hidden layer  connections   layers   within layers  training method  rbms proposed  geoffrey hinton  use  training product  expert models  called contrastive divergence cd cd provides  approximation   maximum likelihood method   ideally  applied  learning  weights   rbm  training  single rbm weight updates  performed  gradient ascent via  following equation formula  formula   probability   visible vector   given  formula formula   partition function used  normalizing  formula   energy function assigned   state   network  lower energy indicates  network     desirable configuration  gradient formula   simple form formula  formula represent averages  respect  distribution formula  issue arises  sampling formula   requires running alternating gibbs sampling   long time cd replaces  step  running alternating gibbs sampling  formula steps values  formula  empirically  shown  perform well  formula steps  data  sampled   sample  used  place  formula  cd procedure works  follows   rbm  trained another rbm  stacked atop  taking  input   final alreadytrained layer  new visible layer  initialized   training vector  values   units   alreadytrained layers  assigned using  current weights  biases  new rbm   trained   procedure   whole process  repeated   desired stopping criterion  met although  approximation  cd  maximum likelihood   crude   shown   follow  gradient   function    empirically shown   effective  training deep architectures convolutional deep belief networks  recent achievement  deep learning   use  convolutional deep belief networks cdbn cdbns  structure  similar   convolutional neural networks   trained similar  deep belief networks therefore  exploit  d structure  images like cnns   make use  pretraining like deep belief networks  provide  generic structure  can  used  many image  signal processing tasks recently many benchmark results  standard image datasets like cifar   obtained using cdbns large memory storage  retrieval neural networks large memory storage  retrieval neural networks lamstar  fast deep learning neural networks  many layers  can use many filters simultaneously  filters may  nonlinear stochastic logic nonstationary  even nonanalytical   biologically motivated  continuously learning  lamstar neural network may serve   dynamic neural network  spatial  time domain    speed  provided  hebbian linkweights chapter   d graupe  serve  integrate  various  usually different filters preprocessing functions   many layers   dynamically rank  significance   various layers  functions relative   given task  deep learning  grossly imitates biological learning  integrates outputs various preprocessors cochlea retina etc  cortexes auditory visual etc   various regions  deep learning capability   enhanced  using inhibition correlation    ability  cope  incomplete data  lost neurons  layers even   midst   task furthermore   fully transparent due   link weights  linkweights also allow dynamic determination  innovation  redundancy  facilitate  ranking  layers  filters   individual neurons relative   task lamstar   applied  many medical  financial predictions see graupe section c adaptive filtering  noisy speech  unknown noise stillimage recognition graupe section d video image recognition software security adaptive control  nonlinear systems  others lamstar   much faster computing speed  somewhat lower error   convolutional neural network based  relufunction filters  max pooling   comparative character recognition study  applications demonstrate delving  aspects   data   hidden  shallow learning networks  even   human senses eye ear     cases  predicting onset  sleep apnea events   electrocardiogram   fetus  recorded  skinsurface electrodes placed   mothers abdomen early  pregnancy  financial prediction section c  graupe   blind filtering  noisy speech lamstar  proposed     developed  d graupe  h kordylewski  modified version known  lamstar  developed  n c schneider  d graupe  deep boltzmann machines  deep boltzmann machine dbm   type  binary pairwise markov random field undirected probabilistic graphical model  multiple layers  hidden random variables    network  symmetrically coupled stochastic binary units  comprises  set  visible units formula   series  layers  hidden units formula    connection  units    layer like rbm    probability assigned  vector   formula   set  hidden units  formula   model parameters representing visiblehidden  hiddenhidden interactions  formula  formula  network   wellknown restricted boltzmann machine interactions  symmetric  links  undirected  contrast   deep belief network dbn   top two layers form  restricted boltzmann machine    undirected graphical model  lower layers form  directed generative model like dbns dbms can learn complex  abstract internal representations   input  tasks   object  speech recognition using limited labeled data  finetune  representations built using  large supply  unlabeled sensory input data however unlike  deep convolutional neural networks  adopt  inference  training procedure   directions bottomup  topdown pass  allow   better unveil  representations   ambiguous  complex input structures however  speed  dbms limits  performance  functionality  exact maximum likelihood learning  intractable  dbms  may perform approximate maximum likelihood learning another option   use meanfield inference  estimate datadependent expectations  approximation  expected sufficient statistics   model  using markov chain monte carlo mcmc  approximate inference  must  done   test input    times slower   single bottomup pass  dbms  makes  joint optimization impractical  large data sets  seriously restricts  use  dbms  tasks   feature representation stacked denoising autoencoders  auto encoder idea  motivated   concept   good representation  example   classifier  good representation can  defined  one  will yield  better performing classifier  encoder   deterministic mapping formula  transforms  input vector x  hidden representation y  formula formula   weight matrix  b   offset vector bias  decoder maps back  hidden representation y   reconstructed input z via formula  whole process  auto encoding   compare  reconstructed input   original  try  minimize  error  make  reconstructed value  close  possible   original  stacked denoising auto encoders  partially corrupted output  cleaned denoised  idea  introduced   vincent et al   specific approach  good representation  good representation  one  can  obtained robustly   corrupted input   will  useful  recovering  corresponding clean input implicit   definition   following ideas  algorithm consists  multiple steps starts   stochastic mapping  formula  formula  formula    corrupting step   corrupted input formula passes   basic auto encoder process   mapped   hidden representation formula   hidden representation  can reconstruct formula   last stage  minimization algorithm runs  order   z  close  possible  uncorrupted input formula  reconstruction error formula might  either  crossentropy loss   affinesigmoid decoder   squared error loss   affine decoder  order  make  deep architecture auto encoders stack one  top  another   encoding function formula   first denoising auto encoder  learned  used  uncorrupt  input corrupted input  can train  second level   stacked auto encoder  trained  output can  used   input   supervised learning algorithm   support vector machine classifier   multiclass logistic regression deep stacking networks one deep architecture based   hierarchy  blocks  simplified neural network modules   deep convex network introduced    weights learning problem  formulated   convex optimization problem   closedform solution  architecture  also called  deep stacking network dsn emphasizing  mechanisms similarity  stacked generalization  dsn block   simple module   easy  train     supervised fashion without backpropagation   entire blocks  designed  deng  dong  block consists   simplified multilayer perceptron mlp   single hidden layer  hidden layer h  logistic sigmoidal units   output layer  linear units connections   layers  represented  weight matrix u inputtohiddenlayer connections  weight matrix w target vectors t form  columns  matrix t   input data vectors x form  columns  matrix x  matrix  hidden units  formula modules  trained  order  lowerlayer weights w  known   stage  function performs  elementwise logistic sigmoid operation  block estimates   final label class y   estimate  concatenated  original input x  form  expanded input   next block thus  input   first block contains  original data   downstream blocks input also   output  preceding blocks  learning  upperlayer weight matrix u given  weights   network can  formulated   convex optimization problem    closedform solution unlike  deep architectures   dbns  goal    discover  transformed feature representation  structure   hierarchy   kind  architecture makes parallel learning straightforward   batchmode optimization problem  purely discriminative tasks dsns perform better  conventional dbn tensor deep stacking networks  architecture   extension  deep stacking networks dsn  improves   two important ways  uses higherorder information  covariance statistics   transforms  nonconvex problem   lowerlayer   convex subproblem   upperlayer tdsns use covariance statistics   data  using  bilinear mapping    two distinct sets  hidden units    layer  predictions via  thirdorder tensor  parallelization  scalability   considered seriously  conventional  learning  s  s  done  batch mode  allow parallelization   cluster  cpu  gpu nodes parallelization allows scaling  design  larger deeper architectures  data sets  basic architecture  suitable  diverse tasks   classification  regression spikeandslab rbms  need  deep learning  realvalued inputs   gaussian restricted boltzmann machines motivates  spikeandslab rbm ssrbms  models continuousvalued inputs  strictly binary latent variables similar  basic rbms   variants  spikeandslab rbm   bipartite graph  like grbms  visible units input  realvalued  difference    hidden layer   hidden unit   binary spike variable   realvalued slab variable  spike   discrete probability mass  zero   slab   density  continuous domain  mixture forms  prior  terms come   statistics literature  extension  ssrbm called ssrbm provides extra modeling capacity using additional terms   energy function one   terms enables  model  form  conditional distribution   spike variables  marginalizing   slab variables given  observation compound hierarchicaldeep models compound hierarchicaldeep models compose deep networks  nonparametric bayesian models features can  learned using deep architectures   dbns dbms deep auto encoders convolutional variants ssrbms deep coding networks dbns  sparse feature learning recursive neural networks conditional dbns denoising auto encoders  provides  better representation allowing faster learning   accurate classification  highdimensional data however  architectures  poor  learning novel classes   examples   network units  involved  representing  input  distributed representation  must  adjusted together high degree  dom limiting  degree  dom reduces  number  parameters  learn facilitating learning  new classes   examples hierarchical bayesian hb models allow learning   examples  example  computer vision statistics  cognitive science compound hd architectures aim  integrate characteristics   hb  deep networks  compound hdpdbm architecture  hierarchical dirichlet process hdp   hierarchical model incorporated  dbm architecture    full generative model generalized  abstract concepts flowing   layers   model   able  synthesize new examples  novel classes  look reasonably natural   levels  learned jointly  maximizing  joint logprobability score   dbm  three hidden layers  probability   visible input   formula   set  hidden units  formula   model parameters representing visiblehidden  hiddenhidden symmetric interaction terms   dbm model  learned    undirected model  defines  joint distribution formula one way  express    learned   conditional model formula   prior term formula  formula represents  conditional dbm model  can  viewed   twolayer dbm   bias terms given   states  formula deep coding networks   advantages   model  can actively update    context  data deep coding network dpcn   predictive coding scheme  topdown information  used  empirically adjust  priors needed   bottomup inference procedure  means   deep locally connected generative model  works  extracting sparse features  timevarying observations using  linear dynamical model   pooling strategy  used  learn invariant feature representations  units compose  form  deep architecture   trained  greedy layerwise unsupervised learning  layers constitute  kind  markov chain    states   layer  depend   preceding  succeeding layers deep predictive coding network dpcn predicts  representation   layer  using  topdown approach using  information  upper layer  temporal dependencies   previous states dpcns can  extended  form  convolutional network deep qnetworks  deep qnetwork dqn   type  deep learning model developed  google deepmind  combines  deep convolutional neural network  qlearning  form  reinforcement learning unlike earlier reinforcement learning agents dqns can learn directly  highdimensional sensory inputs preliminary results  presented    paper published  february  nature  application discussed   paper  limited  atari gaming although   implications   applications however much   work     number  reinforcement learning models  apply deep learning approaches eg networks  separate memory structures integrating external memory  artificial neural networks dates  early research  distributed representations  teuvo kohonens selforganizing maps  example  sparse distributed memory  hierarchical temporal memory  patterns encoded  neural networks  used  addresses  contentaddressable memory  neurons essentially serving  address encoders  decoders however  early controllers   memories   differentiable lstmrelated differentiable memory structures apart  long shortterm memory lstm  approaches   s  s also added differentiable memory  recurrent functions  example semantic hashing approaches  represent previous experiences directly  use  similar experience  form  local model  often called nearest neighbour  knearest neighbors methods  recently deep learning  shown   useful  semantic hashing   deep graphical model  wordcount vectors obtained   large set  documents documents  mapped  memory addresses    way  semantically similar documents  located  nearby addresses documents similar   query document can   found  simply accessing   addresses  differ     bits   address   query document unlike sparse distributed memory  operates  bit addresses semantic hashing works   bit addresses found   conventional computer architecture neural turing machines neural turing machines developed  google deepmind couple lstm networks  external memory resources   can interact   attentional processes  combined system  analogous   turing machine   differentiable endtoend allowing    efficiently trained  gradient descent preliminary results demonstrate  neural turing machines can infer simple algorithms   copying sorting  associative recall  input  output examples memory networks memory networks  another extension  neural networks incorporating longterm memory   developed   facebook research team  longterm memory can  read  written    goal  using   prediction  models   applied   context  question answering qa   longterm memory effectively acts   dynamic knowledge base   output   textual response pointer networks deep neural networks can  potentially improved   get deeper   fewer parameters  maintaining trainability  training extremely deep eg millionlayerdeep neural networks might   practically feasible cpulike architectures   pointer networks  neural randomaccess machines developed  google brain researchers overcome  limitation  using external randomaccess memory  well  adding  components  typically belong   computer architecture   registers alu  pointers  systems operate  probability distribution vectors stored  memory cells  registers thus  model  fully differentiable  trains endtoend  key characteristic   models    depth  size   shortterm memory   number  parameters can  altered independently unlike models like long shortterm memory whose number  parameters grows quadratically  memory size encoderdecoder networks  encoderdecoder framework   framework based  neural networks  aims  map highly structured input  highly structured output   proposed recently   context  machine translation   input  output  written sentences  two natural languages   work  lstm recurrent neural network rnn  convolutional neural network cnn  used   encoder  summarize  source sentence   summary  decoded using  conditional recurrent neural network language model  produce  translation   systems    building blocks gated rnns  cnns  trained attention mechanisms  architectures multilayer kernel machine multilayer kernel machines mkm  introduced    way  learning highly nonlinear functions  iterative application  weakly nonlinear kernels  use  kernel principal component analysis kpca   method  unsupervised greedy layerwise pretraining step   deep learning architecture layer formulath learns  representation   previous layer formula extracting  formula principal component pc   projection layer formula output   feature domain induced   kernel   sake  dimensionality reduction   updated representation   layer  supervised strategy  proposed  select  best informative features among features extracted  kpca  process     drawbacks  using  kpca method   building cells   mkm   straightforward way  use kernel machines  deep learning  developed  microsoft researchers  spoken language understanding  main idea   use  kernel machine  approximate  shallow neural net   infinite number  hidden units  use stacking  splice  output   kernel machine   raw input  building  next higher level   kernel machine  number  levels   deep convex network   hyperparameter   overall system   determined  cross validation applications automatic speech recognition speech recognition   revolutionised  deep learning especially  long shortterm memory lstm  recurrent neural network published  sepp hochreiter j rgen schmidhuber  lstm rnns circumvent  vanishing gradient problem  can learn  deep learning tasks  involve speech events separated  thousands  discrete time steps  one time step corresponds   ms  lstm  forget gates became competitive  traditional speech recognizers  certain tasks  lstm trained  connectionist temporal classification ctc achieved excellent results  certain applications although computers  much slower  today  googles large scale speech recognition suddenly almost doubled  performance  ctctrained lstm now available   smartphone users  initial success  deep learning  speech recognition however  based  smallscale timit tasks  results shown   table    automatic speech recognition   popular timit data set    common data set used  initial evaluations  deep learning architectures  entire set contains speakers  eight major dialects  american english   speaker reads sentences  small size allows many configurations   tried effectively  importantly  timit task concerns phonesequence recognition  unlike wordsequence recognition allows  weak language models  thus  weaknesses  acoustic modeling aspects  speech recognition can   easily analyzed  analysis  timit  li deng  collaborators around contrasting  gmm   generative models  speech vs dnn models stimulated early industrial investment  deep learning  speech recognition  small  large scales eventually leading  pervasive  dominant use   industry  analysis  done  comparable performance less   error rate  discriminative dnns  generative models  error rates listed  including  early results  measured  percent phone error rates per   summarized   time span   past years  industrial researchers extended deep learning  timit  large vocabulary speech recognition  adopting large output layers   dnn based  contextdependent hmm states constructed  decision trees comprehensive reviews   development    state   art   october  provided   recent springer book  microsoft research  earlier article reviewed  background  automatic speech recognition   impact  various machine learning paradigms including deep learning one fundamental principle  deep learning    away  handcrafted feature engineering   use raw features  principle  first explored successfully   architecture  deep autoencoder   raw spectrogram  linear filterbank features  sri   late s  later  microsoft showing  superiority   melcepstral features  contain   stages  fixed transformation  spectrograms  true raw features  speech waveforms   recently  shown  produce excellent largerscale speech recognition results since  initial successful debut  dnns  speaker recognition   late s  speech recognition around   lstm around    huge new progresses made progress  future directions can  summarized  eight major areas largescale automatic speech recognition   first   convincing successful case  deep learning   recent history embraced   industry  academia across  board    two major conferences  signal processing  speech recognition ieeeicassp  interspeech  seen  large increase   numbers  accepted papers   respective annual conference papers   topic  deep learning  speech recognition  importantly  major commercial speech recognition systems eg microsoft cortana xbox skype translator amazon alexa google now apple siri baidu  iflytek voice search   range  nuance speech products etc   based  deep learning methods see also  recent media interview   cto  nuance communications image recognition  common evaluation set  image classification   mnist database data set mnist  composed  handwritten digits  includes training examples  test examples   timit  small size allows multiple configurations   tested  comprehensive list  results   set can  found   current best result  mnist   error rate  achieved  ciresan et al  according  lecun   early s   industrial application cnns already processed  estimated     checks written   us   early s significant additional impact  deep learning  image  object recognition  felt   years although cnns trained  backpropagation   around  decades  gpu implementations  nns  years including cnns fast implementations  cnns  maxpooling  gpus   style  dan ciresan  colleagues  needed  make  dent  computer vision   approach achieved   first time superhuman performance   visual pattern recognition contest also   won  icdar chinese handwriting contest   may  won  isbi image segmentation contest  cnns   play  major role  computer vision conferences   june  paper  dan ciresan et al   leading conference cvpr showed  maxpooling cnns  gpu can dramatically improve many vision benchmark records sometimes  humancompetitive  even superhuman performance  october  similar system  alex krizhevsky   team  geoff hinton won  largescale imagenet competition   significant margin  shallow machine learning methods  november ciresan et als system also won  icpr contest  analysis  large medical images  cancer detection    following year also  miccai grand challenge    topic    error rate   imagenet task using deep learning   reduced quickly following  similar trend  largescale speech recognition    ambitious moves  automatic speech recognition toward automatic speech translation  understanding image classification  recently  extended    challenging task  automatic image captioning   deep learning often   combination  cnns  lstms   essential underlying technology one example application   car computer said   trained  deep learning  may enable cars  interpret camera views another example   technology known  facial dysmorphology novel analysis fdna used  analyze cases  human malformation connected   large database  genetic syndromes natural language processing neural networks   used  implementing language models since  early s recurrent neural networks especially lstm   appropriate  sequential data   language lstm helped  improve machine translation  language modeling lstm combined  cnns also improved automatic image captioning   plethora   applications  key techniques   field  negative sampling  word embedding word embedding   wordvec can  thought    representational layer   deep learning architecture  transforms  atomic word   positional representation   word relative   words   dataset  position  represented   point   vector space using word embedding   input layer   recursive neural network rnn allows  training   network  parse sentences  phrases using  effective compositional vector grammar  compositional vector grammar can  thought   probabilistic context  grammar pcfg implemented   recursive neural network recursive autoencoders built atop word embeddings   trained  assess sentence similarity  detect paraphrasing deep neural architectures  achieved stateoftheart results  many natural language processing tasks   constituency parsing sentiment analysis information retrieval spoken language understanding machine translation contextual entity linking  others drug discovery  toxicology  pharmaceutical industry faces  problem   large percentage  candidate drugs fail  reach  market  failures  chemical compounds  caused  insufficient efficacy   biomolecular target ontarget effect undetected  undesired interactions   biomolecules offtarget effects  unanticipated toxic effects   team led  george dahl won  merck molecular activity challenge using multitask deep neural networks  predict  biomolecular target   compound  sepp hochreiters group used deep learning  detect offtarget  toxic effects  environmental chemicals  nutrients household products  drugs  won  tox data challenge  nih fda  ncats  impressive successes show  deep learning may  superior   virtual screening methods researchers  google  stanford enhanced deep learning  drug discovery  combining data   variety  sources  atomwise introduced atomnet  first deep learning neural networks  structurebased rational drug design subsequently atomnet  used  predict novel candidate biomolecules  several disease targets  notably treatments   ebola virus  multiple sclerosis customer relationship management recently success   reported  application  deep reinforcement learning  direct marketing settings illustrating suitability   method  crm automation  neural network  used  approximate  value  possible direct marketing actions   customer state space defined  terms  rfm variables  estimated value function  shown    natural interpretation  customer lifetime value recommendation systems recommendation systems  used deep learning  extract meaningful deep features  latent factor model  contentbased recommendation  music recently   general approach  learning user preferences  multiple domains using multiview deep learning   introduced  model uses  hybrid collaborative  contentbased approach  enhances recommendations  multiple tasks biomedical informatics recently  deeplearning approach based   autoencoder artificial neural network   used  bioinformatics  predict gene ontology annotations  genefunction relationships  medical informatics deep learning  also  used   health domain including  prediction  sleep quality based  wearable data  predictions  health complications  electronic health record data theories   human brain computational deep learning  closely related   class  theories  brain development specifically neocortical development proposed  cognitive neuroscientists   early s  approachable summary   work  elman et als book rethinking innateness see also shrager  johnson quartz  sejnowski   developmental theories  also instantiated  computational models   technical predecessors  purely computationally motivated deep learning models  developmental models share  interesting property  various proposed learning dynamics   brain eg  wave  nerve growth factor conspire  support  selforganization  just  sort  interrelated neural networks utilized   later purely computational deep learning models   computational neural networks seem analogous   view   brains neocortex   hierarchy  filters    layer captures    information   operating environment   passes  remainder  well  modified base signal   layers    hierarchy  process yields  selforganizing stack  transducers welltuned   operating environment  described   new york times   infants brain seems  organize    influence  waves  socalled trophicfactors different regions   brain become connected sequentially  one layer  tissue maturing  another      whole brain  mature  importance  deep learning  respect   evolution  development  human cognition   escape  attention   researchers one aspect  human development  distinguishes us   nearest primate neighbors may  changes   timing  development among primates  human brain remains relatively plastic  late   postnatal period whereas  brains   closest relatives   completely formed  birth thus humans  greater access   complex experiences afforded      world    formative period  brain development  may enable us  tune   rapidly changing features   environment   animals  constrained  evolutionary structuring   brains  unable  take account    extent   changes  reflected  similar timing changes  hypothesized wave  cortical development  may also lead  changes   extraction  information   stimulus environment   early selforganization   brain  course along   flexibility comes  extended period  immaturity     dependent upon  caretakers   community   support  training  theory  deep learning therefore sees  coevolution  culture  cognition   fundamental condition  human evolution commercial activities deep learning  often presented   step towards realising strong ai  thus many organizations  become interested   use  particular applications  december facebook hired yann lecun  head  new artificial intelligence ai lab     operations  california london  new york  ai lab will develop deep learning techniques  help facebook  tasks   automatically tagging uploaded pictures   names   people   late  facebook also hired vladimir vapnik  main developer   vapnikchervonenkis theory  statistical learning  coinventor   support vector machine method  google also bought deepmind technologies  british startup  developed  system capable  learning   play atari video games using  raw pixels  data input   demonstrated alphago system  achieved one   longstanding grand challenges  ai  learning  game  go well enough  beat  human professional go player  blippar demonstrated  new mobile augmented reality application  makes use  deep learning  recognize objects  real time criticism  comment given  farreaching implications  artificial intelligence coupled   realization  deep learning  emerging  one    powerful techniques  subject  understandably attracting  criticism  comment    cases  outside  field  computer science   main criticism  deep learning concerns  lack  theory surrounding many   methods learning    common deep architectures  implemented using gradient descent  gradient descent   understood    now  theory surrounding  algorithms   contrastive divergence  less clear ie   converge    fast    approximating deep learning methods  often looked    black box   confirmations done empirically rather  theoretically others point   deep learning   looked    step towards realizing strong ai    allencompassing solution despite  power  deep learning methods  still lack much   functionality needed  realizing  goal entirely research psychologist gary marcus  noted thatrealistically deep learning   part   larger challenge  building intelligent machines  techniques lack ways  representing causal relationships   obvious ways  performing logical inferences    also still  long way  integrating abstract knowledge   information   objects          typically used   powerful ai systems like watson use techniques like deep learning  just one element    complicated ensemble  techniques ranging   statistical technique  bayesian inference  deductive reasoningto  extent    viewpoint implies without intending   deep learning will ultimately constitute nothing    primitive discriminatory levels   comprehensive future machine intelligence  recent pair  speculations regarding art  artificial intelligence offers  alternative   expansive outlook  first  speculation    might  possible  train  machine vision stack  perform  sophisticated task  discriminating  old master  amateur figure drawings   second     sensitivity might  fact represent  rudiments   nontrivial machine empathy   suggested moreover    eventuality    line  anthropology  identifies  concern  aesthetics   key element  behavioral modernity   reference   idea   significant degree  artistic sensitivity might inhere within relatively low levels whether biological  digital   cognitive hierarchy  published series  graphic representations   internal states  deep layers neural networks attempting  discern within essentially random data  images     trained seem  demonstrate  striking visual appeal  light   remarkable level  public attention   work captured  original research notice received well  comments   coverage   guardian    time   frequently accessed article   newspapers web site  currently popular  successful deep learning architectures display certain problematic behaviors   confidently classifying unrecognizable images  belonging   familiar category  ordinary images  misclassifying minuscule perturbations  correctly classified images  creator  opencog ben goertzel hypothesized   behaviors  due  limitations   internal representations learned   architectures    limitations  inhibit integration   architectures  heterogeneous multicomponent agi architectures   suggested   issues can  worked around  developing deep learning architectures  internally form states homologous  imagegrammar decompositions  observed entities  events learning  grammar visual  linguistic  training data   equivalent  restricting  system  commonsense reasoning  operates  concepts  terms  production rules   grammar    basic goal   human language acquisition  ai see also grammar induction\r\n"}
{"index":{"_id":118}}
{"conceptLabelTag":"loss function","conceptLabel":"loss function","conceptDescription":"loss function  mathematical optimization statistics decision theory  machine learning  loss function  cost function   function  maps  event  values  one   variables onto  real number intuitively representing  cost associated   event  optimization problem seeks  minimize  loss function  objective function  either  loss function   negative sometimes called  reward function  profit function  utility function  fitness function etc   case     maximized  statistics typically  loss function  used  parameter estimation   event  question   function   difference  estimated  true values   instance  data  concept  old  laplace  reintroduced  statistics  abraham wald   middle   th century   context  economics  example   usually economic cost  regret  classification    penalty   incorrect classification   example  actuarial science   used   insurance context  model benefits paid  premiums particularly since  works  harald cram r   s  optimal control  loss   penalty  failing  achieve  desired value  financial risk management  function  precisely mapped   monetary loss use  statistics parameter estimation  supervised learning tasks   regression  classification can  formulated   minimization   loss function   training set  goal  estimation   find  function  models  input well    applied   training set   predict  values  class labels associated   samples   set  loss function quantifies  amount    prediction deviates   actual values definition formally  begin  considering  family  distributions   random variable x   indexed    intuitively  can think  x   data perhaps formula  formula iid  x   set  things  decision rule will  making decisions   exists  number  possible ways formula  model  data x   decision function can use  make decisions   finite number  models  can thus think    index   family  probability models   infinite family  models    set  parameters   family  distributions    practical note   important  understand     tempting  think  loss functions  necessarily parametric since  seem  take   parameter  fact   infinitedimensional  completely incompatible   notion  example   family  probability functions  uncountably infinite indexes  uncountably infinite space   given  set   possible actions  decision rule   function formula   loss function   real lowerbounded function l      value l x   cost  action x  parameter expected loss  value   loss function    random quantity   depends   outcome   random variable x  frequentist  bayesian statistical theory involve making  decision based   expected value   loss function however  quantity  defined differently   two paradigms frequentist expected loss  first define  expected loss   frequentist context   obtained  taking  expected value  respect   probability distribution p   observed data x   also referred    risk function   decision rule   parameter   decision rule depends   outcome  x  risk function  given  formula    fixed  possibly unknown state  nature x   vector  observations stochastically drawn   population formula   expectation   population values  x dp   probability measure   event space  x parametrized    integral  evaluated   entire support  x bayesian expected loss   bayesian approach  expectation  calculated using  posterior distribution   parameter one   choose  action   minimises  expected loss although  will result  choosing   action    chosen using  frequentist risk  emphasis   bayesian approach   one   interested  choosing  optimal action   actual observed data whereas choosing  actual frequentist optimal decision rule    function   possible observations   much  difficult problem economic choice  uncertainty  economics decisionmaking  uncertainty  often modelled using  von neumannmorgenstern utility function   uncertain variable  interest   endofperiod wealth since  value   variable  uncertain    value   utility function    expected value  utility   maximized decision rules  decision rule makes  choice using  optimality criterion  commonly used criteria  selecting  loss function sound statistical practice requires selecting  estimator consistent   actual acceptable variation experienced   context   particular applied problem thus   applied use  loss functions selecting  statistical method  use  model  applied problem depends  knowing  losses  will  experienced   wrong   problems particular circumstances  common example involves estimating location  typical statistical assumptions  mean  average   statistic  estimating location  minimizes  expected loss experienced   squarederror loss function   median   estimator  minimizes expected loss experienced   absolutedifference loss function still different estimators   optimal   less common circumstances  economics   agent  risk neutral  objective function  simply expressed  monetary terms   profit income  endofperiod wealth   riskaverse  riskloving agents loss  measured   negative   utility function  represents satisfaction   usually interpreted  ordinal terms rather   cardinal absolute terms  measures  cost  possible  example mortality  morbidity   field  public health  safety engineering   optimization algorithms   desirable    loss function   globally continuous  differentiable two  commonly used loss functions   squared loss formula   absolute loss formula however  absolute loss   disadvantage     differentiable  formula  squared loss   disadvantage     tendency   dominated  outlierswhen summing   set  formulas   formula  final sum tends    result    particularly large avalues rather   expression   average avalue  choice   loss function   arbitrary    restrictive  sometimes  loss function may  characterized   desirable properties among  choice principles   example  requirement  completeness   class  symmetric statistics   case  iid observations  principle  complete information   others loss functions  bayesian statistics one   consequences  bayesian inference    addition  experimental data  loss function     wholly determine  decision   important   relationship   loss function   posterior probability    possible   two different loss functions  lead    decision   prior probability distributions associated   compensate   details   loss function combining  three elements   prior probability  data   loss function  allows decisions   based  maximizing  subjective expected utility  concept introduced  leonard j savage regret savage also argued  using nonbayesian methods   minimax  loss function   based   idea  regret ie  loss associated   decision    difference   consequences   best decision     taken   underlying circumstances  known   decision    fact taken    known quadratic loss function  use   quadratic loss function  common  example  using least squares techniques   often  mathematically tractable   loss functions    properties  variances  well   symmetric  error   target causes   loss    magnitude  error   target   target  t   quadratic loss function    constant c  value   constant makes  difference   decision  can  ignored  setting  equal  many common statistics including ttests regression models design  experiments  much else use least squares methods applied using linear regression theory   based   quadratric loss function  quadratic loss function  also used  linearquadratic optimal control problems   problems even   absence  uncertainty  may   possible  achieve  desired values   target variables often loss  expressed   quadratic form   deviations   variables  interest   desired values  approach  tractable   results  linear firstorder conditions   context  stochastic control  expected value   quadratic form  used loss function  statistics  decision theory  frequently used loss function   loss function  formula   indicator notation\r\n"}
{"index":{"_id":119}}
{"conceptLabelTag":"markov decision process","conceptLabel":"markov decision process","conceptDescription":"markov decision process markov decision processes mdps provide  mathematical framework  modeling decision making  situations  outcomes  partly random  partly   control   decision maker mdps  useful  studying  wide range  optimization problems solved via dynamic programming  reinforcement learning mdps  known  least  early   s cf  core body  research  markov decision processes resulted  ronald  howards book published  dynamic programming  markov processes   used   wide area  disciplines including robotics automated control economics  manufacturing  precisely  markov decision process   discrete time stochastic control process   time step  process    state formula   decision maker may choose  action formula   available  state formula  process responds   next time step  randomly moving   new state formula  giving  decision maker  corresponding reward formula  probability   process moves   new state formula  influenced   chosen action specifically   given   state transition function formula thus  next state formula depends   current state formula   decision makers action formula  given formula  formula   conditionally independent   previous states  actions   words  state transitions   mdp satisfies  markov property markov decision processes   extension  markov chains  difference   addition  actions allowing choice  rewards giving motivation conversely   one action exists   state eg wait   rewards    eg zero  markov decision process reduces   markov chain definition  markov decision process   tuple formula  note  theory  markov decision processes   state  formula  formula  finite   basic algorithms  assume    finite problem  core problem  mdps   find  policy   decision maker  function formula  specifies  action formula   decision maker will choose   state formula note    markov decision process  combined   policy   way  fixes  action   state   resulting combination behaves like  markov chain  goal   choose  policy formula  will maximize  cumulative function   random rewards typically  expected discounted sum   potentially infinite horizon  formula   discount factor  satisfies formula  example formula   discount rate  r formula  typically close     markov property  optimal policy   particular problem can indeed  written   function  formula   assumed  algorithms mdps can  solved  linear programming  dynamic programming   follows  present  latter approach suppose  know  state transition function formula   reward function formula   wish  calculate  policy  maximizes  expected discounted reward  standard family  algorithms  calculate  optimal policy requires storage  two arrays indexed  state value formula  contains real values  policy formula  contains actions   end   algorithm formula will contain  solution  formula will contain  discounted sum   rewards   earned  average  following  solution  state formula  algorithm   following two kinds  steps   repeated   order    states    changes take place   defined recursively  follows  order depends   variant   algorithm one can also     states    state  state   often   states  others  long   state  permanently excluded  either   steps  algorithm will eventually arrive   correct solution notable variants value iteration  value iteration   also called backward induction  formula function   used instead  value  formula  calculated within formula whenever   needed lloyd shapleys paper  stochastic games included   special case  value iteration method  mdps    recognized  later  substituting  calculation  formula   calculation  formula gives  combined step  formula   iteration number value iteration starts  formula  formula   guess   value function   iterates repeatedly computing formula   states formula  formula converges   lefthand side equal   righthand side    bellman equation   problem policy iteration  policy iteration step one  performed    step two  repeated   converges  step one   performed     instead  repeating step two  convergence  may  formulated  solved   set  linear equations  variant   advantage     definite stopping condition   array formula   change   course  applying step   states  algorithm  completed modified policy iteration  modified policy iteration step one  performed    step two  repeated several times  step one   performed     prioritized sweeping   variant  steps  preferentially applied  states     way important whether based   algorithm   large changes  formula  formula around  states recently  based  use  states  near  starting state  otherwise  interest   person  program using  algorithm extensions  generalizations  markov decision process   stochastic game   one player partial observability  solution  assumes   state formula  known  action    taken otherwise formula   calculated   assumption   true  problem  called  partially observable markov decision process  pomdp  major advance   area  provided  burnetas  katehakis  optimal adaptive policies  markov decision processes   work  class  adaptive policies  possess uniformly maximum convergence rate properties   total expected finite horizon reward  constructed   assumptions  finite stateaction spaces  irreducibility   transition law  policies prescribe   choice  actions   state  time period   based  indices   inflations   righthand side   estimated average reward optimality equations reinforcement learning   probabilities  rewards  unknown  problem  one  reinforcement learning   purpose   useful  define   function  corresponds  taking  action formula   continuing optimally  according  whatever policy one currently    function  also unknown experience  learning  based  formula pairs together   outcome formula      state formula   tried  formula  formula happened thus one   array formula  uses experience  update  directly   known  qlearning reinforcement learning can solve markov decision processes without explicit specification   transition probabilities  values   transition probabilities  needed  value  policy iteration  reinforcement learning instead  explicit specification   transition probabilities  transition probabilities  accessed   simulator   typically restarted many times   uniformly random initial state reinforcement learning can also  combined  function approximation  address problems    large number  states learning automata another application  mdp process  machine learning theory  called learning automata   also one type  reinforcement learning   environment   stochastic manner  first detail learning automata paper  surveyed  narendra  thathachar   originally described explicitly  finite state automata similar  reinforcement learning learning automata algorithm also   advantage  solving  problem  probability  rewards  unknown  difference  learning automata  qlearning    omit  memory  qvalues  update  action probability directly  find  learning result learning automata   learning scheme   rigorous proof  convergence  learning automata theory  stochastic automaton  consist   states    automaton correspond   states   discretestate discreteparameter markov process   time step t  automaton reads  input   environment updates pt  pt   randomly chooses  successor state according   probabilities pt  outputs  corresponding action  automatons environment  turn reads  action  sends  next input   automaton category theoretic interpretation    rewards  markov decision process formula can  understood  terms  category theory namely let formula denote   monoid  generating set  let dist denote  kleisli category   giry monad   functor formula encodes   set s  states   probability function p   way markov decision processes   generalized  monoids categories  one object  arbitrary categories one can call  result formula  contextdependent markov decision process  moving  one object  another  formula changes  set  available actions   set  possible states fuzzy markov decision processes fmdps   mdps optimal policy   policy  maximize  summation  future rewards therefore optimal policy consist several actions  belong   finite set  actions  fuzzy markov decision processes fmdps first  value function  computed  regular mdps ie   finite set  actions   policy  extracted   fuzzy inference system   words  value function  utilized   input   fuzzy inference system   policy   output   fuzzy inference system continuoustime markov decision process  discretetime markov decision processes decisions  made  discrete time intervals however  continuoustime markov decision processes decisions can  made   time  decision maker chooses  comparison  discretetime markov decision process continuoustime markov decision process can better model  decision making process   system   continuous dynamics ie  system dynamics  defined  partial differential equations pdes definition  order  discuss  continuoustime markov decision process  introduce two sets  notations   state space  action space  finite   state space  action space  continuous problem like  discretetime markov decision processes  continuoustime markov decision process  want  find  optimal policy  control   give us  optimal expected integrated reward  formula linear programming formulation   state space  action space  finite   use linear programming  find  optimal policy   one   earliest approaches applied    consider  ergodic model  means  continuoustime mdp becomes  ergodic continuoustime markov chain   stationary policy   assumption although  decision maker can make  decision   time   current state    benefit   taking   one action   better    take  action    time  system  transitioning   current state  another state   conditionsfor detail check corollary  continuoustime markov decision processes   optimal value function formula  independent  state formula  will   following inequality   exists  function formula  formula will   smallest formula satisfying   equation  order  find formula   use  following linear programming model formula   feasible solution   dlp  formula  nonnative  satisfied  constraints   dlp problem  feasible solution formula   dlp  said    optimal solution    feasible solution formula   dlp   found  optimal solution formula   use  optimal solution  establish  optimal policies hamiltonjacobibellman equation  continuoustime mdp   state space  action space  continuous  optimal criterion   found  solving hamiltonjacobibellman hjb partial differential equation  order  discuss  hjb equation  need  reformulate  problem formula   terminal reward function formula   system state vector formula   system control vector  try  find formula shows   state vector change  time hamiltonjacobibellman equation   follows   solve  equation  find  optimal control formula   give us  optimal value formula application continuoustime markov decision processes  applications  queueing systems epidemic processes  population processes alternative notations  terminology  notation  mdps   entirely settled   two main streams one focuses  maximization problems  contexts like economics using  terms action reward value  calling  discount factor formula  formula    focuses  minimization problems  engineering   using  terms control cost costtogo  calling  discount factor formula  addition  notation   transition probability varies  addition transition probability  sometimes written formula formula  rarely formula constrained markov decision processes constrained markov decision processes cmdps  extensions  markov decision process mdps   three fundamental differences  mdps  cmdps    number  applications  cmdps   recently  used  motion planning scenarios  robotics\r\n"}
{"index":{"_id":120}}
{"conceptLabelTag":"maximum likelihood","conceptLabel":"maximum likelihood","conceptDescription":"maximum likelihood estimation  statistics maximum likelihood estimation mle   method  estimating  parameters   statistical model given observations  finding  parameter values  maximize  likelihood  making  observations given  parameters mle can  seen   special case   maximum  posteriori estimation map  assumes  uniform prior distribution   parameters    variant   map  ignores  prior   therefore  unregularized  method  maximum likelihood corresponds  many wellknown estimation methods  statistics  example one may  interested   heights  adult female penguins   unable  measure  height  every single penguin   population due  cost  time constraints assuming   heights  normally distributed   unknown mean  variance  mean  variance can  estimated  mle   knowing  heights   sample   overall population mle  accomplish   taking  mean  variance  parameters  finding particular parametric values  make  observed results   probable given  model  general   fixed set  data  underlying statistical model  method  maximum likelihood selects  set  values   model parameters  maximizes  likelihood function intuitively  maximizes  agreement   selected model   observed data   discrete random variables  indeed maximizes  probability   observed data   resulting distribution maximum likelihood estimation gives  unified approach  estimation   welldefined   case   normal distribution  many  problems history maximumlikelihood estimation  recommended analyzed  fruitless attempts  proofs  widely popularized  ronald fisher   although    used earlier  carl friedrich gauss pierresimon laplace thorvald n thiele  francis ysidro edgeworth maximumlikelihood estimation finally transcended heuristic justification   proof published  samuel s wilks  now called wilks theorem  theorem shows   error   logarithm  likelihood values  estimates  multiple independent samples  distributed  enables determination   confidence region around  one estimate   parameters ironically   difficult part   proof depends   expected value   fisher information matrix   provided   theorem  fisher wilks continued  improve   generality   theorem throughout  life    general proof published     theory behind maximum likelihood estimation  developed  bayesian statistics reviews   development  maximum likelihood estimation   provided   number  authors principles suppose    sample  n independent  identically distributed observations coming   distribution   unknown probability density function f   however surmised   function f belongs   certain family  distributions    vector  parameters   family called  parametric model    value  unknown   referred    true value   parameter vector   desirable  find  estimator formula     close   true value  possible either    observed variables x   parameter can  vectors  use  method  maximum likelihood one first specifies  joint density function   observations   independent  identically distributed sample  joint density function  now  look   function   different perspective  considering  observed values x x x   fixed parameters   function whereas will   functions variable  allowed  vary ly   function will  called  likelihood note  formula denotes  separation   two categories  input arguments  parameters formula   observations formula  practice  algebra  often  convenient  working   natural logarithm   likelihood function called  loglikelihood   average loglikelihood  hat  indicates    akin   estimator indeed formula estimates  expected loglikelihood   single observation   model  method  maximum likelihood estimates  finding  value   maximizes formula  method  estimation defines  maximum likelihood estimator mle    maximum exists  mle estimate    regardless  whether  maximize  likelihood   loglikelihood function since log   monotonically increasing function  many models  maximum likelihood estimator can  found   explicit function   observed data  many  models however  closedform solution   maximization problem  known  available   mle    found numerically using optimization methods   problems  may  multiple estimates  maximize  likelihood   problems  maximum likelihood estimate exists either  loglikelihood function increases without ever reaching  supremum value    supremum  exist   outside  bounds  formula  set  acceptable parameter values   exposition    assumed   data  independent  identically distributed  method can  applied however   broader setting  long    possible  write  joint density function   parameter   finite dimension    depend   sample size n   simpler extension  allowance can  made  data heterogeneity    joint density  equal  fx fx put another way   now assuming   observation x comes   random variable     distribution function f    complicated case  time series models  independence assumption may    dropped  well  maximum likelihood estimator coincides    probable bayesian estimator given  uniform prior distribution   parameters indeed  maximum  posteriori estimate   parameter  maximizes  probability  given  data given  bayes theorem  formula   prior distribution   parameter   formula   probability   data averaged   parameters since  denominator  independent   bayesian estimator  obtained  maximizing formula  respect     assume   prior formula   uniform distribution  bayesian estimator  obtained  maximizing  likelihood function formula thus  bayesian estimator coincides   maximum likelihood estimator   uniform prior distribution formula properties  maximum likelihood estimator   extremum estimator obtained  maximizing   function   objective function cf  loss function    sample analogue   expected loglikelihood formula   expectation  taken  respect   true density formula maximumlikelihood estimators   optimum properties  finite samples   sense   evaluated  finite samples  estimators may  greater concentration around  true parametervalue however like  estimation methods maximum likelihood estimation possesses  number  attractive limiting properties   sample size increases  infinity sequences  maximum likelihood estimators   properties consistency   conditions outlined   maximum likelihood estimator  consistent  consistency means    sufficiently large number  observations n   possible  find  value   arbitrary precision  mathematical terms  means   n goes  infinity  estimator formula converges  probability   true value  slightly stronger conditions  estimator converges almost surely  strongly  theta  establish consistency  following conditions  sufficient  dominance condition can  employed   case  iid observations   noniid case  uniform convergence  probability can  checked  showing   sequence formula  stochastically equicontinuous  one wants  demonstrate   ml estimator formula converges  almost surely   stronger condition  uniform convergence almost surely    imposed asymptotic normality   wide range  situations maximum likelihood parameter estimates exhibit asymptotic normality     equal   true parameters plus  random error   approximately normal given sufficient data   errors variance decays  n   property  hold   necessary   estimator   suffer   following issues estimate  boundary sometimes  maximum likelihood estimate lies   boundary   set  possible parameters    boundary   strictly speaking allowed  likelihood gets larger  larger   parameter approaches  boundary standard asymptotic theory needs  assumption   true parameter value lies away   boundary    enough data  maximum likelihood estimate will keep away   boundary    smaller samples  estimate can lie   boundary   cases  asymptotic theory clearly   give  practically useful approximation examples    variancecomponent models   component  variance must satisfy  constraint data boundary parameterdependent   theory  apply   simple way  set  data values   positive probability  positive probability density   depend   unknown parameter  simple example   parameterdependence  hold   case  estimating   set  independent identically distributed observations   common distribution  uniform   range  estimation purposes  relevant range       less   largest observation   interval   compact  exists  maximum   likelihood function   estimate  theta  exists  greater estimate  also  greater likelihood  contrast  interval includes  endpoint   compact   case  maximum likelihood estimator exists however   case  maximum likelihood estimator  biased asymptotically  maximum likelihood estimator   normally distributed nuisance parameters  maximum likelihood estimations  model may   number  nuisance parameters   asymptotic behaviour outlined  hold  number  nuisance parameters   increase   number  observations  sample size  wellknown example   case   observations occur  pairs   observations   pair   different unknown mean  otherwise  observations  independent  normally distributed   common variance   n observations   n parameters   well known   maximum likelihood estimate   variance   converge   true value   variance increasing information   asymptotics  hold  cases   assumption  independent identically distributed observations   hold  basic requirement    amount  information   data increases indefinitely   sample size increases   requirement may   met  either    much dependence   data  example  new observations  essentially identical  existing observations   new independent observations  subject   increasing observation error  regularity conditions  ensure  behavior  suppose  conditions  consistency  maximum likelihood estimator  satisfied    maximum likelihood estimator  asymptotically normal distribution proof skipping  technicalities since  loglikelihood function  differentiable  formula lies   interior   parameter set formula   maximum  firstorder condition will  satisfied   loglikelihood  twice differentiable  expression can  expanded   taylor series around  point formula  formula   point intermediate  formula  formula   expression  can derive    expression  square brackets converges  probability  formula   law  large numbers  continuous mapping theorem ensures   inverse   expression also converges  probability  formula  second sum   central limit theorem converges  distribution   multivariate normal  mean zero  variance matrix equal   fisher information formula thus applying slutskys theorem   whole expression  obtain  finally  information equality guarantees    model  correctly specified matrix formula will  equal   fisher information formula    variance expression simplifies  just formula functional invariance  maximum likelihood estimator selects  parameter value  gives  observed data  largest possible probability  probability density   continuous case   parameter consists   number  components   define  separate maximum likelihood estimators   corresponding component   mle   complete parameter consistent    formula   mle    g   transformation    mle  g   definition  maximizes  socalled profile likelihood  mle  also invariant  respect  certain transformations   data  y gx  g  one  one    depend   parameters   estimated   density functions satisfy  hence  likelihood functions  x  y differ    factor    depend   model parameters  example  mle parameters   lognormal distribution        normal distribution fitted   logarithm   data higherorder properties  standard asymptotics tells   maximum likelihood estimator  nconsistent  asymptotically efficient meaning   reaches  cram rrao bound     fisher information matrix  particular  means   bias   maximum likelihood estimator  equal  zero    order n however   consider  higherorder terms   expansion   distribution   estimator  turns    bias  order n  bias  equal  componentwise  einsteins summation convention   repeating indices   adopted  denotes  jkth component   inverse fisher information matrix   using  formulas   possible  estimate  secondorder bias   maximum likelihood estimator  correct   bias  subtracting   estimator  unbiased    terms  order n   called  biascorrected maximum likelihood estimator  biascorrected estimator  secondorder efficient  least within  curved exponential family meaning    minimal mean squared error among  secondorder biascorrected estimators    terms   order n   possible  continue  process    derive  thirdorder biascorrection term    however   shown   maximum likelihood estimator   thirdorder efficient examples discrete uniform distribution consider  case  n tickets numbered   n  placed   box  one  selected  random see uniform distribution thus  sample size   n  unknown   maximum likelihood estimator formula  n   number m   drawn ticket  likelihood   n m n  n m    greatest  n m note   maximum likelihood estimate  n occurs   lower extreme  possible values m m rather  somewhere   middle   range  possible values   result  less bias  expected value   number m   drawn ticket  therefore  expected value  formula  n   result   sample size   maximum likelihood estimator  n will systematically underestimate n  n discrete distribution finite parameter space suppose one wishes  determine just  biased  unfair coin  call  probability  tossing  head p  goal  becomes  determine p suppose  coin  tossed times ie  sample might  something like x h x t x t   count   number  heads h  observed  probability  tossing tails  p   p   suppose  outcome  heads  tails  suppose  coin  taken   box containing three coins one  gives heads  probability p one  gives heads  probability p  another  gives heads  probability p  coins  lost  labels   one    unknown using maximum likelihood estimation  coin    largest likelihood can  found given  data   observed  using  probability mass function   binomial distribution  sample size equal  number successes equal   different values  p  probability  success  likelihood function defined  takes one  three values  likelihood  maximized  p      maximum likelihood estimate  p discrete distribution continuous parameter space now suppose     one coin   p     value p  likelihood function   maximised    maximisation    possible values p one way  maximize  function   differentiating  respect  p  setting  zero   solutions p p  p  solution  maximizes  likelihood  clearly p since p  p result   likelihood  zero thus  maximum likelihood estimator  p   result  easily generalized  substituting  letter   t   place   represent  observed number  successes   bernoulli trials   letter   n   place   represent  number  bernoulli trials exactly   calculation yields  maximum likelihood estimator t n   sequence  n bernoulli trials resulting  t successes continuous distribution continuous parameter space   normal distribution formula   probability density function  corresponding probability density function   sample  n independent identically distributed normal random variables  likelihood    conveniently  formula   sample mean  family  distributions  two parameters   maximize  likelihood formula   parameters simultaneously   possible individually since  logarithm function    continuous strictly increasing function   range   likelihood  values  maximize  likelihood will also maximize  logarithm  likelihoods logarithm   strictly increasing  log likelihood can  written  follows  now compute  derivatives   log likelihood  follows   solved    indeed  maximum   function since     turning point    second derivative  strictly less  zero  expectation value  equal   parameter   given distribution  means   maximum likelihood estimator formula  unbiased similarly  differentiate  log likelihood  respect   equate  zero   solved  inserting  estimate formula  obtain  calculate  expected value   convenient  rewrite  expression  terms  zeromean random variables statistical error formula expressing  estimate   variables yields simplifying  expression  utilizing  facts  formula  formula allows us  obtain  means   estimator formula  biased however formula  consistent formally  say   maximum likelihood estimator  formula    case  mles   obtained individually  general  may    case   mles     obtained simultaneously  normal log likelihood   maximum takes  particularly simple form  maximum log likelihood can  shown       general least squares even  nonlinear least squares   often used  determining likelihoodbased approximate confidence intervals  confidence regions   generally  accurate   using  asymptotic normality discussed  nonindependent variables  may   case  variables  correlated    independent two random variables x  y  independent    joint probability density function   product   individual probability density functions ie suppose one constructs  ordern gaussian vector   random variables formula   variable  means given  formula furthermore let  covariance matrix  denoted  formula  joint probability density function   n random variables   given    two variable case  joint probability density function  given      cases   joint density function exists  likelihood function  defined     section principles using  density iterative procedures consider problems   states formula  parameters   formula require   estimated iterative procedures   expectationmaximization algorithms may  used  solve joint stateparameter estimation problems  example suppose  n samples  state estimates formula together   sample mean formula   calculated  either  minimumvariance kalman filter   minimumvariance smoother using  previous variance estimate formula   next variance iterate may  obtained   maximum likelihood estimate calculation  convergence  mles within filtering  smoothing em algorithms   studied   literature applications maximum likelihood estimation  used   wide range  statistical models including  uses arise across applications  widespread set  fields including\r\n"}
{"index":{"_id":121}}
{"conceptLabelTag":"text mining","conceptLabel":"text mining","conceptDescription":"text mining text mining also referred   text data mining roughly equivalent  text analytics   process  deriving highquality information  text highquality information  typically derived   devising  patterns  trends  means   statistical pattern learning text mining usually involves  process  structuring  input text usually parsing along   addition   derived linguistic features   removal  others  subsequent insertion   database deriving patterns within  structured data  finally evaluation  interpretation   output high quality  text mining usually refers   combination  relevance novelty  interestingness typical text mining tasks include text categorization text clustering conceptentity extraction production  granular taxonomies sentiment analysis document summarization  entity relation modeling ie learning relations  named entities text analysis involves information retrieval lexical analysis  study word frequency distributions pattern recognition taggingannotation information extraction data mining techniques including link  association analysis visualization  predictive analytics  overarching goal  essentially  turn text  data  analysis via application  natural language processing nlp  analytical methods  typical application   scan  set  documents written   natural language  either model  document set  predictive classification purposes  populate  database  search index   information extracted text analytics  term text analytics describes  set  linguistic statistical  machine learning techniques  model  structure  information content  textual sources  business intelligence exploratory data analysis research  investigation  term  roughly synonymous  text mining indeed ronen feldman modified  description  text mining   describe text analytics  latter term  now used  frequently  business settings  text mining  used     earliest application areas dating   s notably lifesciences research  government intelligence  term text analytics also describes  application  text analytics  respond  business problems whether independently   conjunction  query  analysis  fielded numerical data    truism  percent  businessrelevant information originates  unstructured form primarily text  techniques  processes discover  present knowledge facts business rules  relationships   otherwise locked  textual form impenetrable  automated processing history laborintensive manual text mining approaches first surfaced   mids  technological advances  enabled  field  advance   past decade text mining   interdisciplinary field  draws  information retrieval data mining machine learning statistics  computational linguistics   information common estimates say   currently stored  text text mining  believed    high commercial potential value increasing interest   paid  multilingual data mining  ability  gain information across languages  cluster similar items  different linguistic sources according   meaning  challenge  exploiting  large proportion  enterprise information  originates  unstructured form   recognized  decades   recognized   earliest definition  business intelligence bi   october ibm journal article  hp luhn  business intelligence system  describes  system  will utilize dataprocessing machines  autoabstracting  autoencoding  documents   creating interest profiles     action points   organization  incoming  internally generated documents  automatically abstracted characterized   word pattern  sent automatically  appropriate action points yet  management information systems developed starting   s   bi emerged   s  s   software category  field  practice  emphasis   numerical data stored  relational databases    surprising text  unstructured documents  hard  process  emergence  text analytics   current form stems   refocusing  research   late s  algorithm development  application  described  prof marti  hearst   paper untangling text data mining  almost  decade  computational linguistics community  viewed large text collections   resource   tapped  order  produce better text analysis algorithms   paper   attempted  suggest  new emphasis  use  large online text collections  discover new facts  trends   world   suggest   make progress    need fully artificial intelligent text analysis rather  mixture  computationallydriven  userguided analysis may open  door  exciting new results hearsts statement  need fairly well describes  state  text analytics technology  practice  decade later text analysis processes subtaskscomponents   larger textanalytics efforttypically include applications  technology  now broadly applied   wide variety  government research  business needs applications can  sorted   number  categories  analysis type   business function using  approach  classifying solutions application categories include security applications many text mining software packages  marketed  security applications especially monitoring  analysis  online plain text sources   internet news blogs etc  national security purposes   also involved   study  text encryptiondecryption biomedical applications  range  text mining applications   biomedical literature   described one online text mining application   biomedical literature  pubgene  combines biomedical text mining  network visualization   internet service gopubmed   knowledgebased search engine  biomedical texts software applications text mining methods  software  also  researched  developed  major firms including ibm  microsoft   automate  mining  analysis processes   different firms working   area  search  indexing  general   way  improve  results within public sector much effort   concentrated  creating software  tracking  monitoring terrorist activities online media applications text mining   used  large media companies    tribune company  clarify information   provide readers  greater search experiences   turn increases site stickiness  revenue additionally   back end editors  benefiting   able  share associate  package news across properties significantly increasing opportunities  monetize content business  marketing applications text mining  starting   used  marketing  well  specifically  analytical customer relationship management coussement  van den poel apply   improve predictive analytics models  customer churn customer attrition text mining  also  applied  stock returns prediction sentiment analysis sentiment analysis may involve analysis  movie reviews  estimating  favorable  review    movie   analysis may need  labeled data set  labeling   affectivity  words resources  affectivity  words  concepts   made  wordnet  conceptnet respectively text   used  detect emotions   related area  affective computing text based approaches  affective computing   used  multiple corpora   students evaluations children stories  news stories academic applications  issue  text mining   importance  publishers  hold large databases  information needing indexing  retrieval   especially true  scientific disciplines   highly specific information  often contained within written text therefore initiatives   taken   natures proposal   open text mining interface otmi   national institutes  healths common journal publishing document type definition dtd   provide semantic cues  machines  answer specific queries contained within text without removing publisher barriers  public access academic institutions  also become involved   text mining initiative digital humanities  computational sociology  automatic analysis  vast textual corpora  created  possibility  scholars  analyse millions  documents  multiple languages   limited manual intervention key enabling technologies   parsing machine translation topic categorization  machine learning  automatic parsing  textual corpora  enabled  extraction  actors   relational networks   vast scale turning textual data  network data  resulting networks  can contain thousands  nodes   analysed  using tools  network theory  identify  key actors  key communities  parties  general properties   robustness  structural stability   overall network  centrality  certain nodes  automates  approach introduced  quantitative narrative analysis whereby subjectverbobject triplets  identified  pairs  actors linked   action  pairs formed  actorobject content analysis    traditional part  social sciences  media studies   long time  automation  content analysis  allowed  big data revolution  take place   field  studies  social media  newspaper content  include millions  news items gender bias readability content similarity reader preferences  even mood   analyzed based  text mining methods  millions  documents  analysis  readability gender bias  topic bias  demonstrated  flaounas et al showing  different topics  different gender biases  levels  readability  possibility  detect mood shifts   vast population  analysing twitter content  demonstrated  well software text mining computer programs  available  many commercial  open source companies  sources see list  text mining software intellectual property law situation  europe    lack  flexibilities  european copyright  database law  mining  incopyright works   web mining without  permission   copyright owner  illegal   uk    recommendation   hargreaves review  government amended copyright law  allow text mining   limitation  exception     second country   world    following japan  introduced  miningspecific exception  however owing   restriction   copyright directive  uk exception  allows content mining  noncommercial purposes uk copyright law   allow  provision   overridden  contractual terms  conditions  european commission facilitated stakeholder discussion  text  data mining    title  licences  europe  fact   focus   solution   legal issue  licences   limitations  exceptions  copyright law led representatives  universities researchers libraries civil society groups  open access publishers  leave  stakeholder dialogue  may situation   united states  contrast  europe  flexible nature  us copyright law   particular fair use means  text mining  america  well   fair use countries   israel taiwan  south korea  viewed   legal  text mining  transformative meaning     supplant  original work   viewed   lawful  fair use  example  part   google book settlement  presiding judge   case ruled  googles digitisation project  incopyright books  lawful  part    transformative uses   digitisation project displayedone  use  text  data mining implications  recently websites  often used textbased searches   found documents containing specific userdefined words  phrases now  use   semantic web text mining can find content based  meaning  context rather  just   specific word additionally text mining software can  used  build large dossiers  information  specific people  events  example large datasets based  data extracted  news reports can  built  facilitate social networks analysis  counterintelligence  effect  text mining software may act   capacity similar   intelligence analyst  research librarian albeit    limited scope  analysis text mining  also used   email spam filters   way  determining  characteristics  messages   likely   advertisements   unwanted material text mining plays  important role  determining financial market sentiment\r\n"}
{"index":{"_id":122}}
{"conceptLabelTag":"minimum description length","conceptLabel":"minimum description length","conceptDescription":"minimum description length  minimum description length mdl principle   formalization  occams razor    best hypothesis   given set  data   one  leads   best compression   data mdl  introduced  jorma rissanen     important concept  information theory  computational learning theory overview  set  data can  represented   string  symbols   finite say binary alphabet  select  hypothesis  captures   regularity   data scientists look   hypothesis    best compression can  achieved  order     code  fixed  compress  data  generally   turingcomplete computer language  program  output  data  written   language thus  program effectively represents  data  length   shortest program  outputs  data  called  kolmogorov complexity   data    central idea  ray solomonoffs idealized theory  inductive inference inference however  mathematical theory   provide  practical way  reaching  inference   important reasons    mdl attempts  remedy   rather  programs  mdl theory one usually speaks  candidate hypotheses models  codes  set  allowed codes   called  model class  authors refer   model class   model  code   selected    sum   description   code   description   data using  code  minimal one   important properties  mdl methods    provide  natural safeguard  overfitting   implement  tradeoff   complexity   hypothesis model class   complexity   data given  hypothesis  illustration  given   following example example  mdl  coin  flipped times   numbers  heads  tails  recorded consider two model classes   reason  naive statistical method might choose  second model   better explanation   data however  mdl approach  construct  single code based   hypothesis instead  just using  best one      simplest  use  twopart code    element   model class   best performance  specified   data  specified using  code  lot  bits  needed  specify  code  use thus  total codelength based   second model class   larger  bits therefore  conclusion  following  mdl approach  inevitably     enough evidence  support  hypothesis   biased coin even though  best element   second model class provides better fit   data mdl notation central  mdl theory   onetoone correspondence  code length functions  probability distributions  follows   kraftmcmillan inequality   probability distribution formula   possible  construct  code formula    length  bits  formula  equal  formula  code minimizes  expected code length vice versa given  code formula one can construct  probability distribution formula     holds rounding issues  ignored    words searching   efficient code reduces  searching   good probability distribution  vice versa related concepts mdl   strongly connected  probability theory  statistics   correspondence  codes  probability distributions mentioned    led  researchers  view mdl  equivalent  bayesian inference code length  model  data together  mdl correspond  prior probability  marginal likelihood respectively   bayesian framework  bayesian machinery  often useful  constructing efficient mdl codes  mdl framework also accommodates  codes    bayesian  example   shtarkov normalized maximum likelihood code  plays  central role  current mdl theory    equivalent  bayesian inference furthermore rissanen stresses    make  assumptions   true data generating process  practice  model class  typically  simplification  reality  thus   contain  code  probability distribution   true   objective sense   last mentioned reference rissanen bases  mathematical underpinning  mdl   kolmogorov structure function according   mdl philosophy bayesian methods   dismissed    based  unsafe priors   lead  poor results  priors   acceptable   mdl point  view also tend   favored  socalled objective bayesian analysis  however  motivation  usually different  systems mdl    first informationtheoretic approach  learning  early  wallace  boulton pioneered  related concept called minimum message length mml  difference  mdl  mml   source  ongoing confusion superficially  methods appear mostly equivalent     significant differences especially  interpretation\r\n"}
{"index":{"_id":123}}
{"conceptLabelTag":"mutual information","conceptLabel":"mutual information","conceptDescription":"mutual information  probability theory  information theory  mutual information mi  two random variables   measure   mutual dependence   two variables  specifically  quantifies  amount  information  units   bits obtained  one random variable    random variable  concept  mutual information  intricately linked    entropy   random variable  fundamental notion  information theory  defines  amount  information held   random variable  limited  realvalued random variables like  correlation coefficient mi   general  determines  similar  joint distribution pxy    products  factored marginal distribution pxpy mi   expected value   pointwise mutual information pmi   common unit  measurement  mutual information   bit definition formally  mutual information  two discrete random variables x  y can  defined   pxy   joint probability distribution function  x  y  formula  formula   marginal probability distribution functions  x  y respectively   case  continuous random variables  summation  replaced   definite double integral  pxy  now  joint probability density function  x  y  formula  formula   marginal probability density functions  x  y respectively   log base  used  units  mutual information   bit intuitively mutual information measures  information  x  y share  measures  much knowing one   variables reduces uncertainty     example  x  y  independent  knowing x   give  information  y  vice versa   mutual information  zero    extreme  x   deterministic function  y  y   deterministic function  x   information conveyed  x  shared  y knowing x determines  value  y  vice versa   result   case  mutual information      uncertainty contained  y  x alone namely  entropy  y  x moreover  mutual information      entropy  x    entropy  y   special case     x  y    random variable mutual information   measure   inherent dependence expressed   joint distribution  x  y relative   joint distribution  x  y   assumption  independence mutual information therefore measures dependence   following sense ix y     x  y  independent random variables   easy  see  one direction  x  y  independent  pxy px py  therefore moreover mutual information  nonnegative ie ixy see   symmetric ie ixy iyx relation   quantities mutual information can  equivalently expressed   formula  formula   marginal entropies hxy  hyx   conditional entropies  hxy   joint entropy  x  y note  analogy   union difference  intersection  two sets  illustrated   venn diagram using jensens inequality   definition  mutual information  can show  ixy  nonnegative consequently formula   give  detailed deduction  ixy hy hyx  proofs    identities   similar intuitively  entropy hy  regarded   measure  uncertainty   random variable  hyx   measure   x   say  y    amount  uncertainty remaining  y  x  known  thus  right side   first   equalities can  read   amount  uncertainty  y minus  amount  uncertainty  y  remains  x  known   equivalent   amount  uncertainty  y   removed  knowing x  corroborates  intuitive meaning  mutual information   amount  information   reduction  uncertainty  knowing either variable provides    note    discrete case hxx  therefore hx ixx thus ixx ixy  one can formulate  basic principle   variable contains  least  much information      variable can provide mutual information can also  expressed   kullbackleibler divergence   product px py   marginal distributions   two random variables x  y  pxy  random variables joint distribution furthermore let pxy px y py  note    kullbackleibler divergence involves integration  respect   random variable x    expression formula  now  random variable  y thus mutual information can also  understood   expectation   kullbackleibler divergence   univariate distribution px  x   conditional distribution pxy  x given y   different  distributions pxy  px   average  greater  information gain variations several variations  mutual information   proposed  suit various needs among   normalized variants  generalizations    two variables metric many applications require  metric    distance measure  pairs  points  quantity satisfies  properties   metric triangle inequality nonnegativity indiscernability  symmetry  distance metric  also known   variation  information  formula  discrete random variables    entropy terms  nonnegative  formula  one can define  normalized distance  metric d   universal metric      distance measure places x  y closeby   d will also judge  close plugging   definitions shows    settheoretic interpretation  information see  figure  conditional entropy   effectively  jaccard distance  x  y finally  also  metric conditional mutual information sometimes   useful  express  mutual information  two random variables conditioned   third  can  simplified  conditioning   third random variable may either increase  decrease  mutual information    always true   discrete jointly distributed random variables x y z  result   used   basic building block  proving  inequalities  information theory multivariate mutual information several generalizations  mutual information    two random variables   proposed   total correlation  interaction information  shannon entropy  viewed   signed measure   context  information diagrams  explained   article information theory  measure theory    definition  multivariate mutual information  makes sense   follows   formula     define applications applying information diagrams blindly  derive   definition   criticised  indeed   found rather limited practical application since   difficult  visualize  grasp  significance   quantity   large number  random variables  can  zero positive  negative   odd number  variables formula one highdimensional generalization scheme  maximizes  mutual information   joint distribution   target variables  found   useful  feature selection mutual information  also used   area  signal processing   measure  similarity  two signals  example fmi metric   image fusion performance measure  makes use  mutual information  order  measure  amount  information   fused image contains   source images  matlab code   metric can  found  directed information directed information formula measures  amount  information  flows   process formula  formula  formula denotes  vector formula  formula denotes formula  term directed information  coined  james massey   defined  note   formula  directed information becomes  mutual information directed information  many applications  problems  causality plays  important role   capacity  channel  feedback normalized variants normalized variants   mutual information  provided   coefficients  constraint uncertainty coefficient  proficiency  two coefficients   necessarily equal   cases  symmetric measure may  desired    following redundancy measure  attains  minimum  zero   variables  independent   maximum value   one variable becomes completely redundant   knowledge    see also redundancy information theory another symmetrical measure   symmetric uncertainty given   represents  harmonic mean   two uncertainty coefficients formula   consider mutual information   special case   total correlation  dual total correlation  normalized version  respectively  normalized version also known  information quality ratio iqr  quantifies  amount  information   variable based  another variable  total uncertainty theres  normalization  derives  first thinking  mutual information   analogue  covariance thus shannon entropy  analogous  variance   normalized mutual information  calculated akin   pearson correlation coefficient weighted variants   traditional formulation   mutual information  event  object specified  formula  weighted   corresponding probability formula  assumes   objects  events  equivalent apart   probability  occurrence however   applications  may   case  certain objects  events   significant  others   certain patterns  association   semantically important  others  example  deterministic mapping formula may  viewed  stronger   deterministic mapping formula although  relationships  yield   mutual information     mutual information   sensitive     inherent ordering   variable values   therefore  sensitive     form   relational mapping   associated variables    desired   former relationshowing agreement   variable valuesbe judged stronger   later relation    possible  use  following weighted mutual information  places  weight formula   probability   variable value cooccurrence formula  allows  certain probabilities may carry   less significance  others thereby allowing  quantification  relevant holistic  pr gnanz factors    example using larger relative weights  formula formula  formula    effect  assessing greater informativeness   relation formula    relation formula  may  desirable   cases  pattern recognition   like  weighted mutual information   form  weighted kldivergence   known  take negative values   inputs    examples   weighted mutual information also takes negative values adjusted mutual information  probability distribution can  viewed   partition   set one may  ask   set  partitioned randomly    distribution  probabilities     expectation value   mutual information   adjusted mutual information  ami subtracts  expectation value   mi    ami  zero  two different distributions  random  one  two distributions  identical  ami  defined  analogy   adjusted rand index  two different partitions   set absolute mutual information using  ideas  kolmogorov complexity one can consider  mutual information  two sequences independent   probability distribution  establish   quantity  symmetric    logarithmic factor formula requires  chain rule  kolmogorov complexity approximations   quantity via compression can  used  define  distance measure  perform  hierarchical clustering  sequences without   domain knowledge   sequences linear correlation unlike correlation coefficients    product moment correlation coefficient mutual information contains information   dependencelinear  nonlinearand  just linear dependence   correlation coefficient measures however   narrow case   marginal distributions  x  y  normally distributed   joint distribution   bivariate normal distribution    exact relationship     correlation coefficient formula  discrete data  x  y  limited     discrete number  states observation data  summarized   contingency table  row variable x    column variable y  j mutual information  one   measures  association  correlation   row  column variables  measures  association include pearsons chisquared test statistics gtest statistics etc  fact mutual information  equal  gtest statistics divided  n  n   sample size   special case   number  states   row  column variables  ij  degrees  dom   pearsons chisquared test     four terms   summation  one  independent    reason  mutual information function   exact relationship   correlation function formula  binary sequences applications  many applications one wants  maximize mutual information thus increasing dependencies   often equivalent  minimizing conditional entropy examples include\r\n"}
{"index":{"_id":124}}
{"conceptLabelTag":"binary classification","conceptLabel":"binary classification","conceptDescription":"binary classification binary  binomial classification   task  classifying  elements   given set  two groups   basis   classification rule instancing  decision whether  item     qualitative property  specified characteristic  typical binary classification tasks  binary classification  dichotomization applied  practical purposes  therefore  important point    many practical binary classification problems  two groups   symmetric rather  overall accuracy  relative proportion  different types  errors   interest  example  medical testing  false positive detecting  disease     present  considered differently   false negative  detecting  disease    present porting human discriminative abilities  scientific soundness  technical practice  far  trivial statistical binary classification statistical classification   problem studied  machine learning    type  supervised learning  method  machine learning   categories  predefined   used  categorize new probabilistic observations  said categories     two categories  problem  known  statistical binary classification    methods commonly used  binary classification   classifier  best    select domain based upon  number  observations  dimensionality   feature vector  noise   data  many  factors  example random forests perform better  svm classifiers  d point clouds evaluation  binary classifiers   many metrics  can  used  measure  performance   classifier  predictor different fields  different preferences  specific metrics due  different goals  example  medicine sensitivity  specificity  often used   information retrieval precision  recall  preferred  important distinction   metrics   independent   prevalence  often  category occurs   population  metrics  depend   prevalence  types  useful     different properties given  classification   specific data set   four basic data  number  true positives tp true negatives tn false positives fp  false negatives fn  can  arranged   contingency table  columns corresponding  actual value condition positive cp  condition negative cn  rows corresponding  classification value test outcome positive  test outcome negative   eight basic ratios  one can compute   table  come  four complementary pairs  pair summing    obtained  dividing    four numbers   sum   row  column yielding eight numbers  can  referred  generically   form true positive row ratio  false negative column ratio though   conventional terms   thus two pairs  column ratios  two pairs  row ratios  one can summarize   four numbers  choosing one ratio   pair   four numbers   complements  column ratios  true positive rate tpr aka sensitivity  recall  complement  false negative rate fnr  true negative rate tnr aka specificity spc  complement false positive rate fpr    proportion   population   condition resp without  condition    test  correct  complementarily    test  incorrect   independent  prevalence  row ratios  positive predictive value ppv aka precision  complement  false discovery rate fdr  negative predictive value npv  complement  false omission rate     proportion   population   given test result    test  correct  complementarily    test  incorrect  depend  prevalence  diagnostic testing  main ratios used   true column ratios true positive rate  true negative rate    known  sensitivity  specificity  informational retrieval  main ratios   true positive ratios row  column positive predictive value  true positive rate    known  precision  recall one can take ratios   complementary pair  ratios yielding four likelihood ratios two column ratio  ratios two row ratio  ratios   primarily done   column condition ratios yielding likelihood ratios  diagnostic testing taking  ratio  one   groups  ratios yields  final ratio  diagnostic odds ratio dor  can also  defined directly  tptnfpfn tpfnfptn    useful interpretation   odds ratio   prevalenceindependent    number   metrics  simply  accuracy  fraction correct fc  measures  fraction   instances   correctly categorized  complement   fraction incorrect fic  fscore combines precision  recall  one number via  choice  weighing  simply equal weighing   balanced fscore f score  metrics come  regression coefficients  markedness   informedness   geometric mean  matthews correlation coefficient  metrics include youdens j statistic  uncertainty coefficient  phi coefficient  cohens kappa converting continuous values  binary tests whose results   continuous values    blood values can artificially  made binary  defining  cutoff value  test results  designated  positive  negative depending  whether  resultant value  higher  lower   cutoff however  conversion causes  loss  information   resultant binary classification   tell  much     cutoff  value    result  converting  continuous value   close   cutoff   binary one  resultant positive  negative predictive value  generally higher   predictive value given directly   continuous value   cases  designation   test   either positive  negative gives  appearance   inappropriately high certainty   value   fact   interval  uncertainty  example   urine concentration  hcg   continuous value  urine pregnancy test  measured miuml  hcg may show  positive  miuml  cutoff    fact   interval  uncertainty  may  apparent   knowing  original continuous value    hand  test result  far   cutoff generally   resultant positive  negative predictive value   lower   predictive value given   continuous value  example  urine hcg value  miuml confers   high probability  pregnancy  conversion  binary values results    shows just  positive   one  miuml\r\n"}
{"index":{"_id":125}}
{"conceptLabelTag":"receiver operating characteristic","conceptLabel":"receiver operating characteristic","conceptDescription":"receiver operating characteristic  statistics  receiver operating characteristic curve  roc curve   graphical plot  illustrates  performance   binary classifier system   discrimination threshold  varied  curve  created  plotting  true positive rate tpr   false positive rate fpr  various threshold settings  truepositive rate  also known  sensitivity recall  probability  detection  machine learning  falsepositive rate  also known   fallout  probability  false alarm  can  calculated  specificity  roc curve  thus  sensitivity   function  fallout  general   probability distributions   detection  false alarm  known  roc curve can  generated  plotting  cumulative distribution function area   probability distribution  formula   discrimination threshold   detection probability   yaxis versus  cumulative distribution function   falsealarm probability  xaxis roc analysis provides tools  select possibly optimal models   discard suboptimal ones independently   prior  specifying  cost context   class distribution roc analysis  related   direct  natural way  costbenefit analysis  diagnostic decision making  roc curve  first developed  electrical engineers  radar engineers  world war ii  detecting enemy objects  battlefields   soon introduced  psychology  account  perceptual detection  stimuli roc analysis since    used  medicine radiology biometrics   areas  many decades   increasingly used  machine learning  data mining research  roc  also known   relative operating characteristic curve     comparison  two operating characteristics tpr  fpr   criterion changes basic concept  classification model classifier  diagnosis   mapping  instances  certain classesgroups  classifier  diagnosis result can   real value continuous output   case  classifier boundary  classes must  determined   threshold value  instance  determine whether  person  hypertension based   blood pressure measure   can   discrete class label indicating one   classes let us consider  twoclass prediction problem binary classification    outcomes  labeled either  positive p  negative n   four possible outcomes   binary classifier   outcome   prediction  p   actual value  also p    called  true positive tp however   actual value  n    said    false positive fp conversely  true negative tn  occurred    prediction outcome   actual value  n  false negative fn    prediction outcome  n   actual value  p  get  appropriate example   realworld problem consider  diagnostic test  seeks  determine whether  person   certain disease  false positive   case occurs   person tests positive  actually     disease  false negative    hand occurs   person tests negative suggesting   healthy   actually    disease let us define  experiment  p positive instances  n negative instances   condition  four outcomes can  formulated   contingency table  confusion matrix  follows roc space  contingency table can derive several evaluation metrics see infobox  draw  roc curve   true positive rate tpr  false positive rate fpr  needed  functions   classifier parameter  tpr defines  many correct positive results occur among  positive samples available   test fpr    hand defines  many incorrect positive results occur among  negative samples available   test  roc space  defined  fpr  tpr  x  y axes respectively  depicts relative tradeoffs  true positive benefits  false positive costs since tpr  equivalent  sensitivity  fpr  equal  specificity  roc graph  sometimes called  sensitivity vs specificity plot  prediction result  instance   confusion matrix represents one point   roc space  best possible prediction method  yield  point   upper left corner  coordinate   roc space representing sensitivity  false negatives  specificity  false positives  point  also called  perfect classification  completely random guess  give  point along  diagonal line  socalled line  nodiscrimination   left bottom   top right corners regardless   positive  negative base rates  intuitive example  random guessing   decision  flipping coins heads  tails   size   sample increases  random classifiers roc point migrates towards  diagonal divides  roc space points   diagonal represent good classification results better  random points   line represent poor results worse  random note   output   consistently poor predictor  simply  inverted  obtain  good predictor let us look  four prediction results  positive  negative instances plots   four results    roc space  given   figure  result  method  clearly shows  best predictive power among  b  c  result  b lies   random guess line  diagonal line   can  seen   table   accuracy  b  however  c  mirrored across  center point  resulting method c  even better    mirrored method simply reverses  predictions  whatever method  test produced  c contingency table although  original c method  negative predictive power simply reversing  decisions leads   new predictive method c   positive predictive power   c method predicts p  n  c method  predict n  p respectively   manner  c test  perform  best  closer  result   contingency table    upper left corner  better  predicts   distance   random guess line  either direction   best indicator   much predictive power  method    result    line ie  method  worse   random guess    methods predictions must  reversed  order  utilize  power thereby moving  result   random guess line curves  roc space  binary classification  class prediction   instance  often made based   continuous random variable formula    score computed   instance eg estimated probability  logistic regression given  threshold parameter formula  instance  classified  positive  formula  negative otherwise formula follows  probability density formula   instance actually belongs  class positive  formula  otherwisetherefore  true positive rate  given  formula   false positive rate  given  formula  roc curve plots parametrically tprt versus fprt  t   varying parameter  example imagine   blood protein levels  diseased people  healthy people  normally distributed  means  gdl  gdl respectively  medical test might measure  level   certain protein   blood sample  classify  number   certain threshold  indicating disease  experimenter can adjust  threshold black vertical line   figure  will  turn change  false positive rate increasing  threshold  result  fewer false positives   false negatives corresponding   leftward movement   curve  actual shape   curve  determined   much overlap  two distributions   concepts  demonstrated   receiver operating characteristic roc curves applet  interpretations sometimes  roc  used  generate  summary statistic common versions  however  attempt  summarize  roc curve   single number loses information   pattern  tradeoffs   particular discriminator algorithm area   curve  using normalized units  area   curve often referred   simply  auc  equal   probability   classifier will rank  randomly chosen positive instance higher   randomly chosen negative one assuming positive ranks higher  negative  can  seen  follows  area   curve  given   integral boundaries  reversed  large t   lower value   xaxis  formula   score   positive instance  formula   score   negative instance  can   shown   auc  closely related   mannwhitney u  tests whether positives  ranked higher  negatives   also equivalent   wilcoxon test  ranks  auc  related   gini coefficient formula   formula formula    way   possible  calculate  auc  using  average   number  trapezoidal approximations   also common  calculate  area   roc convex hull roc auch roch auc   point   line segment  two prediction results can  achieved  randomly using one   system  probabilities proportional   relative length   opposite component   segment interestingly   also possible  invert concavities just    figure  worse solution can  reflected  become  better solution concavities can  reflected   line segment    extreme form  fusion  much  likely  overfit  data  machine learning community  often uses  roc auc statistic  model comparison however  practice  recently  questioned based upon new machine learning research  shows   auc  quite noisy   classification measure     significant problems  model comparison  reliable  valid auc estimate can  interpreted   probability   classifier will assign  higher score   randomly chosen positive example    randomly chosen negative example however  critical research suggests frequent failures  obtaining reliable  valid auc estimates thus  practical value   auc measure   called  question raising  possibility   auc may actually introduce  uncertainty  machine learning classification accuracy comparisons  resolution nonetheless  coherence  auc   measure  aggregated classification performance   vindicated  terms   uniform rate distribution  auc   linked   number   performance metrics    brier score one recent explanation   problem  roc auc   reducing  roc curve   single number ignores  fact      tradeoffs   different systems  performance points plotted    performance   individual system  well  ignoring  possibility  concavity repair   related alternative measures   informedness  deltap  recommended  measures  essentially equivalent   gini   single prediction point  deltap informedness auc whilst deltap markedness represents  dual viz predicting  prediction   real class   geometric mean   matthews correlation coefficient  measures  engineering  area   roc curve   nodiscrimination line  sometimes preferred equivalent  subtracting   auc  referred    discrimination  psychophysics  sensitivity index d dprime p  deltap    commonly used measure    direct monotonic relationship   discrimination  discrimination  equal also  informedness deskewed wracc  gini coefficient   single point case single parameterization  single system  measures    advantage  represents chance performance whilst represents perfect performance  represents  perverse case  full informedness used  always give  wrong response  varying choices  scale  fairly arbitrary since chance performance always   fixed value  auc     alternative scales bring chance performance   allow    interpreted  kappa statistics informedness   shown   desirable characteristics  machine learning versus  common definitions  kappa   cohen kappa  fleiss kappa sometimes  can   useful  look   specific region   roc curve rather    whole curve   possible  compute partial auc  example one  focus   region   curve  low false positive rate   often  prime interest  population screening tests another common approach  classification problems   p n common  bioinformatics applications   use  logarithmic scale   xaxis detection error tradeoff graph  alternative   roc curve   detection error tradeoff det graph  plots  false negative rate missed detections vs  false positive rate false alarms  nonlinearly transformed x  yaxes  transformation function   quantile function   normal distribution ie  inverse   cumulative normal distribution    fact   transformation  zroc  except   complement   hit rate  miss rate  false negative rate  used  alternative spends  graph area   region  interest    roc area   little interest one primarily cares   region tight   yaxis   top left corner    using miss rate instead   complement  hit rate   lower left corner   det plot furthermore det graphs   useful property  linearity   linear threshold behavior  normal distributions  det plot  used extensively   automatic speaker recognition community   name det  first used  analysis   roc performance  graphs   warping   axes  used  psychologists  perception studies halfway  th century    dubbed double probability paper zscore   standard score  applied   roc curve  curve will  transformed   straight line  zscore  based   normal distribution   mean  zero   standard deviation  one  memory strength theory one must assume   zroc    linear    slope   normal distributions  targets studied objects   subjects need  recall  lures non studied objects   subjects attempt  recall   factor causing  zroc   linear  linearity   zroc curve depends   standard deviations   target  lure strength distributions   standard deviations  equal  slope will    standard deviation   target strength distribution  larger   standard deviation   lure strength distribution   slope will  smaller    studies    found   zroc curve slopes constantly fall  usually   many experiments yielded  zroc slope   slope  implies   variability   target strength distribution  larger   variability   lure strength distribution another variable used  d d prime discussed    measures  can easily  expressed  terms  zvalues although d   commonly used parameter  must  recognized     relevant  strictly adhering    strong assumptions  strength theory made   zscore   roc curve  always linear  assumed except  special situations  yonelinas familiarityrecollection model   twodimensional account  recognition memory instead   subject simply answering yes     specific input  subject gives  input  feeling  familiarity  operates like  original roc curve  changes though   parameter  recollection r recollection  assumed   allornone   trumps familiarity     recollection component zroc    predicted slope  however  adding  recollection component  zroc curve will  concave    decreased slope  difference  shape  slope result   added element  variability due   items  recollected patients  anterograde amnesia  unable  recollect   yonelinas zroc curve    slope close  history  roc curve  first used  world war ii   analysis  radar signals    employed  signal detection theory following  attack  pearl harbor   united states army began new research  increase  prediction  correctly detected japanese aircraft   radar signals   s roc curves  employed  psychophysics  assess human  occasionally nonhuman animal detection  weak signals  medicine roc analysis   extensively used   evaluation  diagnostic tests roc curves  also used extensively  epidemiology  medical research   frequently mentioned  conjunction  evidencebased medicine  radiology roc analysis   common technique  evaluate new radiology techniques   social sciences roc analysis  often called  roc accuracy ratio  common technique  judging  accuracy  default probability models roc curves  widely used  laboratory medicine  assess diagnostic accuracy   test  choose  optimal cutoff   test   compare diagnostic accuracy  several tests roc curves also proved useful   evaluation  machine learning techniques  first application  roc  machine learning   spackman  demonstrated  value  roc curves  comparing  evaluating different classification algorithms roc curves beyond binary classification  extension  roc curves  classification problems    two classes  always  cumbersome   degrees  dom increase quadratically   number  classes   roc space  formula dimensions  formula   number  classes  approaches   made   particular case  three classes threeway roc  calculation   volume   roc surface vus   analyzed  studied   performance metric  multiclass problems however    complexity  approximating  true vus   approaches based   extension  auc   popular   evaluation metric given  success  roc curves   assessment  classification models  extension  roc curves   supervised tasks  also  investigated notable proposals  regression problems   socalled regression error characteristic rec curves   regression roc rroc curves   latter rroc curves become extremely similar  roc curves  classification   notions  asymmetry dominance  convex hull also  area  rroc curves  proportional   error variance   regression model\r\n"}
{"index":{"_id":126}}
{"conceptLabelTag":"LogitBoost","conceptLabel":"LogitBoost","conceptDescription":"margin classifier  machine learning  margin classifier   classifier   able  give  associated distance   decision boundary   example  instance   linear classifier eg perceptron  linear discriminant analysis  used  distance typically euclidean distance though others may  used   example   separating hyperplane   margin   example  notion  margin  important  several machine learning classification algorithms   can  used  bound  generalization error   classifier  bounds  frequently shown using  vc dimension  particular prominence   generalization error bound  boosting algorithms  support vector machines support vector machine definition  margin see support vector machines  maximummargin hyperplane  details margin  boosting algorithms  margin   iterative boosting algorithm given  set  examples  two classes can  defined  follows  classifier  given  example pair formula  formula   domain space  formula   label   example  iterative boosting algorithm  selects  classifier formula   iteration formula  formula   space  possible classifiers  predict real values  hypothesis   weighted  formula  selected   boosting algorithm  iteration formula  margin   example formula can thus  defined    definition  margin  positive   example  labeled correctly  negative   example  labeled incorrectly  definition may  modified      way  define margin  boosting algorithms however   reasons   definition may  appealing examples  marginbased algorithms many classifiers can give  associated margin   example however   classifiers utilize information   margin  learning   data set many boosting algorithms rely   notion   margin  give weights  examples   convex loss  utilized   adaboost logitboost   members   anyboost family  algorithms   example  higher margin will receive less  equal weight   example  lower margin  leads  boosting algorithm  focus weight  low margin examples  nonconvex algorithms eg brownboost  margin still dictates  weighting   example though  weighting  nonmonotone  respect  margin  exists boosting algorithms  provably maximize  minimum margin eg see support vector machines provably maximize  margin   separating hyperplane support vector machines   trained using noisy data  exists  perfect separation   data   given space maximize  soft margin  discussion   can  found   support vector machine article  votedperceptron algorithm   margin maximizing algorithm based   iterative application   classic perceptron algorithm generalization error bounds one theoretical motivation behind margin classifiers    generalization error may  bound  parameters   algorithm   margin term  example    bound    adaboost algorithm let formula   set  formula examples sampled independently  random   distribution formula assume  vcdimension   underlying base classifier  formula  formula   probability formula    bound   formula\r\n"}
{"index":{"_id":127}}
{"conceptLabelTag":"gaussian process","conceptLabel":"gaussian process","conceptDescription":"gaussian process  probability theory  statistics  gaussian process   statistical model  observations occur   continuous domain eg time  space   gaussian process every point   continuous input space  associated   normally distributed random variable moreover every finite collection   random variables   multivariate normal distribution  distribution   gaussian process   joint distribution    infinitely many random variables       distribution  functions   continuous domain eg time  space viewed   machinelearning algorithm  gaussian process uses lazy learning   measure   similarity  points    kernel function  predict  value   unseen point  training data  prediction   just  estimate   point  also  uncertainty information    onedimensional gaussian distribution    marginal distribution   point   kernel functions matrix algebra can  used  calculate  predictions  described   kriging article   parameterised kernel  used optimisation software  typically used  fit  gaussian process model  concept  gaussian processes  named  carl friedrich gauss    based   notion   gaussian distribution normal distribution gaussian processes can  seen   infinitedimensional generalization  multivariate normal distributions gaussian processes  useful  statistical modelling benefiting  properties inherited   normal  example   random process  modelled   gaussian process  distributions  various derived quantities can  obtained explicitly  quantities include  average value   process   range  times   error  estimating  average using sample values   small set  times definition  gaussian process   statistical distribution x t t    finite linear combination  samples   joint gaussian distribution  accurately  linear functional applied   sample function x will give  normally distributed result notationwise one can write x gpmk meaning  random function x  distributed   gp  mean function m  covariance function k   input vector t  two  multidimensional  gaussian process might  also known   gaussian random field  authors assume  random variables x  mean zero  simplifies calculations without loss  generality  allows  mean square properties   process   entirely determined   covariance function k alternative definitions alternatively  time continuous stochastic process  gaussian      every finite set  indices formula   index set formula   multivariate gaussian random variable using characteristic functions  random variables  gaussian property can  formulated  follows formula  gaussian      every finite set  indices formula   real valued formula formula  formula    following equality holds   formula  formula denotes  imaginary number formula  numbers formula  formula can  shown    covariances  means   variables   process covariance functions  key fact  gaussian processes    can  completely defined   secondorder statistics thus   gaussian process  assumed   mean zero defining  covariance function completely defines  process behaviour importantly  nonnegative definiteness   function enables  spectral decomposition using  karhunenloeve expansion basic aspects  can  defined   covariance function   process stationarity isotropy smoothness  periodicity stationarity refers   process behaviour regarding  separation   two points x  x   process  stationary  depends   separation x x   nonstationary  depends   actual position   points x  x  example  special case   ornsteinuhlenbeck process  brownian motion process  stationary   process depends   x x  euclidean distance   direction  x  x   process  considered isotropic  process   concurrently stationary  isotropic  considered   homogeneous  practice  properties reflect  differences  rather  lack     behaviour   process given  location   observer ultimately gaussian processes translate  taking priors  functions   smoothness   priors can  induced   covariance function   expect   nearby input points x  x  corresponding output points y  y   nearby also   assumption  continuity  present   wish  allow  significant displacement   might choose  rougher covariance function extreme examples   behaviour   ornsteinuhlenbeck covariance function   squared exponential   former  never differentiable   latter infinitely differentiable periodicity refers  inducing periodic patterns within  behaviour   process formally   achieved  mapping  input x   two dimensional vector ux cosx sinx usual covariance functions    number  common covariance functions  formula  parameter formula   characteristic lengthscale   process practically  close two points formula  formula     influence   significantly   kronecker delta   standard deviation   noise fluctuations moreover formula   modified bessel function  order formula  formula   gamma function evaluated  formula importantly  complicated covariance function can  defined   linear combination   simpler covariance functions  order  incorporate different insights   dataset  hand clearly  inferential results  dependent   values   hyperparameters eg formula  defining  models behaviour  popular choice    provide maximum  posteriori map estimates     chosen prior   prior   near uniform      maximizing  marginal likelihood   process  marginalization  done   observed process values formula  approach  also known  maximum likelihood ii evidence maximization  empirical bayes brownian motion   integral  gaussian processes  wiener process aka brownian motion   integral   white noise gaussian process    stationary    stationary increments  ornsteinuhlenbeck process   stationary gaussian process  brownian bridge   integral   gaussian process whose increments   independent  fractional brownian motion   integral   gaussian process whose covariance function   generalisation  wiener process applications  gaussian process can  used   prior probability distribution  functions  bayesian inference given  set  n points   desired domain   functions take  multivariate gaussian whose covariance matrix parameter   gram matrix   n points   desired kernel  sample   gaussian inference  continuous values   gaussian process prior  known  gaussian process regression  kriging extending gaussian process regression  multiple target variables  known  cokriging gaussian processes  thus useful   powerful nonlinear multivariate interpolation tool gaussian process regression can   extended  address learning tasks   supervised eg probabilistic classification  unsupervised eg manifold learning learning frameworks gaussian process prediction  kriging  concerned   general gaussian process regression problem kriging   assumed    gaussian process f observed  coordinates x  vector  values  just one sample   multivariate gaussian distribution  dimension equal  number  observed coordinates x therefore   assumption   zeromean distribution    covariance matrix   possible pairs   given set  hyperparameters    log marginal likelihood   maximizing  marginal likelihood towards provides  complete specification   gaussian process f one can briefly note   point   first term corresponds   penalty term   models failure  fit observed values   second term   penalty term  increases proportionally   models complexity  specified making predictions  unobserved values  coordinates x     matter  drawing samples   predictive distribution formula   posterior mean estimate   defined    posterior variance estimate b  defined     covariance   new coordinate  estimation x    observed coordinates x   given hyperparameter vector   defined      variance  point x  dictated    important  note  practically  posterior mean estimate  point estimate  just  linear combination   observations   similar manner  variance   actually independent   observations  known bottleneck  gaussian process prediction    computational complexity  prediction  cubic   number  points x    can become unfeasible  larger data sets works  sparse gaussian processes  usually  based   idea  building  representative set   given process f try  circumvent  issue\r\n"}
{"index":{"_id":128}}
{"conceptLabelTag":"inductive logic programming","conceptLabel":"inductive logic programming","conceptDescription":"inductive logic programming inductive logic programming ilp   subfield  machine learning  uses logic programming   uniform representation  examples background knowledge  hypotheses given  encoding   known background knowledge   set  examples represented   logical database  facts  ilp system will derive  hypothesised logic program  entails   positive  none   negative examples inductive logic programming  particularly useful  bioinformatics  natural language processing gordon plotkin  ehud shapiro laid  initial theoretical foundation  inductive machine learning   logical setting shapiro built  first implementation model inference system   prolog program  inductively inferred logic programs  positive  negative examples  term inductive logic programming  first introduced   paper  stephen muggleton  muggleton also founded  annual international conference  inductive logic programming introduced  theoretical ideas  predicate invention inverse resolution  inverse entailment muggleton implemented inverse entailment first   progol system  term inductive  refers  philosophical ie suggesting  theory  explain observed facts rather  mathematical ie proving  property   members   wellordered set induction formal definition  background knowledge  given   logic theory commonly   form  horn clauses used  logic programming  positive  negative examples  given   conjunction formula  formula  unnegated  negated ground literals respectively  correct hypothesis   logic proposition satisfying  following requirements necessity   impose  restriction   forbids  generation   hypothesis  long   positive facts  explainable without  sufficiency requires  generated hypothesis  explain  positive examples formula weak consistency forbids generation   hypothesis  contradicts  background knowledge strong consistency also forbids generation   hypothesis   inconsistent   negative examples formula given  background knowledge  implies weak consistency   negative examples  given  requirements coincide deroski requires  sufficiency called completeness   strong consistency example  following wellknown example  learning definitions  family relations uses  abbreviations  starts   background knowledge cf picture  positive examples   trivial proposition formula  denote  absence  negative examples plotkins relative least general generalization rlgg approach  inductive logic programming shall  used  obtain  suggestion    formally define  daughter relation formula  approach uses  following steps  resulting horn clause   hypothesis obtained   rlgg approach ignoring  background knowledge facts  clause informally reads formula  called  daughter  formula  formula   parent  formula  formula  female    commonly accepted definition concerning   requirements necessity  satisfied   predicate formula doesnt appear   background knowledge  hence  imply  property containing  predicate    positive examples  sufficiency  satisfied   computed hypothesis since  together  formula   background knowledge implies  first positive example formula  similarly  formula   background knowledge implies  second positive example formula weak consistency  satisfied  since holds   finite herbrand structure described   background knowledge similar  strong consistency  common definition   grandmother relation viz formula   learned using   approach since  variable occurs   clause body   corresponding literals    deleted   th step   approach  overcome  flaw  step    modified    can  parametrized  different literal postselection heuristics historically  golem implementation  based   rlgg approach inductive logic programming system inductive logic programming system   program  takes   input logic theories formula  outputs  correct hypothesis wrt theories formula  algorithm   ilp system consists  two parts hypothesis search  hypothesis selection first  hypothesis  searched   inductive logic programming procedure   subset   found hypotheses   systems one hypothesis  chosen   selection algorithm  selection algorithm scores    found hypotheses  returns  ones   highest score  example  score function include minimal compression length   hypothesis   lowest kolmogorov complexity   highest score   returned  ilp system  complete iff   input logic theories formula  correct hypothesis wrt   input theories can  found   hypothesis search procedure hypothesis search modern ilp systems like progol hail  imparo find  hypothesis using  principle   inverse entailment  theories formula first  construct  intermediate theory called  bridge theory satisfying  conditions formula  formula   formula  generalize  negation   bridge theory   antientailment however  operation   antientailment since  highly nondeterministic  computationally  expensive therefore  alternative hypothesis search can  conducted using  operation   inverse subsumption antisubsumption instead   less nondeterministic  antientailment questions  completeness   hypothesis search procedure  specific ilp system arise  example progols hypothesis search procedure based   inverse entailment inference rule   complete  yamamotos example    hand imparo  complete   antientailment procedure   extended inverse subsumption procedure\r\n"}
{"index":{"_id":129}}
{"conceptLabelTag":"semi supervised learning","conceptLabel":"semi supervised learning","conceptDescription":"semisupervised learning semisupervised learning   class  supervised learning tasks  techniques  also make use  unlabeled data  training typically  small amount  labeled data   large amount  unlabeled data semisupervised learning falls  unsupervised learning without  labeled training data  supervised learning  completely labeled training data many machinelearning researchers  found  unlabeled data  used  conjunction   small amount  labeled data can produce considerable improvement  learning accuracy  acquisition  labeled data   learning problem often requires  skilled human agent eg  transcribe  audio segment   physical experiment eg determining  d structure   protein  determining whether   oil   particular location  cost associated   labeling process thus may render  fully labeled training set infeasible whereas acquisition  unlabeled data  relatively inexpensive   situations semisupervised learning can   great practical value semisupervised learning  also  theoretical interest  machine learning    model  human learning    supervised learning framework   given  set  formula independently identically distributed examples formula  corresponding labels formula additionally   given formula unlabeled examples formula semisupervised learning attempts  make use   combined information  surpass  classification performance    obtained either  discarding  unlabeled data   supervised learning   discarding  labels   unsupervised learning semisupervised learning may refer  either transductive learning  inductive learning  goal  transductive learning   infer  correct labels   given unlabeled data formula   goal  inductive learning   infer  correct mapping  formula  formula intuitively  can think   learning problem   exam  labeled data    example problems   teacher solved  class  teacher also provides  set  unsolved problems   transductive setting  unsolved problems   takehome exam   want   well    particular   inductive setting   practice problems   sort  will encounter   inclass exam   unnecessary  according  vapniks principle imprudent  perform transductive learning  way  inferring  classification rule   entire input space however  practice algorithms formally designed  transduction  induction  often used interchangeably assumptions used  order  make  use  unlabeled data  must assume  structure   underlying distribution  data semisupervised learning algorithms make use   least one   following assumptions smoothness assumption points   close      likely  share  label  accurately    continuity assumption rather   smoothness assumption   also generally assumed  supervised learning  yields  preference  geometrically simple decision boundaries   case  semisupervised learning  smoothness assumption additionally yields  preference  decision boundaries  lowdensity regions     fewer points close      different classes cluster assumption  data tend  form discrete clusters  points    cluster   likely  share  label although data sharing  label may  spread across multiple clusters    special case   smoothness assumption  gives rise  feature learning  clustering algorithms manifold assumption  data lie approximately   manifold  much lower dimension   input space   case  can attempt  learn  manifold using   labeled  unlabeled data  avoid  curse  dimensionality  learning can proceed using distances  densities defined   manifold  manifold assumption  practical  highdimensional data   generated   process  may  hard  model directly       degrees  dom  instance human voice  controlled    vocal folds  images  various facial expressions  controlled    muscles   like   cases  use distances  smoothness   natural space   generating problem rather    space   possible acoustic waves  images respectively history  heuristic approach  selftraining also known  selflearning  selflabeling  historically  oldest approach  semisupervised learning  examples  applications starting   s see  instance scudder  transductive learning framework  formally introduced  vladimir vapnik   s interest  inductive learning using generative models also began   s  probably approximately correct learning bound  semisupervised learning   gaussian mixture  demonstrated  ratsaby  venkatesh  semisupervised learning  recently become  popular  practically relevant due   variety  problems   vast quantities  unlabeled data  availableeg text  websites protein sequences  images   review  recent work see  survey article  zhu methods generative models generative approaches  statistical learning first seek  estimate formula  distribution  data points belonging   class  probability formula   given point formula  label formula   proportional  formula  bayes rule semisupervised learning  generative models can  viewed either   extension  supervised learning classification plus information  formula    extension  unsupervised learning clustering plus  labels generative models assume   distributions take  particular form formula parameterized   vector formula   assumptions  incorrect  unlabeled data may actually decrease  accuracy   solution relative      obtained  labeled data alone however   assumptions  correct   unlabeled data necessarily improves performance  unlabeled data  distributed according   mixture  individualclass distributions  order  learn  mixture distribution   unlabeled data  must  identifiable   different parameters must yield different summed distributions gaussian mixture distributions  identifiable  commonly used  generative models  parameterized joint distribution can  written  formula  using  chain rule  parameter vector formula  associated   decision function formula  parameter   chosen based  fit    labeled  unlabeled data weighted  formula lowdensity separation another major class  methods attempts  place boundaries  regions     data points labeled  unlabeled one    commonly used algorithms   transductive support vector machine  tsvm  despite  name may  used  inductive learning  well whereas support vector machines  supervised learning seek  decision boundary  maximal margin   labeled data  goal  tsvm   labeling   unlabeled data    decision boundary  maximal margin     data  addition   standard hinge loss formula  labeled data  loss function formula  introduced   unlabeled data  letting formula tsvm  selects formula   reproducing kernel hilbert space formula  minimizing  regularized empirical risk  exact solution  intractable due   nonconvex term formula  research  focused  finding useful approximations  approaches  implement lowdensity separation include gaussian process models information regularization  entropy minimization   tsvm   special case graphbased methods graphbased methods  semisupervised learning use  graph representation   data   node   labeled  unlabeled example  graph may  constructed using domain knowledge  similarity  examples two common methods   connect  data point   formula nearest neighbors   examples within  distance formula  weight formula   edge  formula  formula   set  formula within  framework  manifold regularization  graph serves   proxy   manifold  term  added   standard tikhonov regularization problem  enforce smoothness   solution relative   manifold   intrinsic space   problem  well  relative   ambient input space  minimization problem becomes  formula   reproducing kernel hilbert space  formula   manifold    data lie  regularization parameters formula  formula control smoothness   ambient  intrinsic spaces respectively  graph  used  approximate  intrinsic regularization term defining  graph laplacian formula  formula  formula  vector formula    laplacian can also  used  extend  supervised learning algorithms regularized least squares  support vector machines svm  semisupervised versions laplacian regularized least squares  laplacian svm heuristic approaches  methods  semisupervised learning   intrinsically geared  learning   unlabeled  labeled data  instead make use  unlabeled data within  supervised learning framework  instance  labeled  unlabeled examples formula may inform  choice  representation distance metric  kernel   data   unsupervised first step  supervised learning proceeds    labeled examples selftraining   wrapper method  semisupervised learning first  supervised learning algorithm  trained based   labeled data   classifier   applied   unlabeled data  generate  labeled examples  input   supervised learning algorithm generally   labels  classifier   confident   added   step cotraining   extension  selftraining   multiple classifiers  trained  different ideally disjoint sets  features  generate labeled examples  one another  human cognition human responses  formal semisupervised learning problems  yielded varying conclusions   degree  influence   unlabeled data   summary see  natural learning problems may also  viewed  instances  semisupervised learning much  human concept learning involves  small amount  direct instruction eg parental labeling  objects  childhood combined  large amounts  unlabeled experience eg observation  objects without naming  counting    least without feedback human infants  sensitive   structure  unlabeled natural categories   images  dogs  cats  male  female faces  recent work  shown  infants  children take  account    unlabeled examples available   sampling process   labeled examples arise\r\n"}
{"index":{"_id":130}}
{"conceptLabelTag":"overfitting","conceptLabel":"overfitting","conceptDescription":"overfitting  statistics  machine learning one    common tasks   fit  model   set  training data     able  make reliable predictions  general untrained data  overfitting  statistical model describes random error  noise instead   underlying relationship overfitting occurs   model  excessively complex     many parameters relative   number  observations  model    overfit  poor predictive performance   overreacts  minor fluctuations   training data  possibility  overfitting exists   criterion used  training  model       criterion used  judge  efficacy   model  particular  model  typically trained  maximizing  performance   set  training data however  efficacy  determined    performance   training data    ability  perform well  unseen data overfitting occurs   model begins  memorize training data rather  learning  generalize  trend   extreme example   number  parameters      greater   number  observations  simple model  learning process can perfectly predict  training data simply  memorizing  training data   entirety    model will typically fail drastically  making predictions  new  unseen data since  simple model   learned  generalize    potential  overfitting depends     number  parameters  data  also  conformability   model structure   data shape   magnitude  model error compared   expected level  noise  error   data even   fitted model     excessive number  parameters     expected   fitted relationship will appear  perform less well   new data set    data set used  fitting  particular  value   coefficient  determination will shrink relative   original training data  order  avoid overfitting   necessary  use additional techniques eg crossvalidation regularization early stopping pruning bayesian priors  parameters  model comparison  can indicate   training   resulting  better generalization  basis   techniques  either  explicitly penalize overly complex models   test  models ability  generalize  evaluating  performance   set  data  used  training   assumed  approximate  typical unseen data   model will encounter  good analogy   overfitting problem  imagine  baby trying  learn    window      window  start  show  windows   detects   initial phase   windows  glasses   frame   can look outside    may  opened   keep showing   windows  baby may also falsely deduce   windows  green    green frames  windows thus overfitting  problem machine learning usually  learning algorithm  trained using  set  training data exemplary situations    desired output  known  goal    algorithm will also perform well  predicting  output  fed validation data    encountered   training overfitting   use  models  procedures  violate occams razor  example  including  adjustable parameters   ultimately optimal   using   complicated approach   ultimately optimal   example     many adjustable parameters consider  dataset  training data  can  adequately predicted   linear function  two dependent variables   function requires  three parameters  intercept  two slopes replacing  simple function   new  complex quadratic function    new  complex linear function    two dependent variables carries  risk occams razor implies   given complex function   priori less probable   given simple function   new  complicated function  selected instead   simple function       large enough gain  trainingdata fit  offset  complexity increase   new complex function overfits  data   complex overfitted function will likely perform worse   simpler function  validation data outside  training dataset even though  complex function performed  well  perhaps even better   training dataset  comparing different types  models complexity   measured solely  counting  many parameters exist   model  expressivity   parameter must  considered  well  example   nontrivial  directly compare  complexity   neural net  can track curvilinear relationships  parameters   regression model  parameters overfitting  especially likely  cases  learning  performed  long   training examples  rare causing  learner  adjust   specific random features   training data    causal relation   target function   process  overfitting  performance   training examples still increases   performance  unseen data becomes worse   simple example consider  database  retail purchases  includes  item bought  purchaser   date  time  purchase  easy  construct  model  will fit  training set perfectly  using  date  time  purchase  predict   attributes   model will  generalize    new data   past times will never occur  generally  learning algorithm  said  overfit relative   simpler one     accurate  fitting known data hindsight  less accurate  predicting new data foresight one can intuitively understand overfitting   fact  information   past experience can  divided  two groups information   relevant   future  irrelevant information noise everything else  equal   difficult  criterion   predict ie  higher  uncertainty   noise exists  past information  needs   ignored  problem  determining  part  ignore  learning algorithm  can reduce  chance  fitting noise  called robust consequences   obvious consequence  overfitting  poor performance   validation dataset  negative consequences include regression outside machine learning overfitting  also  problem   broad study  regression including regression done  hand   extreme case    p variables   linear regression  p data points  fitted line will go exactly  every point    variety  rules  thumb   number  observations needed per independent variable including    process  regression model selection  mean squared error   random regression function can  decomposited  random noise approximation bias  variance   estimate  regression function  biasvariance tradeoff  often used  overcome overfitted model underfitting underfitting occurs   statistical model  machine learning algorithm  capture  underlying trend   data  occurs   model  algorithm   fit  data enough underfitting occurs   model  algorithm shows low variance  high bias  contrast  opposite overfitting  high variance  low bias   often  result   excessively simple model\r\n"}
{"index":{"_id":131}}
{"conceptLabelTag":"data analysis","conceptLabel":"data analysis","conceptDescription":"data analysis analysis  data   process  inspecting cleansing transforming  modeling data   goal  discovering useful information suggesting conclusions  supporting decisionmaking data analysis  multiple facets  approaches encompassing diverse techniques   variety  names  different business science  social science domains data mining   particular data analysis technique  focuses  modeling  knowledge discovery  predictive rather  purely descriptive purposes business intelligence covers data analysis  relies heavily  aggregation focusing  business information  statistical applications  people divide data analysis  descriptive statistics exploratory data analysis eda  confirmatory data analysis cda eda focuses  discovering new features   data  cda  confirming  falsifying existing hypotheses predictive analytics focuses  application  statistical models  predictive forecasting  classification  text analytics applies statistical linguistic  structural techniques  extract  classify information  textual sources  species  unstructured data   varieties  data analysis data integration   precursor  data analysis  data analysis  closely linked  data visualization  data dissemination  term data analysis  sometimes used   synonym  data modeling  process  data analysis analysis refers  breaking  whole   separate components  individual examination data analysis   process  obtaining raw data  converting   information useful  decisionmaking  users data  collected  analyzed  answer questions test hypotheses  disprove theories statistician john tukey defined data analysis   procedures  analyzing data techniques  interpreting  results   procedures ways  planning  gathering  data  make  analysis easier  precise   accurate    machinery  results  mathematical statistics  apply  analyzing data   several phases  can  distinguished described   phases  iterative   feedback  later phases may result  additional work  earlier phases data requirements  data necessary  inputs   analysis  specified based upon  requirements   directing  analysis  customers  will use  finished product   analysis  general type  entity upon   data will  collected  referred    experimental unit eg  person  population  people specific variables regarding  population eg age  income may  specified  obtained data may  numerical  categorical ie  text label  numbers data collection data  collected   variety  sources  requirements may  communicated  analysts  custodians   data   information technology personnel within  organization  data may also  collected  sensors   environment   traffic cameras satellites recording devices etc  may also  obtained  interviews downloads  online sources  reading documentation data processing data initially obtained must  processed  organized  analysis  instance  may involve placing data  rows  columns   table format ie structured data   analysis   within  spreadsheet  statistical software data cleaning  processed  organized  data may  incomplete contain duplicates  contain errors  need  data cleaning will arise  problems   way  data  entered  stored data cleaning   process  preventing  correcting  errors common tasks include record matching identifying inaccuracy  data overall quality  existing data deduplication  column segmentation  data problems can also  identified   variety  analytical techniques  example  financial information  totals  particular variables may  compared  separately published numbers believed   reliable unusual amounts    predetermined thresholds may also  reviewed   several types  data cleaning  depend   type  data   phone numbers email addresses employers etc quantitative data methods  outlier detection can  used  get rid  likely incorrectly entered data textual data spellcheckers can  used  lessen  amount  mistyped words    harder  tell   words   correct exploratory data analysis   data  cleaned  can  analyzed analysts may apply  variety  techniques referred   exploratory data analysis  begin understanding  messages contained   data  process  exploration may result  additional data cleaning  additional requests  data   activities may  iterative  nature descriptive statistics    average  median may  generated  help understand  data data visualization may also  used  examine  data  graphical format  obtain additional insight regarding  messages within  data modeling  algorithms mathematical formulas  models called algorithms may  applied   data  identify relationships among  variables   correlation  causation  general terms models may  developed  evaluate  particular variable   data based   variables   data   residual error depending  model accuracy ie data model error inferential statistics includes techniques  measure relationships  particular variables  example regression analysis may  used  model whether  change  advertising independent variable x explains  variation  sales dependent variable y  mathematical terms y sales   function  x advertising  may  described  y ax b error   model  designed     b minimize  error   model predicts y   given range  values  x analysts may attempt  build models   descriptive   data  simplify analysis  communicate results data product  data product   computer application  takes data inputs  generates outputs feeding  back   environment  may  based   model  algorithm  example   application  analyzes data  customer purchasing history  recommends  purchases  customer might enjoy communication   data  analyzed  may  reported  many formats   users   analysis  support  requirements  users may  feedback  results  additional analysis   much   analytical cycle  iterative  determining   communicate  results  analyst may consider data visualization techniques  help clearly  efficiently communicate  message   audience data visualization uses information displays   tables  charts  help communicate key messages contained   data tables  helpful   user  might lookup specific numbers  charts eg bar charts  line charts may help explain  quantitative messages contained   data quantitative messages author stephen  described eight types  quantitative messages  users may attempt  understand  communicate   set  data   associated graphs used  help communicate  message customers specifying requirements  analysts performing  data analysis may consider  messages   course   process techniques  analyzing quantitative data author jonathan koomey  recommended  series  best practices  understanding quantitative data  include   variables  examination analysts typically obtain descriptive statistics      mean average median  standard deviation  may also analyze  distribution   key variables  see   individual values cluster around  mean analysts may use robust statistical measurements  solve certain analytical problems hypothesis testing  used   particular hypothesis   true state  affairs  made   analyst  data  gathered  determine whether  state  affairs  true  false  example  hypothesis might   unemployment   effect  inflation  relates   economics concept called  phillips curve hypothesis testing involves considering  likelihood  type   type ii errors  relate  whether  data supports accepting  rejecting  hypothesis regression analysis may  used   analyst  trying  determine  extent   independent variable x affects dependent variable y eg   extent  changes   unemployment rate x affect  inflation rate y    attempt  model  fit  equation line  curve   data   y   function  x necessary condition analysis nca may  used   analyst  trying  determine  extent   independent variable x allows variable y eg   extent   certain unemployment rate x necessary   certain inflation rate y whereas multiple regression analysis uses additive logic   xvariable can produce  outcome   xs can compensate      sufficient   necessary necessary condition analysis nca uses necessity logic  one   xvariables allow  outcome  exist  may  produce    necessary   sufficient  single necessary condition must  present  compensation   possible analytical activities  data users users may  particular data points  interest within  data set  opposed  general messaging outlined   lowlevel user analytic activities  presented   following table  taxonomy can also  organized  three poles  activities retrieving values finding data points  arranging data points barriers  effective analysis barriers  effective analysis may exist among  analysts performing  data analysis  among  audience distinguishing fact  opinion cognitive biases  innumeracy   challenges  sound data analysis confusing fact  opinion effective analysis requires obtaining relevant facts  answer questions support  conclusion  formal opinion  test hypotheses facts  definition  irrefutable meaning   person involved   analysis   able  agree upon   example  august  congressional budget office cbo estimated  extending  bush tax cuts     time period  add approximately trillion   national debt everyone   able  agree  indeed    cbo reported  can  examine  report  makes   fact whether persons agree  disagree   cbo    opinion  another example  auditor   public company must arrive   formal opinion  whether financial statements  publicly traded corporations  fairly stated   material respects  requires extensive analysis  factual data  evidence  support  opinion  making  leap  facts  opinions   always  possibility   opinion  erroneous cognitive biases    variety  cognitive biases  can adversely effect analysis  example confirmation bias   tendency  search   interpret information   way  confirms ones preconceptions  addition individuals may discredit information    support  views analysts may  trained specifically   aware   biases    overcome    book psychology  intelligence analysis retired cia analyst richards heuer wrote  analysts  clearly delineate  assumptions  chains  inference  specify  degree  source   uncertainty involved   conclusions  emphasized procedures  help surface  debate alternative points  view innumeracy effective analysts  generally adept   variety  numerical techniques however audiences may    literacy  numbers  numeracy   said   innumerate persons communicating  data may also  attempting  mislead  misinform deliberately using bad numerical techniques  example whether  number  rising  falling may    key factor  important may   number relative  another number    size  government revenue  spending relative   size   economy gdp   amount  cost relative  revenue  corporate financial statements  numerical technique  referred   normalization  commonsizing   many  techniques employed  analysts whether adjusting  inflation ie comparing real vs nominal data  considering population increases demographics etc analysts apply  variety  techniques  address  various quantitative messages described   section  analysts may also analyze data  different assumptions  scenarios  example  analysts perform financial statement analysis  will often recast  financial statements  different assumptions  help arrive   estimate  future cash flow    discount  present value based   interest rate  determine  valuation   company   stock similarly  cbo analyzes  effects  various policy options   governments revenue outlays  deficits creating alternative future scenarios  key measures  topics analytics  business intelligence analytics   extensive use  data statistical  quantitative analysis explanatory  predictive models  factbased management  drive decisions  actions    subset  business intelligence    set  technologies  processes  use data  understand  analyze business performance education  education  educators  access   data system   purpose  analyzing student data  data systems present data  educators   overthecounter data format embedding labels supplemental documentation   help system  making key packagedisplay  content decisions  improve  accuracy  educators data analyses practitioner notes  section contains rather technical explanations  may assist practitioners   beyond  typical scope    article initial data analysis   important distinction   initial data analysis phase   main analysis phase    initial data analysis one refrains   analysis   aimed  answering  original research question  initial data analysis phase  guided   following four questions quality  data  quality   data   checked  early  possible data quality can  assessed  several ways using different types  analysis frequency counts descriptive statistics mean standard deviation median normality skewness kurtosis frequency histograms n variables  compared  coding schemes  variables external   data set  possibly corrected  coding schemes   comparable  choice  analyses  assess  data quality   initial data analysis phase depends   analyses  will  conducted   main analysis phase quality  measurements  quality   measurement instruments    checked   initial data analysis phase      focus  research question   study one  check whether structure  measurement instruments corresponds  structure reported   literature   two ways  assess measurement initial transformations  assessing  quality   data    measurements one might decide  impute missing data   perform initial transformations  one   variables although  can also  done   main analysis phase possible transformations  variables    implementation   study fulfill  intentions   research design one  check  success   randomization procedure  instance  checking whether background  substantive variables  equally distributed within  across groups   study   need  use  randomization procedure one  check  success   nonrandom sampling  instance  checking whether  subgroups   population  interest  represented  sample  possible data distortions    checked  characteristics  data sample   report  article  structure   sample must  accurately described   especially important  exactly determine  structure   sample  specifically  size   subgroups  subgroup analyses will  performed   main analysis phase  characteristics   data sample can  assessed  looking  final stage   initial data analysis   final stage  findings   initial data analysis  documented  necessary preferable  possible corrective actions  taken also  original plan   main data analyses can    specified   detail  rewritten  order    several decisions   main data analyses can    made analysis several analyses can  used   initial data analysis phase   important  take  measurement levels   variables  account   analyses  special statistical techniques  available   level nonlinear analysis nonlinear analysis will  necessary   data  recorded   nonlinear system nonlinear systems can exhibit complex dynamic effects including bifurcations chaos harmonics  subharmonics    analyzed using simple linear methods nonlinear data analysis  closely related  nonlinear system identification main data analysis   main analysis phase analyses aimed  answering  research question  performed  well    relevant analysis needed  write  first draft   research report exploratory  confirmatory approaches   main analysis phase either  exploratory  confirmatory approach can  adopted usually  approach  decided  data  collected   exploratory analysis  clear hypothesis  stated  analysing  data   data  searched  models  describe  data well   confirmatory analysis clear hypotheses   data  tested exploratory data analysis   interpreted carefully  testing multiple models      high chance  finding  least one     significant   can  due   type error   important  always adjust  significance level  testing multiple models   example  bonferroni correction also one   follow   exploratory analysis   confirmatory analysis    dataset  exploratory analysis  used  find ideas   theory    test  theory  well   model  found exploratory   dataset  following   analysis   confirmatory analysis    dataset  simply mean   results   confirmatory analysis  due    type error  resulted   exploratory model   first place  confirmatory analysis therefore will    informative   original exploratory analysis stability  results   important  obtain  indication   generalizable  results     hard  check one can look   stability   results   results reliable  reproducible   two main ways    statistical methods many statistical methods   used  statistical analyses   brief list  four    popular methods \r\n"}
{"index":{"_id":132}}
{"conceptLabelTag":"feature space","conceptLabel":"feature space","conceptDescription":"feature vector  pattern recognition  machine learning  feature vector   ndimensional vector  numerical features  represent  object many algorithms  machine learning require  numerical representation  objects since  representations facilitate processing  statistical analysis  representing images  feature values might correspond   pixels   image  representing texts perhaps term occurrence frequencies feature vectors  equivalent   vectors  explanatory variables used  statistical procedures   linear regression feature vectors  often combined  weights using  dot product  order  construct  linear predictor function   used  determine  score  making  prediction  vector space associated   vectors  often called  feature space  order  reduce  dimensionality   feature space  number  dimensionality reduction techniques can  employed higherlevel features can  obtained  already available features  added   feature vector  example   study  diseases  feature age  useful   defined  age year  death year  birth  process  referred   feature construction feature construction   application   set  constructive operators   set  existing features resulting  construction  new features examples   constructive operators include checking   equality conditions  arithmetic operators  array operators maxs mins averages  well    sophisticated operators  example countsc  counts  number  features   feature vector s satisfying  condition c   example distances   recognition classes generalized   accepting device feature construction  long  considered  powerful tool  increasing  accuracy  understanding  structure particularly  highdimensional problems applications include studies  disease  emotion recognition  speech\r\n"}
{"index":{"_id":133}}
{"conceptLabelTag":"multivariate adaptive regression splines","conceptLabel":"multivariate adaptive regression splines","conceptDescription":"multivariate adaptive regression splines  statistics multivariate adaptive regression splines mars   form  regression analysis introduced  jerome h friedman     nonparametric regression technique  can  seen   extension  linear models  automatically models nonlinearities  interactions  variables  term mars  trademarked  licensed  salford systems  order  avoid trademark infringements many open source implementations  mars  called earth  basics  section introduces mars using   examples  start   set  data  matrix  input variables x   vector   observed responses y   response   row  x  example  data       one independent variable   x matrix  just  single column given  measurements   like  build  model  predicts  expected y   given x  linear model    data   hat   formula indicates  formula  estimated   data  figure   right shows  plot   function  line giving  predicted formula versus x   original values  y shown  red dots  data   extremes  x indicates   relationship  y  x may  nonlinear look   red dots relative   regression line  low  high values  x  thus turn  mars  automatically build  model taking  account nonlinearities mars software constructs  model   given x  y  follows  figure   right shows  plot   function  predicted formula versus x   original values  y   shown  red dots  predicted response  now  better fit   original y values mars  automatically produced  kink   predicted y  take  account nonlinearity  kink  produced  hinge functions  hinge functions   expressions starting  formula  formula  formula  formula else formula hinge functions  described   detail    simple example  can easily see   plot  y   nonlinear relationship  x  might perhaps guess  y varies   square  x however  general  will  multiple independent variables   relationship  y   variables will  unclear   easily visible  plotting  can use mars  discover  nonlinear relationship  example mars expression  multiple variables   expression models air pollution  ozone level   function   temperature     variables note   last term   formula   last line incorporates  interaction  formula  formula  figure   right plots  predicted formula  formula  formula vary    variables fixed   median values  figure shows  wind   affect  ozone level unless visibility  low  see  mars can build quite flexible regression surfaces  combining hinge functions  obtain   expression  mars model building procedure automatically selects  variables  use  variables  important others   positions   kinks   hinge functions    hinge functions  combined  mars model mars builds models   form  model   weighted sum  basis functions formula  formula   constant coefficient  example  line   formula  ozone   one basis function multiplied   coefficient  basis function formula takes one   following three forms  constant   just one  term  intercept   ozone formula   intercept term   hinge function  hinge function   form formula  formula mars automatically selects variables  values   variables  knots   hinge functions examples   basis functions can  seen   middle three lines   ozone formula  product  two   hinge functions  basis functions can model interaction  two   variables  example   last line   ozone formula hinge functions hinge functions   key part  mars models  hinge function takes  form   formula   constant called  knot  figure   right shows  mirrored pair  hinge functions   knot   hinge function  zero  part   range  can  used  partition  data  disjoint regions    can  treated independently thus  example  mirrored pair  hinge functions   expression creates  piecewise linear graph shown   simple mars model   previous section one might assume   piecewise linear functions can  formed  hinge functions  hinge functions can  multiplied together  form nonlinear functions hinge functions  also called hockey stick  rectifier functions instead   formula notation used   article hinge functions  often represented  formula  formula means take  positive part  model building process mars builds  model  two phases  forward   backward pass  twostage approach      used  recursive partitioning trees  forward pass mars starts   model  consists  just  intercept term    mean   response values mars  repeatedly adds basis function  pairs   model   step  finds  pair  basis functions  gives  maximum reduction  sumofsquares residual error    greedy algorithm  two basis functions   pair  identical except   different side   mirrored hinge function  used   function  new basis function consists   term already   model multiplied   new hinge function  hinge function  defined   variable   knot   add  new basis function mars must search   combinations   following existing terms called parent terms   context  variables  select one   new basis function  values   variable   knot   new hinge function  calculate  coefficient   term mars applies  linear regression   terms  process  adding terms continues   change  residual error   small  continue    maximum number  terms  reached  maximum number  terms  specified   user  model building starts  search   step  done   brute force fashion   key aspect  mars      nature  hinge functions  search can  done relatively quickly using  fast leastsquares update technique actually  search   quite brute force  search can  sped    heuristic  reduces  number  parent terms  consider   step fast mars  backward pass  forward pass usually builds  overfit model  overfit model   good fit   data used  build  model  will  generalize well  new data  build  model  better generalization ability  backward pass prunes  model  removes terms one  one deleting  least effective term   step   finds  best submodel model subsets  compared using  gcv criterion described   backward pass   advantage   forward pass   step  can choose  term  delete whereas  forward pass   step can  see  next pair  terms  forward pass adds terms  pairs   backward pass typically discards one side   pair   terms  often  seen  pairs   final model  paired hinge can  seen   equation  formula   first mars example     complete pairs retained   ozone example generalized cross validation  backward pass uses generalized cross validation gcv  compare  performance  model subsets  order  choose  best subset lower values  gcv  better  gcv   form  regularization  trades  goodnessoffit  model complexity  formula   gcv   rss   residual sumofsquares measured   training data  n   number  observations  number  rows   x matrix  effectivenumberofparameters  defined   mars context   penalty     mars software allows  user  preset penalty note    number  hingefunction knots   formula penalizes  addition  knots thus  gcv formula adjusts ie increases  training rss  take  account  flexibility   model  penalize flexibility  models    flexible will model  specific realization  noise   data instead  just  systematic structure   data generalized cross validation   named   uses  formula  approximate  error    determined  leaveoneout validation   just  approximation  works well  practice gcvs  introduced  craven  wahba  extended  friedman  mars constraints one constraint  already  mentioned  user can specify  maximum number  terms   forward pass   constraint can  placed   forward pass  specifying  maximum allowable degree  interaction typically  one  two degrees  interaction  allowed  higher degrees can  used   data warrants   maximum degree  interaction   first mars example   one ie  interactions   additive model   ozone example   two  constraints   forward pass  possible  example  user can specify  interactions  allowed   certain input variables  constraints  make sense   knowledge   process  generated  data pros  cons  regression modeling technique  best   situations  guidelines   intended  give  idea   pros  cons  mars   will  exceptions   guidelines   useful  compare mars  recursive partitioning    done  recursive partitioning  also commonly called regression trees decision trees  cart see  recursive partitioning article  details external links several   commercial software packages  available  fitting marstype models\r\n"}
{"index":{"_id":134}}
{"conceptLabelTag":"delta rule","conceptLabel":"delta rule","conceptDescription":"delta rule  machine learning  delta rule   gradient descent learning rule  updating  weights   inputs  artificial neurons   singlelayer neural network    special case    general backpropagation algorithm   neuron formula  activation function formula  delta rule  formulas formulath weight formula  given    holds  formula  formula  delta rule  commonly stated  simplified form   neuron   linear activation function    delta rule  similar   perceptrons update rule  derivation  different  perceptron uses  heaviside step function   activation function formula   means  formula   exist  zero   equal  zero elsewhere  makes  direct application   delta rule impossible derivation   delta rule  delta rule  derived  attempting  minimize  error   output   neural network  gradient descent  error   neural network  formula outputs can  measured    case  wish  move  weight space   neuron  space   possible values     neurons weights  proportion   gradient   error function  respect   weight  order     calculate  partial derivative   error  respect   weight   formulath weight  derivative can  written      concerning    formulath neuron  can substitute  error formula   omitting  summation next  use  chain rule  split   two derivatives  find  left derivative  simply apply  general power rule  find  right derivative   apply  chain rule  time differentiating  respect   total input  formula formula note   output   formulath neuron formula  just  neurons activation function formula applied   neurons input formula  can therefore write  derivative  formula  respect  formula simply  formulas first derivative next  rewrite formula   last term   sum   formula weights   weight formula times  corresponding input formula     concerned   formulath weight   term   summation   relevant  formula clearly giving us  final equation   gradient  noted  gradient descent tells us   change   weight   proportional   gradient choosing  proportionality constant formula  eliminating  minus sign  enable us  move  weight   negative direction   gradient  minimize error  arrive   target equation\r\n"}
{"index":{"_id":135}}
{"conceptLabelTag":"lazy learning","conceptLabel":"lazy learning","conceptDescription":"lazy learning  machine learning lazy learning   learning method   generalization beyond  training data  delayed   query  made   system  opposed   eager learning   system tries  generalize  training data  receiving queries  main advantage gained  employing  lazy learning method   casebased reasoning    target function will  approximated locally     knearest neighbor algorithm   target function  approximated locally   query   system lazy learning systems can simultaneously solve multiple problems  deal successfully  changes   problem domain  disadvantages  lazy learning include  large space requirement  store  entire training dataset particularly noisy training data increases  case base unnecessarily   abstraction  made   training phase another disadvantage   lazy learning methods  usually slower  evaluate though   coupled   faster training phase lazy classifiers   useful  large datasets   attributes\r\n"}
{"index":{"_id":136}}
{"conceptLabelTag":"conditional random field","conceptLabel":"conditional random field","conceptDescription":"conditional random field conditional random fields crfs   class  statistical modelling method often applied  pattern recognition  machine learning    used  structured prediction whereas  ordinary classifier predicts  label   single sample without regard  neighboring samples  crf can take context  account eg  linear chain crf   popular  natural language processing predicts sequences  labels  sequences  input samples crfs   type  discriminative undirected probabilistic graphical model   used  encode known relationships  observations  construct consistent interpretations   often used  labeling  parsing  sequential data   natural language text  biological sequences   computer vision specifically crfs find applications  shallow parsing named entity recognition gene finding  peptide critical functional region finding among  tasks   alternative   related hidden markov models hmms  computer vision crfs  often used  object recognition  image segmentation description lafferty mccallum  pereira define  crf  observations formula  random variables formula  follows let formula   graph   formula  formula   conditional random field   random variables formula conditioned  formula obey  markov property  respect   graph formula  formula means  formula  formula  neighbors  formula   means    crf   undirected graphical model whose nodes can  divided  exactly two disjoint sets formula  formula  observed  output variables respectively  conditional distribution formula   modeled inference  general graphs  problem  exact inference  crfs  intractable  inference problem   crf  basically      mrf    arguments hold however  exist special cases   exact inference  feasible  exact inference  impossible several algorithms can  used  obtain approximate solutions  include parameter learning learning  parameters formula  usually done  maximum likelihood learning  formula   nodes  exponential family distributions   nodes  observed  training  optimization  convex  can  solved  example using gradient descent algorithms  quasinewton methods    lbfgs algorithm    hand   variables  unobserved  inference problem    solved   variables exact inference  intractable  general graphs  approximations    used examples  sequence modeling  graph  interest  usually  chain graph  input sequence  observed variables formula represents  sequence  observations  formula represents  hidden  unknown state variable  needs   inferred given  observations  formula  structured  form  chain   edge   formula  formula  well    simple interpretation   formula  labels   element   input sequence  layout admits efficient algorithms   conditional dependency   formula  formula  defined   fixed set  feature functions   form formula  can informally  thought   measurements   input sequence  partially determine  likelihood   possible value  formula  model assigns  feature  numerical weight  combines   determine  probability   certain value  formula linearchain crfs  many    applications  conceptually simpler hidden markov models hmms  relax certain assumptions   input  output sequence distributions  hmm can loosely  understood   crf   specific feature functions  use constant probabilities  model state transitions  emissions conversely  crf can loosely  understood   generalization   hmm  makes  constant transition probabilities  arbitrary functions  vary across  positions   sequence  hidden states depending   input sequence notably  contrast  hmms crfs can contain  number  feature functions  feature functions can inspect  entire input sequence formula   point  inference   range   feature functions need    probabilistic interpretation variants higherorder crfs  semimarkov crfs crfs can  extended  higher order models  making  formula dependent   fixed number formula  previous variables formula training  inference   practical  small values  formula   o since  computational cost increases exponentially  formula largemargin models  structured prediction    structured support vector machine can  seen   alternative training procedure  crfs  exists another generalization  crfs  semimarkov conditional random field semicrf  models variablelength segmentations   label sequence formula  provides much   power  higherorder crfs  model longrange dependencies   formula   reasonable computational cost latentdynamic conditional random field latentdynamic conditional random fields ldcrf  discriminative probabilistic latent variable models dplvm   type  crfs  sequence tagging tasks   latent variable models   trained discriminatively   ldcrf like   sequence tagging task given  sequence  observations x formula  main problem  model must solve    assign  sequence  labels y formula  one finite set  labels instead  directly modeling yx   ordinary linearchain crf    set  latent variables h  inserted  x  y using  chain rule  probability  allows capturing latent structure   observations  labels  ldcrfs can  trained using quasinewton methods  specialized version   perceptron algorithm called  latentvariable perceptron   developed    well based  collins structured perceptron algorithm  models find applications  computer vision specifically gesture recognition  video streams  shallow parsing software    partial list  software  implement generic crf tools    partial list  software  implement crf related tools\r\n"}
{"index":{"_id":137}}
{"conceptLabelTag":"structured prediction","conceptLabel":"structured prediction","conceptDescription":"structured prediction structured prediction  structured output learning   umbrella term  supervised machine learning techniques  involves predicting structured objects rather  scalar discrete  real values  example  problem  translating  natural language sentence   syntactic representation    parse tree can  seen   structured prediction problem    structured output domain   set   possible parse trees probabilistic graphical models form  large class  structured prediction models  particular bayesian networks  random fields  popularly used  solve structured prediction problems   wide variety  application domains including bioinformatics natural language processing speech recognition  computer vision  algorithms  models  structured prediction include inductive logic programming casebased reasoning structured svms markov logic networks  constrained conditional models similar  commonly used supervised learning techniques structured prediction models  typically trained  means  observed data    true prediction value  used  adjust model parameters due   complexity   model   interrelations  predicted variables  process  prediction using  trained model   training   often computationally infeasible  approximate inference  learning methods  used example sequence tagging sequence tagging   class  problems prevalent  natural language processing  input data  often sequences eg sentences  text  sequence tagging problem appears  several guises eg partofspeech tagging  named entity recognition  pos tagging  example  word   sequence must receive  tag class label  expresses  type  word  main challenge   problem   resolve ambiguity  word sentence can also   verb  english   can tagged   problem can  solved  simply performing classification  individual tokens  approach   take  account  empirical fact  tags   occur independently instead  tag displays  strong conditional dependence   tag   previous word  fact can  exploited   sequence model    hidden markov model  conditional random field  predicts  entire tag sequence   sentence rather  just individual tags  means   viterbi algorithm structured perceptron one   easiest ways  understand algorithms  general structured prediction   structured perceptron  collins  algorithm combines  perceptron algorithm  learning linear classifiers   inference algorithm classically  viterbi algorithm  used  sequence data  can  described abstractly  follows first define  joint feature function x y  maps  training sample x   candidate prediction y   vector  length n x  y may   structure n  problemdependent  must  fixed   model let gen   function  generates candidate predictions   practice finding  argmax  genx will  done using  algorithm   viterbi   algorithm   maxsum rather   exhaustive search   exponentially large set  candidates  idea  learning  similar  multiclass perceptron\r\n"}
{"index":{"_id":138}}
{"conceptLabelTag":"explanation based learning","conceptLabel":"explanation based learning","conceptDescription":"explanationbased learning explanationbased learning ebl   form  machine learning  exploits   strong  even perfect domain theory  make generalizations  form concepts  training examples details  example  ebl using  perfect domain theory   program  learns  play chess   shown examples  specific chess position  contains  important feature say forced loss  black queen  two moves includes many irrelevant features    specific scattering  pawns   board ebl can take  single training example  determine    relevant features  order  form  generalization  domain theory  perfect  complete   contains  principle  information needed  decide  question   domain  example  domain theory  chess  simply  rules  chess knowing  rules  principle   possible  deduce  best move   situation however actually making   deduction  impossible  practice due  combinatoric explosion ebl uses training examples  make searching  deductive consequences   domain theory efficient  practice  essence  ebl system works  finding  way  deduce  training example   systems existing database  domain theory   short proof   training example extends  domaintheory database enabling  ebl system  find  classify future examples   similar   training example  quickly  main drawback   methodthe cost  applying  learned proof macros   become numerouswas analyzed  minton basic formulation ebl software takes four inputs application  especially good application domain   ebl  natural language processing nlp   rich domain theory ie  natural language grammaralthough neither perfect  complete  tuned   particular application  particular language usage using  treebank training examples rayner pioneered  work  first successful industrial application    commercial nl interface  relational databases  method   successfully applied  several largescale natural language parsing system   utility problem  solved  omitting  original grammar domain theory  using specialized lrparsing techniques resulting  huge speedups   cost  coverage    gain  disambiguation ebllike techniques  also  applied  surface generation  converse  parsing  applying ebl  nlp  operationality criteria can  handcrafted  can  inferred   treebank using either  entropy   ornodes   target coveragedisambiguation tradeoff recallprecision tradeoff fscore ebl can also  used  compile grammarbased language models  speech recognition  general unification grammars note   utility problem first exposed  minton  solved  discarding  original grammardomain theory    quoted articles tend  contain  phrase grammar specializationquite  opposite   original term explanationbased generalization perhaps  best name   technique   datadriven search space reduction  people  worked  ebl  nlp include guenther neumann aravind joshi srinivas bangalore  khalil simaan\r\n"}
{"index":{"_id":139}}
{"conceptLabelTag":"eager learning","conceptLabel":"eager learning","conceptDescription":"eager learning  artificial intelligence eager learning   learning method    system tries  construct  general inputindependent target function  training   system  opposed  lazy learning  generalization beyond  training data  delayed   query  made   system  main advantage gained  employing  eager learning method    artificial neural network    target function will  approximated globally  training thus requiring much less space  using  lazy learning system eager learning systems also deal much better  noise   training data eager learning   example  offline learning   posttraining queries   system   effect   system   thus   query   system will always produce   result  main disadvantage  eager learning     generally unable  provide good local approximations   target function\r\n"}
{"index":{"_id":140}}
{"conceptLabelTag":"graphical model","conceptLabel":"graphical model","conceptDescription":"graphical model  graphical model  probabilistic graphical model pgm   probabilistic model    graph expresses  conditional dependence structure  random variables   commonly used  probability theory statisticsparticularly bayesian statisticsand machine learning types  graphical models generally probabilistic graphical models use  graphbased representation   foundation  encoding  complete distribution   multidimensional space   graph    compact  factorized representation   set  independences  hold   specific distribution two branches  graphical representations  distributions  commonly used namely bayesian networks  markov random fields  families encompass  properties  factorization  independences   differ   set  independences  can encode   factorization   distribution   induce bayesian network   network structure   model   directed acyclic graph  model represents  factorization   joint probability   random variables  precisely   events  formula   joint probability satisfies  formula   set  parents  node formula   words  joint distribution factors   product  conditional distributions  example  graphical model   figure shown    actually   directed acyclic graph   ancestral graph consists   random variables formula   joint probability density  factors   two nodes  conditionally independent given  values   parents  general  two sets  nodes  conditionally independent given  third set   criterion called dseparation holds   graph local independences  global independences  equivalent  bayesian networks  type  graphical model  known   directed graphical model bayesian network  belief network classic machine learning models like hidden markov models neural networks  newer models   variableorder markov models can  considered special cases  bayesian networks markov random field  markov random field also known   markov network   model   undirected graph  graphical model  many repeated subunits can  represented  plate notation applications  framework   models  provides algorithms  discovering  analyzing structure  complex distributions  describe  succinctly  extract  unstructured information allows    constructed  utilized effectively applications  graphical models include causal inference information extraction speech recognition computer vision decoding  lowdensity paritycheck codes modeling  gene regulatory networks gene finding  diagnosis  diseases  graphical models  protein structure\r\n"}
{"index":{"_id":141}}
{"conceptLabelTag":"backpropagation","conceptLabel":"backpropagation","conceptDescription":"backpropagation  backward propagation  errors  backpropagation   common method  training artificial neural networks  used  conjunction   optimization method   gradient descent  algorithm repeats  two phase cycle propagation  weight update   input vector  presented   network   propagated forward   network layer  layer   reaches  output layer  output   network   compared   desired output using  loss function   error value  calculated     neurons   output layer  error values   propagated backwards starting   output   neuron   associated error value  roughly represents  contribution   original output backpropagation uses  error values  calculate  gradient   loss function  respect   weights   network   second phase  gradient  fed   optimization method   turn uses   update  weights   attempt  minimize  loss function  importance   process     network  trained  neurons   intermediate layers organize     way   different neurons learn  recognize different characteristics   total input space  training   arbitrary input pattern  present  contains noise   incomplete neurons   hidden layer   network will respond   active output   new input contains  pattern  resembles  feature   individual neurons  learned  recognize   training backpropagation requires  known desired output   input value  order  calculate  loss function gradient   therefore usually considered    supervised learning method nonetheless   also used   unsupervised networks   autoencoders    generalization   delta rule  multilayered feedforward networks made possible  using  chain rule  iteratively compute gradients   layer backpropagation requires   activation function used   artificial neurons  nodes  differentiable motivation  goal   supervised learning algorithm   find  function  best maps  set  inputs   correct output  example    classification task   input   image   animal   correct output    name   animal  motivation  developing  backpropagation algorithm   find  way  train  multilayered neural network    can learn  appropriate internal representations  allow   learn  arbitrary mapping  input  output  goal  backpropagation   compute  partial derivative  gradient formula   loss function formula  respect   weight formula   network loss function sometimes referred    cost function  error function    confused   gauss error function  loss function   function  maps values  one   variables onto  real number intuitively representing  cost associated   event  backpropagation  loss function calculates  difference   input training example   expected output   example   propagated   network assumptions   loss function  backpropagation  work two assumptions  made   form   error function  first    can  written   average formula  error functions formula  individual training examples formula  reason   assumption    backpropagation algorithm calculates  gradient   error function   single training example  needs   generalized   overall error function  practice training examples  placed  batches   error  averaged   end   batch    used  update  weights  second assumption    can  written   function   outputs   neural network example loss function let formula  vectors  formula select  error function formula measuring  difference  two outputs  standard choice  formula  square   euclidean distance   vectors formula  formula  factor  formula conveniently cancels  exponent   error function  subsequently differentiated  error function  formula training examples can  written   averageformulaand  partial derivative  respect   outputsformula algorithm let formula   neural network  formula connections  formula will denote vectors  formula formula vectors  formula  formula vectors  formula   called inputs outputs  weights respectively  neural network corresponds   function formula  given  weight formula maps  input formula   output formula  backpropagation algorithm takes  input  sequence  training examples formula  produces  sequence  weights formula starting   initial weight formula usually chosen  random  weights  computed  turn first compute formula using  formula  formula  output   backpropagation algorithm   formula giving us  new function formula  computation      step hence   case formula  described calculating formula  formula  done  considering  variable weight formula  applying gradient descent   function formula  find  local minimum starting  formula  makes formula  minimizing weight found  gradient descent algorithm  code  implement  algorithm  explicit formulas  required   gradient   function formula   function formula  backpropagation learning algorithm can  divided  two phases propagation  weight update phase propagation  propagation involves  following steps phase weight update   weight  following steps must  followed  ratio percentage influences  speed  quality  learning   called  learning rate  greater  ratio  faster  neuron trains   lower  ratio   accurate  training   sign   gradient   weight indicates whether  error varies directly   inversely   weight therefore  weight must  updated   opposite direction descending  gradient phases   repeated   performance   network  satisfactory code  following  pseudocode   stochastic gradient descent algorithm  training  threelayer network  one hidden layer  lines labeled backward pass can  implemented using  backpropagation algorithm  calculates  gradient   error   network regarding  networks modifiable weights often  term backpropagation  used    general sense  refer   entire procedure encompassing   calculation   gradient   use  stochastic gradient descent  backpropagation properties can  used   gradientbased optimizer   lbfgs  truncated newton backpropagation networks  necessarily multilayer perceptrons usually  one input multiple hidden  one output layer  order   hidden layer  serve  useful function multilayer networks must  nonlinear activation functions   multiple layers  multilayer network using  linear activation functions  equivalent   single layer linear network nonlinear activation functions   commonly used include  rectifier logistic function  softmax function   gaussian function  backpropagation algorithm  calculating  gradient   rediscovered  number  times    special case    general technique called automatic differentiation   reverse accumulation mode   also closely related   gaussnewton algorithm   also part  continuing research  neural backpropagation intuition learning   optimization problem  showing  mathematical derivation   backpropagation algorithm  helps  develop  intuitions   relationship   actual output   neuron   correct output   particular training case consider  simple neural network  two input units one output unit   hidden units  neuron uses  linear output    weighted sum   input initially  training  weights will  set randomly   neuron learns  training examples    case consists   set  tuples formula formula formula  formula  formula   inputs   network  formula   correct output  output  network  eventually produce given  identical inputs  network given formula  formula will compute  output formula   likely differs  formula since  weights  initially random  common method  measuring  discrepancy   expected output formula   actual output formula  using  squared error measure  formula   discrepancy  error   example consider  network   single training case formula thus  input formula  formula   respectively   correct output formula  now   actual output formula  plotted   xaxis   error formula   formulaaxis  result   parabola  minimum   parabola corresponds   output formula  minimizes  error formula   single training case  minimum also touches  formulaaxis  means  error will  zero   network can produce  output formula  exactly matches  expected output formula therefore  problem  mapping inputs  outputs can  reduced   optimization problem  finding  function  will produce  minimal error however  output   neuron depends   weighted sum    inputs  formula  formula   weights   connection   input units   output unit therefore  error also depends   incoming weights   neuron   ultimately  needs   changed   network  enable learning   weight  plotted   separate horizontal axis   error   vertical axis  result   parabolic bowl   neuron  formula weights   plot  require  elliptic paraboloid  formula dimensions  backpropagation algorithm aims  find  set  weights  minimizes  error   several methods  finding  minima   parabola   function   dimension one way  analytically  solving systems  equations however  relies   network   linear system   goal    able  also train multilayer nonlinear networks since  multilayered linear network  equivalent   singlelayer network  method used  backpropagation  gradient descent  analogy  understanding gradient descent  basic intuition behind gradient descent can  illustrated   hypothetical scenario  person  stuck   mountains   trying  get  ie trying  find  minima   heavy fog   visibility  extremely low therefore  path   mountain   visible   must use local information  find  minima  can use  method  gradient descent  involves looking   steepness   hill   current position  proceeding   direction   steepest descent ie downhill    trying  find  top   mountain ie  maxima    proceed   direction steepest ascent ie uphill using  method   eventually find  way   mountain however assume also   steepness   hill   immediately obvious  simple observation  rather  requires  sophisticated instrument  measure   person happens     moment  takes quite  time  measure  steepness   hill   instrument thus   minimize  use   instrument   wanted  get   mountain  sunset  difficulty   choosing  frequency     measure  steepness   hill    go  track   analogy  person represents  backpropagation algorithm   path taken   mountain represents  sequence  parameter settings   algorithm will explore  steepness   hill represents  slope   error surface   point  instrument used  measure steepness  differentiation  slope   error surface can  calculated  taking  derivative   squared error function   point  direction  chooses  travel  aligns   gradient   error surface   point  amount  time  travels  taking another measurement   learning rate   algorithm see  limitation section   discussion   limitations   type  hill climbing algorithm derivation since backpropagation uses  gradient descent method one needs  calculate  derivative   squared error function  respect   weights   network assuming one output neuron  squared error function    factor  formula  included  cancel  exponent  differentiating later  expression will  multiplied   arbitrary learning rate    doesnt matter   constant coefficient  introduced now   neuron formula  output formula  defined   input formula   neuron   weighted sum  outputs formula  previous neurons   neuron    first layer   input layer  formula   input layer  simply  inputs formula   network  number  input units   neuron  formula  variable formula denotes  weight  neurons formula  formula  activation function formula   general nonlinear  differentiable  commonly used activation function   logistic function    nice derivative  finding  derivative   error calculating  partial derivative   error  respect   weight formula  done using  chain rule twice   last factor   righthand side     one term   sum formula depends  formula     neuron    first layer   input layer formula  just formula  derivative   output  neuron formula  respect   input  simply  partial derivative   activation function assuming    logistic function  used    reason  backpropagation requires  activation function   differentiable  first factor  straightforward  evaluate   neuron    output layer   formula  however  formula    arbitrary inner layer   network finding  derivative formula  respect  formula  less obvious considering formula   function   inputs   neurons formula receiving input  neuron formula  taking  total derivative  respect  formula  recursive expression   derivative  obtained therefore  derivative  respect  formula can  calculated    derivatives  respect   outputs formula   next layer  one closer   output neuron  known putting   together   update  weight formula using gradient descent one must choose  learning rate formula  change  weight   added   old weight  equal   product   learning rate   gradient multiplied  formula  formula  required  order  update   direction   minimum   maximum   error function   singlelayer network  expression becomes  delta rule  better understand  backpropagation works    example  illustrate   back propagation algorithm page extension  choice  learning rate formula  important   method since  high value can cause  strong  change causing  minimum   missed    low learning rate slows  training unnecessarily various optimizations  backpropagation   quickprop  primarily aimed  speeding   error minimization  improvements mainly try  increase reliability backpropagation  adaptive learning rate  order  avoid oscillation inside  network   alternating connection weights   improve  rate  convergence   refinements   algorithm  use  adaptive learning rate backpropagation  inertia  using  variable inertia term momentum formula  gradient   last change can  weighted    weight adjustment additionally depends   previous change   momentum formula  equal   change depends solely   gradient   value  will  depend   last change similar   ball rolling   mountain whose current speed  determined     current slope   mountain  also    inertia inertia can  added  backpropagationformulawhere  will depend   current weight change formula    current gradient   error function slope   mountain st summand  well    weight change   previous point  time inertia nd summand  inertia  previous problems   backpropagation getting stuck  steep ravines  flat plateaus  avoided since  example  gradient   error function becomes  small  flat plateaus   immediately lead   deceleration   gradient descent  deceleration  delayed   addition   inertia term    flat plateau can  overcome  quickly modes  learning   two modes  learning  choose  stochastic  batch  stochastic learning  propagation  followed immediately   weight update  batch learning many propagations occur  updating  weights accumulating errors   samples within  batch stochastic learning introduces noise   gradient descent process using  local gradient calculated  one data point  reduces  chance   network getting stuck   local minima yet batch learning typically yields  faster  stable descent   local minima since  update  performed   direction   average error   batch samples  modern applications  common compromise choice   use minibatches meaning batch learning    batch  small size   stochastically selected samples training data collection online learning  used  dynamic environments  provide  continuous stream  new training data patterns offline learning makes use   training set  static patterns history according  various sources basics  continuous backpropagation  derived   context  control theory  henry j kelley    arthur e bryson  using principles  dynamic programming  stuart dreyfus published  simpler derivation based    chain rule vapnik cites reference   book  support vector machines arthur e bryson  yuchi ho described    multistage dynamic system optimization method   seppo linnainmaa finally published  general method  automatic differentiation ad  discrete connected networks  nested differentiable functions  corresponds   modern version  backpropagation   efficient even   networks  sparse  stuart dreyfus used backpropagation  adapt parameters  controllers  proportion  error gradients  paul werbos mentioned  possibility  applying  principle  artificial neural networks    applied linnainmaas ad method  neural networks   way   widely used today  david e rumelhart geoffrey e hinton  ronald j williams showed  computer experiments   method can generate useful internal representations  incoming data  hidden layers  neural networks   s  fell   favour   returned    s now able  train much larger networks using huge modern computing power   gpus   context   new hardware   sometimes referred   deep learning though   often seen  marketing hype  example  backpropagation  used  train  deep neural network  state   art speech recognition\r\n"}
{"index":{"_id":142}}
{"conceptLabelTag":"decision trees","conceptLabel":"decision trees","conceptDescription":"decision tree  decision tree   decision support tool  uses  treelike graph  model  decisions   possible consequences including chance event outcomes resource costs  utility   one way  display  algorithm decision trees  commonly used  operations research specifically  decision analysis  help identify  strategy  likely  reach  goal   also  popular tool  machine learning overview  decision tree   flowchartlike structure    internal node represents  test   attribute eg whether  coin flip comes  heads  tails  branch represents  outcome   test   leaf node represents  class label decision taken  computing  attributes  paths  root  leaf represents classification rules  decision analysis  decision tree   closely related influence diagram  used   visual  analytical decision support tool   expected values  expected utility  competing alternatives  calculated  decision tree consists  three types  nodes decision trees  commonly used  operations research  operations management   practice decisions    taken online   recall  incomplete knowledge  decision tree   paralleled   probability model   best choice model  online selection model algorithm another use  decision trees    descriptive means  calculating conditional probabilities decision trees influence diagrams utility functions   decision analysis tools  methods  taught  undergraduate students  schools  business health economics  public health   examples  operations research  management science methods decision tree building blocks decision tree elements drawn  left  right  decision tree   burst nodes splitting paths   sink nodes converging paths therefore used manually  can grow  big    often hard  draw fully  hand traditionally decision trees   created manually   aside example shows although increasingly specialized software  employed decision rules  decision tree can  linearized  decision rules   outcome   contents   leaf node   conditions along  path form  conjunction    clause  general  rules   form decision rules can  generated  constructing association rules   target variable   right  can also denote temporal  causal relations decision tree using flowchart symbols commonly  decision tree  drawn using flowchart symbols    easier  many  read  understand analysis example analysis can take  account  decision makers eg  companys preference  utility function  example  basic interpretation   situation    company prefers bs risk  payoffs  realistic risk preference coefficients greater  kin  range  risk aversion  company  need  model  third strategy neither   b influence diagram much   information   decision tree can  represented  compactly   influence diagram focusing attention   issues  relationships  events association rule induction decision trees can also  seen  generative models  induction rules  empirical data  optimal decision tree   defined   tree  accounts     data  minimizing  number  levels  questions several algorithms  generate  optimal trees   devised   id cls assistant  cart advantages  disadvantages among decision support tools decision trees  influence diagrams  several advantages decision trees disadvantages  decision trees\r\n"}
{"index":{"_id":143}}
{"conceptLabelTag":"hidden markov model","conceptLabel":"hidden markov model","conceptDescription":"hidden markov model  hidden markov model hmm   statistical markov model    system  modeled  assumed    markov process  unobserved hidden states  hmm can  presented   simplest dynamic bayesian network  mathematics behind  hmm  developed  l e baum  coworkers   closely related   earlier work   optimal nonlinear filtering problem  ruslan l stratonovich    first  describe  forwardbackward procedure  simpler markov models like  markov chain  state  directly visible   observer  therefore  state transition probabilities    parameters   hidden markov model  state   directly visible   output dependent   state  visible  state   probability distribution   possible output tokens therefore  sequence  tokens generated   hmm gives  information   sequence  states  adjective hidden refers   state sequence    model passes    parameters   model  model  still referred    hidden markov model even   parameters  known exactly hidden markov models  especially known   application  temporal pattern recognition   speech handwriting gesture recognition partofspeech tagging musical score following partial discharges  bioinformatics  hidden markov model can  considered  generalization   mixture model   hidden variables  latent variables  control  mixture component   selected   observation  related   markov process rather  independent    recently hidden markov models   generalized  pairwise markov models  triplet markov models  allow consideration   complex data structures   modelling  nonstationary data description  terms  urns   discrete form  hidden markov process can  visualized   generalization   urn problem  replacement   item   urn  returned   original urn   next step consider  example   room    visible   observer    genie  room contains urns x x x    contains  known mix  balls  ball labeled y y y  genie chooses  urn   room  randomly draws  ball   urn   puts  ball onto  conveyor belt   observer can observe  sequence   balls    sequence  urns     drawn  genie   procedure  choose urns  choice   urn   nth ball depends  upon  random number   choice   urn   n th ball  choice  urn   directly depend   urns chosen   single previous urn therefore   called  markov process  can  described   upper part  figure  markov process    observed   sequence  labeled balls thus  arrangement  called  hidden markov process   illustrated   lower part   diagram shown  figure  one can see  balls y y y y can  drawn   state even   observer knows  composition   urns   just observed  sequence  three balls eg y y  y   conveyor belt  observer still   sure  urn ie   state  genie  drawn  third ball  however  observer can work   information    likelihood   third ball came     urns architecture  diagram  shows  general architecture   instantiated hmm  oval shape represents  random variable  can adopt    number  values  random variable xt   hidden state  time   model    diagram xt x x x  random variable yt   observation  time  yt y y y y  arrows   diagram often called  trellis diagram denote conditional dependencies   diagram   clear   conditional probability distribution   hidden variable xt  time given  values   hidden variable   times depends    value   hidden variable xt  values  time t     influence   called  markov property similarly  value   observed variable yt  depends   value   hidden variable xt   time   standard type  hidden markov model considered   state space   hidden variables  discrete   observations  can either  discrete typically generated   categorical distribution  continuous typically   gaussian distribution  parameters   hidden markov model   two types transition probabilities  emission probabilities also known  output probabilities  transition probabilities control  way  hidden state  time  chosen given  hidden state  time formula  hidden state space  assumed  consist  one  possible values modeled   categorical distribution see  section   extensions   possibilities  means      possible states   hidden variable  time can      transition probability   state     possible states   hidden variable  time formula   total  formula transition probabilities note   set  transition probabilities  transitions   given state must sum  thus  formula matrix  transition probabilities   markov matrix   one transition probability can  determined   others  known    total  formula transition parameters  addition     possible states    set  emission probabilities governing  distribution   observed variable   particular time given  state   hidden variable   time  size   set depends   nature   observed variable  example   observed variable  discrete  possible values governed   categorical distribution  will  formula separate parameters   total  formula emission parameters   hidden states    hand   observed variable   dimensional vector distributed according   arbitrary multivariate gaussian distribution  will  parameters controlling  means  formula parameters controlling  covariance matrix   total  formula emission parameters    case unless  value   small  may   practical  restrict  nature   covariances  individual elements   observation vector eg  assuming   elements  independent     less restrictively  independent     fixed number  adjacent elements inference several inference problems  associated  hidden markov models  outlined  probability   observed sequence  task   compute   best way given  parameters   model  probability   particular output sequence  requires summation   possible state sequences  probability  observing  sequence  length l  given    sum runs   possible hiddennode sequences applying  principle  dynamic programming  problem  can  handled efficiently using  forward algorithm probability   latent variables  number  related tasks ask   probability  one     latent variables given  models parameters   sequence  observations formula filtering  task   compute given  models parameters   sequence  observations  distribution  hidden states   last latent variable   end   sequence ie  compute formula  task  normally used   sequence  latent variables  thought    underlying states   process moves    sequence  points  time  corresponding observations   point  time    natural  ask   state   process   end  problem can  handled efficiently using  forward algorithm smoothing   similar  filtering  asks   distribution   latent variable somewhere   middle   sequence ie  compute formula   formula   perspective described   can  thought    probability distribution  hidden states   point  time k   past relative  time t  forwardbackward algorithm   efficient method  computing  smoothed values   hidden state variables  likely explanation  task unlike  previous two asks   joint probability   entire sequence  hidden states  generated  particular sequence  observations see illustration   right  task  generally applicable  hmms  applied  different sorts  problems      tasks  filtering  smoothing  applicable  example  partofspeech tagging   hidden states represent  underlying parts  speech corresponding   observed sequence  words   case    interest   entire sequence  parts  speech rather  simply  part  speech   single word  filtering  smoothing  compute  task requires finding  maximum   possible state sequences  can  solved efficiently   viterbi algorithm statistical significance      problems  may also  interesting  ask  statistical significance    probability   sequence drawn    distribution will   hmm probability   case   forward algorithm   maximum state sequence probability   case   viterbi algorithm  least  large     particular output sequence   hmm  used  evaluate  relevance   hypothesis   particular output sequence  statistical significance indicates  false positive rate associated  failing  reject  hypothesis   output sequence  concrete example  similar example   elaborated   viterbi algorithm page learning  parameter learning task  hmms   find given  output sequence   set   sequences  best set  state transition  emission probabilities  task  usually  derive  maximum likelihood estimate   parameters   hmm given  set  output sequences  tractable algorithm  known  solving  problem exactly   local maximum likelihood can  derived efficiently using  baumwelch algorithm   baldichauvin algorithm  baumwelch algorithm   special case   expectationmaximization algorithm   hmms  used  time series prediction  sophisticated methods like markov chain monte carlo sampling  proven   favorable  finding  single maximum likelihood model   terms  accuracy  stability mathematical description general description  basic hidden markov model can  described  follows note     model  also  one   prior distribution   initial state formula   specified typical learning models correspond  assuming  discrete uniform distribution  possible states ie  particular prior distribution  assumed   bayesian setting  parameters  associated  random variables  follows  characterizations use formula  formula  describe arbitrary distributions  observations  parameters respectively typically formula will   conjugate prior  formula  two  common choices  formula  gaussian  categorical see  compared   simple mixture model  mentioned   distribution   observation   hidden markov model   mixture density   states   corresponding  mixture components   useful  compare   characterizations   hmm   corresponding characterizations   mixture model using   notation  nonbayesian mixture model  bayesian mixture model examples  following mathematical descriptions  fully written   explained  ease  implementation  typical nonbayesian hmm  gaussian observations looks like   typical bayesian hmm  gaussian observations looks like   typical nonbayesian hmm  categorical observations looks like   typical bayesian hmm  categorical observations looks like  note     bayesian characterizations formula  concentration parameter controls  density   transition matrix     high value  formula significantly   probabilities controlling  transition    particular state will   similar meaning  will   significant probability  transitioning      states   words  path followed   markov chain  hidden states will  highly random   low value  formula significantly    small number   possible transitions    given state will  significant probability meaning   path followed   hidden states will  somewhat predictable  twolevel bayesian hmm  alternative    two bayesian examples    add another level  prior parameters   transition matrix   replace  lines   following   means   following imagine   value  formula  significantly    different formula vectors will  dense ie  probability mass will  spread  fairly evenly   states however   extent   mass  unevenly spread formula controls  states  likely  get  mass  others now imagine instead  formula  significantly   will make  formula vectors sparse ie almost   probability mass  distributed   small number  states    rest  transition   state will   unlikely notice    different formula vectors   starting state   even    vectors  sparse different vectors may distribute  mass  different ending states however     vectors formula controls  ending states  likely  get mass assigned    example  formula    formula will  sparse    given starting state   set  states formula   transitions  likely  occur will   small typically   one  two members now   probabilities  formula      equivalently one    models without formula  used   different   will  different states   corresponding formula    states  equally likely  occur   given formula    hand   values  formula  unbalanced   one state   much higher probability  others almost  formula will contain  state hence regardless   starting state transitions will nearly always occur   given state hence  twolevel model   just described allows independent control   overall density   transition matrix   density  states   transitions  likely ie  density   prior distribution  states   particular hidden variable formula   cases   done  still assuming ignorance   particular states   likely  others    desired  inject  information   model  probability vector formula can  directly specified     less certainty   relative probabilities  nonsymmetric dirichlet distribution can  used   prior distribution  formula   instead  using  symmetric dirichlet distribution   single parameter formula  equivalently  general dirichlet   vector   whose values  equal  formula use  general dirichlet  values   variously greater  less  formula according   state    less preferred poisson hidden markov model poisson hidden markov models phmm  special cases  hidden markov models   poisson process   rate  varies  association  changes   different states   markov model phmms   necessarily markovian processes    underlying markov chain  markov process   observed    poisson signal  observed applications hmms can  applied  many fields   goal   recover  data sequence    immediately observable   data  depend   sequence  applications include history  forward  backward recursions used  hmm  well  computations  marginal smoothing probabilities  first described  ruslan l stratonovich  pages    late s   papers  russian  hidden markov models  later described   series  statistical papers  leonard e baum   authors   second half   s one   first applications  hmms  speech recognition starting   mids   second half   s hmms began   applied   analysis  biological sequences  particular dna since    become ubiquitous   field  bioinformatics types hidden markov models can model complex markov processes   states emit  observations according   probability distribution one  example   gaussian distribution    hidden markov model  states output  represented   gaussian distribution moreover   represent even  complex behavior   output   states  represented  mixture  two   gaussians   case  probability  generating  observation   product   probability  first selecting one   gaussians   probability  generating  observation   gaussian extensions   hidden markov models considered   state space   hidden variables  discrete   observations  can either  discrete typically generated   categorical distribution  continuous typically   gaussian distribution hidden markov models can also  generalized  allow continuous state spaces examples   models     markov process  hidden variables   linear dynamical system   linear relationship among related variables    hidden  observed variables follow  gaussian distribution  simple cases    linear dynamical system just mentioned exact inference  tractable   case using  kalman filter however  general exact inference  hmms  continuous latent variables  infeasible  approximate methods must  used    extended kalman filter   particle filter hidden markov models  generative models    joint distribution  observations  hidden states  equivalently   prior distribution  hidden states  transition probabilities  conditional distribution  observations given states  emission probabilities  modeled   algorithms implicitly assume  uniform prior distribution   transition probabilities however   also possible  create hidden markov models   types  prior distributions  obvious candidate given  categorical distribution   transition probabilities   dirichlet distribution    conjugate prior distribution   categorical distribution typically  symmetric dirichlet distribution  chosen reflecting ignorance   states  inherently  likely  others  single parameter   distribution termed  concentration parameter controls  relative density  sparseness   resulting transition matrix  choice  yields  uniform distribution values greater  produce  dense matrix    transition probabilities  pairs  states  likely   nearly equal values less  result   sparse matrix     given source state   small number  destination states  nonnegligible transition probabilities   also possible  use  twolevel prior dirichlet distribution   one dirichlet distribution  upper distribution governs  parameters  another dirichlet distribution  lower distribution   turn governs  transition probabilities  upper distribution governs  overall distribution  states determining  likely  state   occur  concentration parameter determines  density  sparseness  states   twolevel prior distribution   concentration parameters  set  produce sparse distributions might  useful  example  unsupervised partofspeech tagging   parts  speech occur much  commonly  others learning algorithms  assume  uniform prior distribution generally perform poorly   task  parameters  models   sort  nonuniform prior distributions can  learned using gibbs sampling  extended versions   expectationmaximization algorithm  extension   previously described hidden markov models  dirichlet priors uses  dirichlet process  place   dirichlet distribution  type  model allows   unknown  potentially infinite number  states   common  use  twolevel dirichlet process similar   previously described model  two levels  dirichlet distributions   model  called  hierarchical dirichlet process hidden markov model  hdphmm  short   originally described   name infinite hidden markov model    formalized   different type  extension uses  discriminative model  place   generative model  standard hmms  type  model directly models  conditional distribution   hidden states given  observations rather  modeling  joint distribution  example   model   socalled maximum entropy markov model memm  models  conditional distribution   states using logistic regression also known   maximum entropy model  advantage   type  model   arbitrary features ie functions   observations can  modeled allowing domainspecific knowledge   problem  hand   injected   model models   sort   limited  modeling direct dependencies   hidden state   associated observation rather features  nearby observations  combinations   associated observation  nearby observations   fact  arbitrary observations   distance   given hidden state can  included   process used  determine  value   hidden state furthermore    need   features   statistically independent        case   features  used   generative model finally arbitrary features  pairs  adjacent hidden states can  used rather  simple transition probabilities  disadvantages   models   types  prior distributions  can  placed  hidden states  severely limited    possible  predict  probability  seeing  arbitrary observation  second limitation  often   issue  practice since many common usages  hmms   require  predictive probabilities  variant   previously described discriminative model   linearchain conditional random field  uses  undirected graphical model aka markov random field rather   directed graphical models  memms  similar models  advantage   type  model      suffer   socalled label bias problem  memms  thus may make  accurate predictions  disadvantage   training can  slower   memms yet another variant   factorial hidden markov model  allows   single observation   conditioned   corresponding hidden variables   set  formula independent markov chains rather   single markov chain   equivalent   single hmm  formula states assuming   formula states   chain  therefore learning    model  difficult   sequence  length formula  straightforward viterbi algorithm  complexity formula  find  exact solution  junction tree algorithm   used   results   formula complexity  practice approximate techniques   variational approaches   used     models can  extended  allow   distant dependencies among hidden states eg allowing   given state   dependent   previous two  three states rather   single previous state ie  transition probabilities  extended  encompass sets  three  four adjacent states   general formula adjacent states  disadvantage   models   dynamicprogramming algorithms  training    formula running time  formula adjacent states  formula total observations ie  lengthformula markov chain another recent extension   triplet markov model    auxiliary underlying process  added  model  data specificities many variants   model   proposed one  also mention  interesting link    established   theory  evidence   triplet markov models   allows  fuse data  markovian context   model nonstationary data\r\n"}
{"index":{"_id":144}}
{"conceptLabelTag":"statistical inference","conceptLabel":"statistical inference","conceptDescription":"statistical inference statistical inference   process  deducing properties   underlying distribution  analysis  data inferential statistical analysis infers properties   population  includes testing hypotheses  deriving estimates  population  assumed   larger   observed data set   words  observed data  assumed   sampled   larger population inferential statistics can  contrasted  descriptive statistics descriptive statistics  solely concerned  properties   observed data    assume   data came   larger population introduction statistical inference makes propositions   population using data drawn   population   form  sampling given  hypothesis   population    wish  draw inferences statistical inference consists  firstly selecting  statistical model   process  generates  data  secondly deducing propositions   model konishi kitagawa state  majority   problems  statistical inference can  considered   problems related  statistical modeling relatedly sir david cox  said   translation  subjectmatter problem  statistical model  done  often   critical part   analysis  conclusion   statistical inference   statistical proposition  common forms  statistical proposition   following models  assumptions  statistical inference requires  assumptions  statistical model   set  assumptions concerning  generation   observed data  similar data descriptions  statistical models usually emphasize  role  population quantities  interest    wish  draw inference descriptive statistics  typically used   preliminary step   formal inferences  drawn degree  modelsassumptions statisticians distinguish  three levels  modeling assumptions importance  valid modelsassumptions whatever level  assumption  made correctly calibrated inference  general requires  assumptions   correct ie   datagenerating mechanisms really   correctly specified incorrect assumptions  simple random sampling can invalidate statistical inference  complex semi  fully parametric assumptions  also cause  concern  example incorrectly assuming  cox model can   cases lead  faulty conclusions incorrect assumptions  normality   population also invalidates  forms  regressionbased inference  use   parametric model  viewed skeptically   experts  sampling human populations  sampling statisticians   deal  confidence intervals   limit   statements  estimators based   large samples   central limit theorem ensures   estimators will  distributions   nearly normal  particular  normal distribution    totally unrealistic  catastrophically unwise assumption  make    dealing   kind  economic population   central limit theorem states   distribution   sample mean   large samples  approximately normally distributed   distribution   heavy tailed approximate distributions given  difficulty  specifying exact distributions  sample statistics many methods   developed  approximating   finite samples approximation results measure  close  limiting distribution approaches  statistics sample distribution  example  independent samples  normal distribution approximates  two digits  accuracy  distribution   sample mean  many population distributions   berryesseen theorem yet  many practical purposes  normal approximation provides  good approximation   samplemeans distribution      independent samples according  simulation studies  statisticians experience following kolmogorovs work   s advanced statistics uses approximation theory  functional analysis  quantify  error  approximation   approach  metric geometry  probability distributions  studied  approach quantifies approximation error   example  kullbackleibler divergence bregman divergence   hellinger distance  indefinitely large samples limiting results like  central limit theorem describe  sample statistics limiting distribution  one exists limiting results   statements  finite samples  indeed  irrelevant  finite samples however  asymptotic theory  limiting distributions  often invoked  work  finite samples  example limiting results  often invoked  justify  generalized method  moments   use  generalized estimating equations   popular  econometrics  biostatistics  magnitude   difference   limiting distribution   true distribution formally  error   approximation can  assessed using simulation  heuristic application  limiting results  finite samples  common practice  many applications especially  lowdimensional models  logconcave likelihoods    oneparameter exponential families randomizationbased models   given dataset   produced   randomization design  randomization distribution   statistic   hypothesis  defined  evaluating  test statistic     plans     generated   randomization design  frequentist inference randomization allows inferences   based   randomization distribution rather   subjective model    important especially  survey sampling  design  experiments statistical inference  randomized studies  also  straightforward  many  situations  bayesian inference randomization  also  importance  survey sampling use  sampling without replacement ensures  exchangeability   sample   population  randomized experiments randomization warrants  missing  random assumption  covariate information objective randomization allows properly inductive procedures many statisticians prefer randomizationbased analysis  data   generated  welldefined randomization procedures however   true   fields  science  developed theoretical knowledge  experimental control randomized experiments may increase  costs  experimentation without improving  quality  inferences similarly results  randomized experiments  recommended  leading statistical authorities  allowing inferences  greater reliability   observational studies    phenomena however  good observational study may  better   bad randomized experiment  statistical analysis   randomized experiment may  based   randomization scheme stated   experimental protocol    need  subjective model however   time  hypotheses   tested using objective statistical models  accurately describe randomized experiments  random samples   cases  randomized studies  uneconomical  unethical modelbased analysis  randomized experiments   standard practice  refer   statistical model often  linear model  analyzing data  randomized experiments however  randomization scheme guides  choice   statistical model    possible  choose  appropriate model without knowing  randomization scheme seriously misleading results can  obtained analyzing data  randomized experiments  ignoring  experimental protocol common mistakes include forgetting  blocking used   experiment  confusing repeated measurements    experimental unit  independent replicates   treatment applied  different experimental units paradigms  inference different schools  statistical inference  become established  schoolsor paradigmsare  mutually exclusive  methods  work well  one paradigm often  attractive interpretations   paradigms bandyopadhyay forster describe four paradigms  classical statistics  error statistics ii bayesian statistics iii likelihoodbased statistics  iv  akaikeaninformation criterionbased statistics  classical  frequentist paradigm  bayesian paradigm   aicbased paradigm  summarized   likelihoodbased paradigm  essentially  subparadigm   aicbased paradigm frequentist inference  paradigm calibrates  plausibility  propositions  considering notional repeated sampling   population distribution  produce datasets similar   one  hand  considering  datasets characteristics  repeated sampling  frequentist properties   statistical proposition can  quantifiedalthough  practice  quantification may  challenging frequentist inference objectivity  decision theory one interpretation  frequentist inference  classical inference     applicable   terms  frequency probability    terms  repeated sampling   population however  approach  neyman develops  procedures  terms  preexperiment probabilities    undertaking  experiment one decides   rule  coming   conclusion    probability   correct  controlled   suitable way   probability need    frequentist  repeated sampling interpretation  contrast bayesian inference works  terms  conditional probabilities ie probabilities conditional   observed data compared   marginal  conditioned  unknown parameters probabilities used   frequentist approach  frequentist procedures  significance testing  confidence intervals can  constructed without regard  utility functions however  elements  frequentist statistics   statistical decision theory  incorporate utility functions  particular frequentist developments  optimal inference   minimumvariance unbiased estimators  uniformly  powerful testing make use  loss functions  play  role  negative utility functions loss functions need   explicitly stated  statistical theorists  prove   statistical procedure   optimality property however lossfunctions  often useful  stating optimality properties  example medianunbiased estimators  optimal  absolute value loss functions    minimize expected loss  least squares estimators  optimal  squared error loss functions    minimize expected loss  statisticians using frequentist inference must choose    parameters  interest   estimatorstest statistic   used  absence  obviously explicit utilities  prior distributions  helped frequentist procedures  become widely viewed  objective bayesian inference  bayesian calculus describes degrees  belief using  language  probability beliefs  positive integrate  one  obey probability axioms bayesian inference uses  available posterior beliefs   basis  making statistical propositions   several different justifications  using  bayesian approach bayesian inference subjectivity  decision theory many informal bayesian inferences  based  intuitively reasonable summaries   posterior  example  posterior mean median  mode highest posterior density intervals  bayes factors can   motivated   way   users utility function need   stated   sort  inference  summaries   depend   extent  stated prior beliefs   generally viewed  subjective conclusions methods  prior construction    require external input   proposed   yet fully developed formally bayesian inference  calibrated  reference   explicitly stated utility  loss function  bayes rule   one  maximizes expected utility averaged   posterior uncertainty formal bayesian inference therefore automatically provides optimal decisions   decision theoretic sense given assumptions data  utility bayesian inference can  made  essentially  problem although  every statistical inference need   bayesian interpretation analyses    formally bayesian can  logically incoherent  feature  bayesian procedures  use proper priors ie  integrable  one     guaranteed   coherent  advocates  bayesian inference assert  inference must take place   decisiontheoretic framework   bayesian inference   conclude   evaluation  summarization  posterior beliefs  paradigms  inference minimum description length  minimum description length mdl principle   developed  ideas  information theory   theory  kolmogorov complexity  mdl principle selects statistical models  maximally compress  data inference proceeds without assuming counterfactual  nonfalsifiable datagenerating mechanisms  probability models   data  might  done  frequentist  bayesian approaches however   data generating mechanism  exist  reality  according  shannons source coding theorem  provides  mdl description   data  average  asymptotically  minimizing description length  descriptive complexity mdl estimation  similar  maximum likelihood estimation  maximum  posteriori estimation using maximumentropy bayesian priors however mdl avoids assuming   underlying probability model  known  mdl principle can also  applied without assumptions  eg  data arose  independent sampling  mdl principle   applied  communicationcoding theory  information theory  linear regression   data mining  evaluation  mdlbased inferential procedures often uses techniques  criteria  computational complexity theory fiducial inference fiducial inference   approach  statistical inference based  fiducial probability also known   fiducial distribution  subsequent work  approach   called illdefined extremely limited  applicability  even fallacious however  argument       shows   socalled confidence distribution    valid probability distribution  since    invalidated  application  confidence intervals    necessarily invalidate conclusions drawn  fiducial arguments  attempt  made  reinterpret  early work  fishers fiducial argument   special case   inference theory using upper  lower probabilities structural inference developing ideas  fisher   pitman   george  barnard developed structural inference  pivotal inference  approach using invariant probabilities  group families barnard reformulated  arguments behind fiducial inference   restricted class  models   fiducial procedures   welldefined  useful inference topics  topics   usually included   area  statistical inference\r\n"}
{"index":{"_id":145}}
{"conceptLabelTag":"multidimensional scaling","conceptLabel":"multidimensional scaling","conceptDescription":"multidimensional scaling multidimensional scaling mds   means  visualizing  level  similarity  individual cases   dataset  refers   set  related ordination techniques used  information visualization  particular  display  information contained   distance matrix  mds algorithm aims  place  object  ndimensional space    betweenobject distances  preserved  well  possible  object   assigned coordinates     n dimensions  number  dimensions   mds plot n can exceed   specified  priori choosing n optimizes  object locations   twodimensional scatterplot types mds algorithms fall   taxonomy depending   meaning   input matrix classical multidimensional scaling   also known  principal coordinates analysis pcoa torgerson scaling  torgersongower scaling  takes  input matrix giving dissimilarities  pairs  items  outputs  coordinate matrix whose configuration minimizes  loss function called strain  example given  aerial distances  many cities   matrix formula  formula   distance   coordinates  formula  formula city given  formula now  want  find  coordinates   cities  problem  addressed  classical mds metric multidimensional scaling    superset  classical mds  generalizes  optimization procedure   variety  loss functions  input matrices  known distances  weights     useful loss function   context  called stress   often minimized using  procedure called stress majorization metric mds minimizes  cost function called stress    residual sum  squaresformula  formula nonmetric multidimensional scaling  contrast  metric mds nonmetric mds finds   nonparametric monotonic relationship   dissimilarities   itemitem matrix   euclidean distances  items   location   item   lowdimensional space  relationship  typically found using isotonic regression let formula denote  vector  proximities formula  monotonic transformation  formula  d  point distances  coordinates    found  minimize  socalled stress generalized multidimensional scaling  extension  metric multidimensional scaling    target space   arbitrary smooth noneuclidean space  cases   dissimilarities  distances   surface   target space  another surface gmds allows finding  minimumdistortion embedding  one surface  another details  data   analyzed   collection  formula objects colors faces stocks    distance function  defined  distances   entries   dissimilarity matrix  goal  mds  given formula  find formula vectors formula    formula   vector norm  classical mds  norm   euclidean distance    broader sense  may   metric  arbitrary distance function   words mds attempts  find  embedding   formula objects  formula   distances  preserved   dimension formula  chosen     may plot  vectors formula  obtain  visualization   similarities   formula objects note   vectors formula   unique   euclidean distance  may  arbitrarily translated rotated  reflected since  transformations   change  pairwise distances formula   various approaches  determining  vectors formula usually mds  formulated   optimization problem  formula  found   minimizer   cost function  example  solution may   found  numerical optimization techniques   particularly chosen cost functions minimizers can  stated analytically  terms  matrix eigendecompositions procedure   several steps  conducting mds research\r\n"}
{"index":{"_id":146}}
{"conceptLabelTag":"empirical risk minimization","conceptLabel":"empirical risk minimization","conceptDescription":"empirical risk minimization empirical risk minimization erm   principle  statistical learning theory  defines  family  learning algorithms   used  give theoretical bounds   performance  learning algorithms background consider  following situation    general setting  many supervised learning problems   two spaces  objects formula  formula   like  learn  function formula often called hypothesis  outputs  object formula given formula        disposal  training set    examples formula  formula   input  formula   corresponding response   wish  get  formula  put   formally  assume     joint probability distribution formula  formula  formula    training set consists  formula instances formula drawn iid  formula note   assumption   joint probability distribution allows us  model uncertainty  predictions eg  noise  data  formula    deterministic function  formula  rather  random variable  conditional distribution formula   fixed formula  also assume    given  nonnegative realvalued loss function formula  measures  different  prediction formula   hypothesis    true outcome formula  risk associated  hypothesis formula   defined   expectation   loss function  loss function commonly used  theory   loss function formula  formula   indicator notation  ultimate goal   learning algorithm   find  hypothesis formula among  fixed class  functions formula    risk formula  minimal empirical risk minimization  general  risk formula   computed   distribution formula  unknown   learning algorithm  situation  referred   agnostic learning however  can compute  approximation called empirical risk  averaging  loss function   training set empirical risk minimization principle states   learning algorithm  choose  hypothesis formula  minimizes  empirical risk thus  learning algorithm defined   erm principle consists  solving   optimization problem properties computational complexity empirical risk minimization   classification problem  loss function  known    nphard problem even   relatively simple class  functions  linear classifiers though  can  solved efficiently  minimal empirical risk  zero ie data  linearly separable  practice machine learning algorithms cope   either  employing  convex approximation  loss function like hinge loss  svm   easier  optimize   posing assumptions   distribution formula  thus stop  agnostic learning algorithms     result applies\r\n"}
{"index":{"_id":147}}
{"conceptLabelTag":"apriori algorithm","conceptLabel":"apriori algorithm","conceptDescription":"apriori algorithm apriori   algorithm  frequent item set mining  association rule learning  transactional databases  proceeds  identifying  frequent individual items   database  extending   larger  larger item sets  long   item sets appear sufficiently often   database  frequent item sets determined  apriori can  used  determine association rules  highlight general trends   database   applications  domains   market basket analysis overview  apriori algorithm  proposed  agrawal  srikant  apriori  designed  operate  databases containing transactions  example collections  items bought  customers  details   website frequentation  algorithms  designed  finding association rules  data   transactions winepi  minepi    timestamps dna sequencing  transaction  seen   set  items  itemset given  threshold formula  apriori algorithm identifies  item sets   subsets   least formula transactions   database apriori uses  bottom  approach  frequent subsets  extended one item   time  step known  candidate generation  groups  candidates  tested   data  algorithm terminates    successful extensions  found apriori uses breadthfirst search   hash tree structure  count candidate item sets efficiently  generates candidate item sets  length formula  item sets  length formula   prunes  candidates    infrequent sub pattern according   downward closure lemma  candidate set contains  frequent formulalength item sets    scans  transaction database  determine frequent item sets among  candidates  pseudo code   algorithm  given    transaction database formula   support threshold  formula usual set theoretic notation  employed though note  formula   multiset formula   candidate set  level formula   step  algorithm  assumed  generate  candidate sets   large item sets   preceding level heeding  downward closure lemma formula accesses  field   data structure  represents candidate set formula   initially assumed   zero many details  omitted  usually   important part   implementation   data structure used  storing  candidate sets  counting  frequencies formula examples example consider  following database   row   transaction   cell   individual item   transaction  association rules  can  determined   database   following  can also illustrate    variety  examples example assume   large supermarket tracks sales data  stockkeeping unit sku   item  item   butter  bread  identified   numerical sku  supermarket   database  transactions   transaction   set  skus   bought together let  database  transactions consist  following itemsets  will use apriori  determine  frequent item sets   database     will say   item set  frequent   appears   least transactions   database  value   support threshold  first step  apriori   count   number  occurrences called  support   member item separately  scanning  database   first time  obtain  following result   itemsets  size   support   least     frequent  next step   generate  list   pairs   frequent items  example regarding  pair  first table  example shows items  appearing together  three   itemsets therefore  say item  support  three  pairs   meet  exceed  minimum support     frequent  pairs    now     frequent  larger set  contains    frequent   way  can prune sets  will now look  frequent triples   database   can already exclude   triples  contain one   two pairs   example    frequent triplets    minimal threshold    triplets  excluded    super sets  pairs   already   threshold   thus determined  frequent sets  items   database  illustrated   items   counted  one   subsets  already known     threshold limitations apriori  historically significant suffers   number  inefficiencies  tradeoffs   spawned  algorithms candidate generation generates large numbers  subsets  algorithm attempts  load   candidate set   many  possible   scan bottomup subset exploration essentially  breadthfirst traversal   subset lattice finds  maximal subset s    formula   proper subsets later algorithms   maxminer try  identify  maximal frequent item sets without enumerating  subsets  perform s   search space rather   purely bottomup approach\r\n"}
{"index":{"_id":148}}
{"conceptLabelTag":"learning theory","conceptLabel":"learning theory","conceptDescription":"statistical learning theory statistical learning theory   framework  machine learning drawing   fields  statistics  functional analysis statistical learning theory deals   problem  finding  predictive function based  data statistical learning theory  led  successful applications  fields   computer vision speech recognition bioinformatics  baseball introduction  goals  learning  understanding  prediction learning falls  many categories including supervised learning unsupervised learning online learning  reinforcement learning   perspective  statistical learning theory supervised learning  best understood supervised learning involves learning   training set  data every point   training   inputoutput pair   input maps   output  learning problem consists  inferring  function  maps   input   output    learned function can  used  predict output  future input depending   type  output supervised learning problems  either problems  regression  problems  classification   output takes  continuous range  values    regression problem using ohms law   example  regression   performed  voltage  input  current  output  regression  find  functional relationship  voltage  current     classification problems      output will   element   discrete set  labels classification   common  machine learning applications  facial recognition  instance  picture   persons face    input   output label    persons name  input   represented   large multidimensional vector whose elements represent pixels   picture  learning  function based   training set data  function  validated   test set  data data    appear   training set formal description take formula    vector space   possible inputs  formula    vector space   possible outputs statistical learning theory takes  perspective     unknown probability distribution   product space formula ie  exists  unknown formula  training set  made   formula samples   probability distribution   notated every formula   input vector   training data  formula   output  corresponds     formalism  inference problem consists  finding  function formula   formula let formula   space  functions formula called  hypothesis space  hypothesis space   space  functions  algorithm will search  let formula   loss functional  metric   difference   predicted value formula   actual value formula  expected risk  defined    target function  best possible function formula  can  chosen  given   formula  satisfies   probability distribution formula  unknown  proxy measure   expected risk must  used  measure  based   training set  sample   unknown probability distribution   called  empirical risk  learning algorithm  chooses  function formula  minimizes  empirical risk  called empirical risk minimization loss functions  choice  loss function   determining factor   function formula  will  chosen   learning algorithm  loss function also affects  convergence rate   algorithm   important   loss function   convex different loss functions  used depending  whether  problem  one  regression  one  classification regression   common loss function  regression   square loss function also known   lnorm  familiar loss function  used  ordinary least squares regression  form   absolute value loss also known   lnorm  also sometimes used classification   sense  indicator function    natural loss function  classification  takes  value   predicted output      actual output   takes  value   predicted output  different   actual output  binary classification  formula    formula   heaviside step function regularization  machine learning problems  major problem  arises    overfitting  learning   prediction problem  goal    find  function   closely fits  previously observed data   find one  will  accurately predict output  future input empirical risk minimization runs  risk  overfitting finding  function  matches  data exactly    predict future output well overfitting  symptomatic  unstable solutions  small perturbation   training set data  cause  large variation   learned function  can  shown    stability   solution can  guaranteed generalization  consistency  guaranteed  well regularization can solve  overfitting problem  give  problem stability regularization can  accomplished  restricting  hypothesis space formula  common example   restricting formula  linear functions  can  seen   reduction   standard problem  linear regression formula  also  restricted  polynomial  degree formula exponentials  bounded functions  l restriction   hypothesis space avoids overfitting   form   potential functions  limited     allow   choice   function  gives empirical risk arbitrarily close  zero one example  regularization  tikhonov regularization  consists  minimizing  formula   fixed  positive parameter  regularization parameter tikhonov regularization ensures existence uniqueness  stability   solution\r\n"}
{"index":{"_id":149}}
{"conceptLabelTag":"canonical analysis","conceptLabel":"canonical analysis","conceptDescription":"canonical analysis  statistics canonical analysis  bar measuring rod ruler belongs   family  regression methods  data analysis regression analysis quantifies  relationship   predictor variable   criterion variable   coefficient  correlation r coefficient  determination r   standard regression coefficient multiple regression analysis expresses  relationship   set  predictor variables   single criterion variable   multiple correlation r multiple coefficient  determination r   set  standard partial regression weights etc canonical variate analysis captures  relationship   set  predictor variables   set  criterion variables   canonical correlations    sets  canonical weights c  d canonical analysis canonical analysis belongs   group  methods  involve solving  characteristic equation   latent roots  vectors  describes formal structures  hyperspace invariant  respect   rotation   coordinates   type  solution rotation leaves many optimizing properties preserved provided  takes place  certain ways    subspace   corresponding hyperspace  rotation   maximum intervariate correlation structure   different simpler   meaningful structure increases  interpretability   canonical weights c  d    canonical analysis differs  harold hotellings canonical variate analysis also called  canonical correlation analysis designed  obtain maximum canonical correlations   predictor  criterion canonical variates  difference   canonical variate analysis  canonical analysis  analogous   difference   principal components analysis  factor analysis    characteristic set  commonalities eigenvalues  eigenvectors canonical analysis simple canonical analysis   multivariate technique   concerned  determining  relationships  groups  variables   data set  data set  split  two groups x  y based   common characteristics  purpose  canonical analysis    find  relationship  x  y ie can  form  x represent y  works  finding  linear combination  x variables ie x x etc  linear combination  y variables ie y y etc    highly correlated  combination  known   first canonical variates   usually denoted u  v   pair  u  v  called  canonical function  next canonical functions u  v   restricted     uncorrelated  u  v everything  scaled    variance equals one can also construct relationships   made  agree  constraint restrictions arising  theory   agree  common senseintuition   called maximum correlation models tofallis mathematically canonical analysis maximizes uxyv subject  uxxu   vyyv   x  y   data matrices row  instance  column  feature\r\n"}
{"index":{"_id":150}}
{"conceptLabelTag":"principal component analysis","conceptLabel":"principal component analysis","conceptDescription":"principal component analysis principal component analysis pca   statistical procedure  uses  orthogonal transformation  convert  set  observations  possibly correlated variables   set  values  linearly uncorrelated variables called principal components  number  principal components  less   equal   number  original variables  transformation  defined    way   first principal component   largest possible variance   accounts   much   variability   data  possible   succeeding component  turn   highest variance possible   constraint    orthogonal   preceding components  resulting vectors   uncorrelated orthogonal basis set pca  sensitive   relative scaling   original variables pca  invented   karl pearson   analogue   principal axis theorem  mechanics   later independently developed  named  harold hotelling   s depending   field  application   also named  discrete kosambikarhunenlo ve transform klt  signal processing  hotelling transform  multivariate quality control proper orthogonal decomposition pod  mechanical engineering singular value decomposition svd  x golub  van loan eigenvalue decomposition evd  xx  linear algebra factor analysis   discussion   differences  pca  factor analysis see ch  eckartyoung theorem harman  schmidtmirsky theorem  psychometrics empirical orthogonal functions eof  meteorological science empirical eigenfunction decomposition sirovich empirical component analysis lorenz quasiharmonic modes brooks et al spectral decomposition  noise  vibration  empirical modal analysis  structural dynamics pca  mostly used   tool  exploratory data analysis   making predictive models pca can  done  eigenvalue decomposition   data covariance  correlation matrix  singular value decomposition   data matrix usually  mean centering  normalizing  using zscores  data matrix   attribute  results   pca  usually discussed  terms  component scores sometimes called factor scores  transformed variable values corresponding   particular data point  loadings  weight    standardized original variable   multiplied  get  component score pca   simplest   true eigenvectorbased multivariate analyses often  operation can  thought   revealing  internal structure   data   way  best explains  variance   data   multivariate dataset  visualised   set  coordinates   highdimensional data space axis per variable pca can supply  user   lowerdimensional picture  projection  shadow   object  viewed     sense see   informative viewpoint   done  using   first  principal components    dimensionality   transformed data  reduced pca  closely related  factor analysis factor analysis typically incorporates  domain specific assumptions   underlying structure  solves eigenvectors   slightly different matrix pca  also related  canonical correlation analysis cca cca defines coordinate systems  optimally describe  crosscovariance  two datasets  pca defines  new orthogonal coordinate system  optimally describes variance   single dataset intuition pca can  thought   fitting  ndimensional ellipsoid   data   axis   ellipsoid represents  principal component   axis   ellipsoid  small   variance along  axis  also small   omitting  axis   corresponding principal component   representation   dataset  lose   commensurately small amount  information  find  axes   ellipsoid  must first subtract  mean   variable   dataset  center  data around  origin   compute  covariance matrix   data  calculate  eigenvalues  corresponding eigenvectors   covariance matrix   must orthogonalize  set  eigenvectors  normalize   become unit vectors    done    mutually orthogonal unit eigenvectors can  interpreted   axis   ellipsoid fitted   data  proportion   variance   eigenvector represents can  calculated  dividing  eigenvalue corresponding   eigenvector   sum   eigenvalues   important  note   procedure  sensitive   scaling   data      consensus     best scale  data  obtain optimal results details pca  mathematically defined   orthogonal linear transformation  transforms  data   new coordinate system    greatest variance   projection   data comes  lie   first coordinate called  first principal component  second greatest variance   second coordinate    consider  data matrix x  columnwise zero empirical mean  sample mean   column   shifted  zero     n rows represents  different repetition   experiment     p columns gives  particular kind  feature say  results   particular sensor mathematically  transformation  defined   set  pdimensional vectors  weights  loadings formula  map  row vector formula  x   new vector  principal component scores formula given     way   individual variables  t considered   data set successively inherit  maximum possible variance  x   loading vector w constrained    unit vector first component  first loading vector w thus   satisfy equivalently writing   matrix form gives since w   defined    unit vector  equivalently also satisfies  quantity   maximised can  recognised   rayleigh quotient  standard result   symmetric matrix   xx    quotients maximum possible value   largest eigenvalue   matrix  occurs  w   corresponding eigenvector  w found  first component   data vector x can   given   score t x w   transformed coordinates    corresponding vector   original variables x w w  components  kth component can  found  subtracting  first k principal components  x   finding  loading vector  extracts  maximum variance   new data matrix  turns    gives  remaining eigenvectors  xx   maximum values   quantity  brackets given   corresponding eigenvalues thus  loading vectors  eigenvectors  xx  kth component   data vector x can therefore  given   score t x w   transformed coordinates    corresponding vector   space   original variables x w w  w   kth eigenvector  xx  full principal components decomposition  x can therefore  given   w   pbyp matrix whose columns   eigenvectors  xx covariances xx  can  recognised  proportional   empirical sample covariance matrix   dataset x  sample covariance q  two   different principal components   dataset  given    eigenvalue property  w   used  move  line  line however eigenvectors w  w corresponding  eigenvalues   symmetric matrix  orthogonal   eigenvalues  different  can  orthogonalised   vectors happen  share  equal repeated value  product   final line  therefore zero    sample covariance  different principal components   dataset another way  characterise  principal components transformation  therefore   transformation  coordinates  diagonalise  empirical sample covariance matrix  matrix form  empirical covariance matrix   original variables can  written  empirical covariance matrix   principal components becomes    diagonal matrix  eigenvalues  xx dimensionality reduction  transformation t x w maps  data vector x   original space  p variables   new space  p variables   uncorrelated   dataset however    principal components need   kept keeping   first l principal components produced  using   first l loading vectors gives  truncated transformation   matrix t now  n rows   l columns   words pca learns  linear transformation formula   columns  matrix w form  orthogonal basis   l features  components  representation t   decorrelated  construction    transformed data matrices   l columns  score matrix maximises  variance   original data    preserved  minimising  total squared reconstruction error formula  formula  dimensionality reduction can    useful step  visualising  processing highdimensional datasets  still retaining  much   variance   dataset  possible  example selecting l  keeping   first two principal components finds  twodimensional plane   highdimensional dataset    data   spread     data contains clusters   may   spread   therefore  visible   plotted    twodimensional diagram whereas  two directions   data  two   original variables  chosen  random  clusters may  much less spread apart     may  fact  much  likely  substantially overlay   making  indistinguishable similarly  regression analysis  larger  number  explanatory variables allowed  greater   chance  overfitting  model producing conclusions  fail  generalise   datasets one approach especially    strong correlations  different possible explanatory variables   reduce     principal components   run  regression    method called principal component regression dimensionality reduction may also  appropriate   variables   dataset  noisy   column   dataset contains independent identically distributed gaussian noise   columns  t will also contain similarly identically distributed gaussian noise   distribution  invariant   effects   matrix w  can  thought    highdimensional rotation   coordinate axes however     total variance concentrated   first  principal components compared    noise variance  proportionate effect   noise  lessthe first  components achieve  higher signaltonoise ratio pca thus can   effect  concentrating much   signal   first  principal components  can usefully  captured  dimensionality reduction   later principal components may  dominated  noise   disposed  without great loss singular value decomposition  principal components transformation can also  associated  another matrix factorization  singular value decomposition svd  x    nbyp rectangular diagonal matrix  positive numbers called  singular values  x u   nbyn matrix  columns    orthogonal unit vectors  length n called  left singular vectors  x  w   pbyp whose columns  orthogonal unit vectors  length p  called  right singular vectors  x  terms   factorization  matrix xx can  written comparison   eigenvector factorization  xx establishes   right singular vectors w  x  equivalent   eigenvectors  xx   singular values   equal   square roots   eigenvalues  xx using  singular value decomposition  score matrix t can  written   column  t  given  one   left singular vectors  x multiplied   corresponding singular value  form  also  polar decomposition  t efficient algorithms exist  calculate  svd  x without   form  matrix xx  computing  svd  now  standard way  calculate  principal components analysis   data matrix unless   handful  components  required    eigendecomposition  truncated score matrix t can  obtained  considering   first l largest singular values   singular vectors  truncation   matrix m  t using  truncated singular value decomposition   way produces  truncated matrix    nearest possible matrix  rank l   original matrix   sense   difference   two   smallest possible frobenius norm  result known   eckartyoung theorem  considerations given  set  points  euclidean space  first principal component corresponds   line  passes   multidimensional mean  minimizes  sum  squares   distances   points   line  second principal component corresponds    concept   correlation   first principal component   subtracted   points  singular values    square roots   eigenvalues   matrix xx  eigenvalue  proportional   portion   variance  correctly   sum   squared distances   points   multidimensional mean   correlated   eigenvector  sum    eigenvalues  equal   sum   squared distances   points   multidimensional mean pca essentially rotates  set  points around  mean  order  align   principal components  moves  much   variance  possible using  orthogonal transformation   first  dimensions  values   remaining dimensions therefore tend   small  may  dropped  minimal loss  information see  pca  often used   manner  dimensionality reduction pca   distinction    optimal orthogonal transformation  keeping  subspace   largest variance  defined   advantage however comes   price  greater computational requirements  compared  example   applicable   discrete cosine transform   particular   dctii   simply known   dct nonlinear dimensionality reduction techniques tend    computationally demanding  pca pca  sensitive   scaling   variables    just two variables      sample variance   positively correlated   pca will entail  rotation    loadings   two variables  respect   principal component will  equal    multiply  values   first variable    first principal component will  almost     variable   small contribution    variable whereas  second component will  almost aligned   second original variable  means  whenever  different variables  different units like temperature  mass pca   somewhat arbitrary method  analysis different results   obtained  one used fahrenheit rather  celsius  example note  pearsons original paper  entitled  lines  planes  closest fit  systems  points  space  space implies physical euclidean space   concerns   arise one way  making  pca less arbitrary   use variables scaled     unit variance  standardizing  data  hence use  autocorrelation matrix instead   autocovariance matrix   basis  pca however  compresses  expands  fluctuations   dimensions   signal space  unit variance mean subtraction aka mean centering  necessary  performing pca  ensure   first principal component describes  direction  maximum variance  mean subtraction   performed  first principal component might instead correspond   less   mean   data  mean  zero  needed  finding  basis  minimizes  mean square error   approximation   data meancentering  unnecessary  performing  principal components analysis   correlation matrix   data  already centered  calculating correlations correlations  derived   crossproduct  two standard scores zscores  statistical moments hence  name pearson productmoment correlation also see  article  kromrey fosterjohnson  meancentering  moderated regression much ado  nothing  autoencoder neural network   linear hidden layer  similar  pca upon convergence  weight vectors   k neurons   hidden layer will form  basis   space spanned   first k principal components unlike pca  technique will  necessarily produce orthogonal vectors pca   popular primary technique  pattern recognition    however optimized  class separability  alternative   linear discriminant analysis   take   account properties  limitations  pca properties  statistical implication   property    last  pcs   simply unstructured leftovers  removing  important pcs   last pcs  variances  small  possible   useful    right  can help  detect unsuspected nearconstant linear relationships   elements    may also  useful  regression  selecting  subset  variables    outlier detection   look   usage  first look  diagonal elements  perhaps  main statistical implication   result     can  decompose  combined variances    elements   decreasing contributions due   pc   can also decompose  whole covariance matrix  contributions formula   pc although  strictly decreasing  elements  formula will tend  become smaller  formula increases  formula decreases  increasing formula whereas  elements  formula tend  stay    sizebecause   normalization constraints formula limitations  noted   results  pca depend   scaling   variables  scaleinvariant form  pca   developed  applicability  pca  limited  certain assumptions made   derivation pca  information theory dimensionality reduction loses information  general pcabased dimensionality reduction tends  minimize  information loss  certain signal  noise models   assumption  ie   data vector formula   sum   desired informationbearing signal formula   noise signal formula one can show  pca can  optimal  dimensionality reduction   informationtheoretic pointofview  particular linsker showed   formula  gaussian  formula  gaussian noise   covariance matrix proportional   identity matrix  pca maximizes  mutual information formula   desired information formula   dimensionalityreduced output formula   noise  still gaussian    covariance matrix proportional   identity matrix ie  components   vector formula  iid   informationbearing signal formula  nongaussian    common scenario pca  least minimizes  upper bound   information loss   defined   optimality  pca  also preserved   noise formula  iid   least  gaussian  terms   kullbackleibler divergence   informationbearing signal formula  general even    signal model holds pca loses  informationtheoretic optimality  soon   noise formula becomes dependent computing pca using  covariance method  following   detailed description  pca using  covariance method see also   opposed   correlation method  note    better  use  singular value decomposition using standard software  goal   transform  given data set x  dimension p   alternative data set y  smaller dimension l equivalently   seeking  find  matrix y  y   kosambikarhunenlo ve transform klt  matrix x organize  data set suppose   data comprising  set  observations  p variables   want  reduce  data    observation can  described   l variables l p suppose    data  arranged   set  n data vectors formula   formula representing  single grouped observation   p variables calculate  deviations   mean mean subtraction   integral part   solution towards finding  principal component basis  minimizes  mean square error  approximating  data hence  proceed  centering  data  follows derivation  pca using  covariance method let x   ddimensional random vector expressed  column vector without loss  generality assume x  zero mean  want  find formula  orthonormal transformation matrix p   px   diagonal covariant matrix ie px   random vector    distinct components pairwise uncorrelated  quick computation assuming formula  unitary yields hence formula holds     formula  diagonalisable  formula    constructive  varx  guaranteed    nonnegative definite matrix  thus  guaranteed   diagonalisable   unitary matrix iterative computation  practical implementations especially  high dimensional data large  covariance method  rarely used     efficient one way  compute  first principal component efficiently  shown   following pseudocode   data matrix  zero mean without ever computing  covariance matrix  algorithm  simply  efficient way  calculating normalizing  placing  result back  power iteration  avoids  operations  calculating  covariance matrix subsequent principal components can  computed  subtracting component  see gramschmidt   repeating  algorithm  find  next principal component however  simple approach   numerically stable     small number  principal components  required  imprecisions   calculations will additively affect  estimates  subsequent principal components  advanced methods build   basic idea    closely related lanczos algorithm one way  compute  eigenvalue  corresponds   principal component   measure  difference  meansquareddistance   rows   centroid    subtracting   principal component  eigenvalue  corresponds   component   removed  equal   difference  nipals method nonlinear iterative partial least squares nipals   algorithm  computing  first  components   principal component  partial least squares analysis  veryhighdimensional datasets    generated   omics sciences eg genomics metabolomics   usually  necessary  compute  first  pcs  nonlinear iterative partial least squares nipals algorithm calculates t  w  x  outer product tw can   subtracted  x leaving  residual matrix e  can   used  calculate subsequent pcs  results   dramatic reduction  computational time since calculation   covariance matrix  avoided however  large data matrices  matrices    high degree  column collinearity nipals suffers  loss  orthogonality due  machine precision limitations accumulated   iteration step  gramschmidt gs reorthogonalization algorithm  applied    scores   loadings   iteration step  eliminate  loss  orthogonality onlinesequential estimation   online  streaming situation  data arriving piece  piece rather   stored   single batch   useful  make  estimate   pca projection  can  updated sequentially  can  done efficiently  requires different algorithms pca  qualitative variables  pca   common   want  introduce qualitative variables  supplementary elements  example many quantitative variables   measured  plants   plants  qualitative variables  available   example  species    plant belongs  data  subjected  pca  quantitative variables  analyzing  results   natural  connect  principal components   qualitative variable species    following results  produced  results    called introducing  qualitative variable  supplementary element  procedure  detailed   husson l  pag s  pag s  software offer  option   automatic way    case  spad  historically following  work  ludovic lebart   first  propose  option   r package factominer applications interest rate derivatives portfolios principal component analysis can  directly applied   risk management  interest rate derivatives portfolios trading multiple swap instruments   usually  function   market quotable swap instruments  sought   reduced  usually  principal components representing  path  interest rates   macro basis converting risks   represented    factor loadings  multipliers provides assessments  understanding beyond  available  simply collectively viewing risks  individual buckets neuroscience  variant  principal components analysis  used  neuroscience  identify  specific properties   stimulus  increase  neurons probability  generating  action potential  technique  known  spiketriggered covariance analysis   typical application  experimenter presents  white noise process   stimulus usually either   sensory input   test subject    current injected directly   neuron  records  train  action potentials  spikes produced   neuron   result presumably certain features   stimulus make  neuron  likely  spike  order  extract  features  experimenter calculates  covariance matrix   spiketriggered ensemble  set   stimuli defined  discretized   finite time window typically   order  ms  immediately preceded  spike  eigenvectors   difference   spiketriggered covariance matrix   covariance matrix   prior stimulus ensemble  set   stimuli defined    length time window  indicate  directions   space  stimuli along   variance   spiketriggered ensemble differed       prior stimulus ensemble specifically  eigenvectors   largest positive eigenvalues correspond   directions along   variance   spiketriggered ensemble showed  largest positive change compared   variance   prior since    directions   varying  stimulus led   spike   often good approximations   sought  relevant stimulus features  neuroscience pca  also used  discern  identity   neuron   shape   action potential spike sorting   important procedure  extracellular recording techniques often pick  signals    one neuron  spike sorting one first uses pca  reduce  dimensionality   space  action potential waveforms   performs clustering analysis  associate specific action potentials  individual neurons pca   dimension reduction technique  particularly suited  detect coordinated activities  large neuronal ensembles    used  determining collective variables ie order parameters  phase transitions   brain relation  pca  kmeans clustering   asserted    relaxed solution  means clustering specified   cluster indicators  given   principal components   pca subspace spanned   principal directions  identical   cluster centroid subspace however  pca   useful relaxation  kmeans clustering    new result see  example    straightforward  uncover counterexamples   statement   cluster centroid subspace  spanned   principal directions relation  pca  factor analysis principal component analysis creates variables   linear combinations   original variables  new variables   property   variables   orthogonal  principal components can  used  find clusters   set  data pca   variancefocused approach seeking  reproduce  total variable variance   components reflect  common  unique variance   variable pca  generally preferred  purposes  data reduction ie translating variable space  optimal factor space     goal   detect  latent construct  factors factor analysis  similar  principal component analysis   factor analysis also involves linear combinations  variables different  pca factor analysis   correlationfocused approach seeking  reproduce  intercorrelations among variables    factors represent  common variance  variables excluding unique variance  terms   correlation matrix  corresponds  focusing  explaining  offdiagonal terms ie shared covariance  pca focuses  explaining  terms  sit   diagonal however   side result  trying  reproduce  ondiagonal terms pca also tends  fit relatively well  offdiagonal correlations results given  pca  factor analysis   similar   situations     always  case     problems   results  significantly different factor analysis  generally used   research purpose  detecting data structure ie latent constructs  factors  causal modeling correspondence analysis correspondence analysis ca  developed  jeanpaul benz cri   conceptually similar  pca  scales  data    nonnegative   rows  columns  treated equivalently   traditionally applied  contingency tables ca decomposes  chisquared statistic associated   table  orthogonal factors  ca   descriptive technique  can  applied  tables    chisquared statistic  appropriate   several variants  ca  available including detrended correspondence analysis  canonical correspondence analysis one special extension  multiple correspondence analysis  may  seen   counterpart  principal component analysis  categorical data generalizations nonlinear generalizations    modern methods  nonlinear dimensionality reduction find  theoretical  algorithmic roots  pca  kmeans pearsons original idea   take  straight line  plane  will   best fit   set  data points principal curves  manifolds give  natural geometric framework  pca generalization  extend  geometric interpretation  pca  explicitly constructing  embedded manifold  data approximation   encoding using standard geometric projection onto  manifold    illustrated  fig see also  elastic map algorithm  principal geodesic analysis another popular generalization  kernel pca  corresponds  pca performed   reproducing kernel hilbert space associated   positive definite kernel multilinear generalizations  multilinear subspace learning pca  generalized  multilinear pca mpca  extracts features directly  tensor representations mpca  solved  performing pca   mode   tensor iteratively mpca   applied  face recognition gait recognition etc mpca   extended  uncorrelated mpca nonnegative mpca  robust mpca higher order nway principal component analysis may  performed  models   tucker decomposition parafac multiple factor analysis coinertia analysis statis  distatis robustness weighted pca  pca finds  mathematically optimal method   minimizing  squared error   sensitive  outliers   data  produce large errors pca tries  avoid  therefore  common practice  remove outliers  computing pca however   contexts outliers can  difficult  identify  example  data mining algorithms like correlation clustering  assignment  points  clusters  outliers   known beforehand  recently proposed generalization  pca based   weighted pca increases robustness  assigning different weights  data objects based   estimated relevancy robust pca via decomposition  lowrank  sparse matrices robust principal component analysis rpca   modification   widely used statistical procedure principal component analysis pca  works well  respect  grossly corrupted observations sparse pca  particular disadvantage  pca    principal components  usually linear combinations   input variables sparse pca overcomes  disadvantage  finding linear combinations  contain just   input variables similar techniques independent component analysis independent component analysis ica  directed  similar problems  principal component analysis  finds additively separable components rather  successive approximations network component analysis given  matrix formula  tries  decompose   two matrices   formula  key difference  techniques   pca  ica      entries  formula  constrained    formula  termed  regulatory layer   general   decomposition can  multiple solutions  prove    following conditions  satisfied   decomposition  unique   multiplication   scalar\r\n"}
